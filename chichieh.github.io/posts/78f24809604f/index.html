<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="DeepSeek 本地部屬 llama.cpp 與 ollama" /><meta name="author" content="ChiChieh Huang" /><meta property="og:locale" content="en" /><meta name="description" content="DeepSeek 近期在 LLM 領域的發展炙手可熱，吸引了全球關注。在這篇文章我們使用較具彈性 llama.cpp，以及易於安裝 Ollama，一步一步帶大家示範如何快速在本地部屬 DeepSeek-r1 模型。" /><meta property="og:description" content="DeepSeek 近期在 LLM 領域的發展炙手可熱，吸引了全球關注。在這篇文章我們使用較具彈性 llama.cpp，以及易於安裝 Ollama，一步一步帶大家示範如何快速在本地部屬 DeepSeek-r1 模型。" /><link rel="canonical" href="https://chichieh-huang.com/posts/78f24809604f/" /><meta property="og:url" content="https://chichieh-huang.com/posts/78f24809604f/" /><meta property="og:site_name" content="ChiChieh Huang" /><meta property="og:image" content="https://chichieh-huang.com/assets/78f24809604f/1*4OZ7nN2mRUj5SUXiP5KUkg.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-02-16T17:50:07+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://chichieh-huang.com/assets/78f24809604f/1*4OZ7nN2mRUj5SUXiP5KUkg.png" /><meta property="twitter:title" content="DeepSeek 本地部屬 llama.cpp 與 ollama" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"ChiChieh Huang","url":"https://medium.com/@cch.chichieh"},"dateModified":"2025-03-12T00:39:26+08:00","datePublished":"2025-02-16T17:50:07+08:00","description":"DeepSeek 近期在 LLM 領域的發展炙手可熱，吸引了全球關注。在這篇文章我們使用較具彈性 llama.cpp，以及易於安裝 Ollama，一步一步帶大家示範如何快速在本地部屬 DeepSeek-r1 模型。","headline":"DeepSeek 本地部屬 llama.cpp 與 ollama","image":"https://chichieh-huang.com/assets/78f24809604f/1*4OZ7nN2mRUj5SUXiP5KUkg.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://chichieh-huang.com/posts/78f24809604f/"},"url":"https://chichieh-huang.com/posts/78f24809604f/"}</script><title>DeepSeek 本地部屬 | llama.cpp 與 ollama | ChiChieh Huang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ChiChieh Huang"><meta name="application-name" content="ChiChieh Huang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">ChiChieh Huang</a><p class="site-subtitle fst-italic mb-0"><div class="medium-followers-container"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-link"> <span class="followers-count">880+ followers on&nbsp;</span> <svg class="medium-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z"/><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z"/><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z"/> </svg> <span>Medium</span> </a></div>Hi~ 我專注於 Generative AI 產品開發，熱愛桌球與奇幻小說，並希望透過中文內容與更多人分享 AI 知識，讓技術更貼近社群。</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/wsxqaza12" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="javascript:location.href = 'mailto:' + ['cch.chichieh','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>DeepSeek 本地部屬 | llama.cpp 與 ollama</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>DeepSeek 本地部屬 | llama.cpp 與 ollama</h1><p class="post-desc fw-light mb-4">DeepSeek 近期在 LLM 領域的發展炙手可熱，吸引了全球關注。在這篇文章我們使用較具彈性 llama.cpp，以及易於安裝 Ollama，一步一步帶大家示範如何快速在本地部屬 DeepSeek-r1 模型。</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1739699407" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Feb 16, 2025 </time> </span> <span> Updated <time data-ts="1741711166" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 11, 2025 </time> </span><div class="mt-3 mb-3"> <a href="/assets/78f24809604f/1*4OZ7nN2mRUj5SUXiP5KUkg.png" class="popup img-link preview-img shimmer"><img src="/assets/78f24809604f/1*4OZ7nN2mRUj5SUXiP5KUkg.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://medium.com/@cch.chichieh">ChiChieh Huang</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2638 words" > <em>14 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">DeepSeek 本地部屬 | llama.cpp 與 ollama</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">DeepSeek 本地部屬 | llama.cpp 與 ollama</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><h3 id="deepseek-本地部署--llamacpp-與-ollama"><span class="me-2">DeepSeek 本地部署 | llama.cpp 與 ollama</span><a href="#deepseek-本地部署--llamacpp-與-ollama" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/assets/78f24809604f/1*qvAF4i_TmS3RaWzBsbCigg.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*qvAF4i_TmS3RaWzBsbCigg.png" alt="" loading="lazy"></a></p><p>DeepSeek 近期在 LLM 領域的發展炙手可熱，吸引了全球關注。2024 年 12 月 26 日，DeepSeek 推出 DeepSeek-V3，專為數學、程式設計與中文應用優化，性能對標當時的 OpenAI GPT-4o。</p><p>在 V3 之後，DeepSeek 團隊持續優化架構，並於 2025 年 1 月 20 日 推出 DeepSeek-R1-Zero、DeepSeek-R1 和 DeepSeek-R1-Distill。相較於 V3，R1 採用更高效的計算方式，以更低的成本實現 OpenAI 在 2024 年 12 月 5日才發布的 Reasoning models — GPT-o1 相當的性能。且在 7 日後，2025 年 1 月 27 日，他們又推出了以視覺為基礎的 Janus-Pro-7B 模型。</p><p>雖然部分專家認為 R1 目前主要吸引用戶關注，在企業應用層面尚未帶來革命性變革，但令人興奮的是 DeepSeek 的模型是開源的，任何人都能使用 DeepSeek 的模型建立屬於自己的本地 LLM，並自由加入客製化功能。 因此本篇文章帶你實作 DeepSeek 本地化，並結合 llama.cpp 與 ollama 等工具，讓你能輕鬆打造專屬的 AI 模型。</p><h3 id="一本地運行-llm-模型工具與模型選擇"><span class="me-2">一、本地運行 LLM 模型工具與模型選擇</span><a href="#一本地運行-llm-模型工具與模型選擇" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>在本地運行 LLM 模型時，有許多的推理框架可以選擇，如下圖所示。在這篇文章我們使用較具彈性 llama.cpp，以及易於安裝 Ollama 來示範。</p><ul><li><strong>llama.cpp</strong> ：支持使用 CPU 運行 LLM，非常適合顯示卡 VRAM 不夠但又想測試的使用者。<li><strong>Ollama：</strong> ollama 的底層架構是基於 llama.cpp，並進行了一些調整，最終提供了一個較為便捷的操作介面。</ul><p><a href="/assets/78f24809604f/1*4OZ7nN2mRUj5SUXiP5KUkg.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*4OZ7nN2mRUj5SUXiP5KUkg.png" alt="" loading="lazy"></a></p><p>DeepSeek-V3 與 DeepSeek-R1 皆是 671B 的大型模型，一般使用者的消費級電腦是跑不動的，因此這篇文章測試會以 <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B" target="_blank">DeepSeek-R1-Distill-Qwen-14B</a> 為主。如果你有興趣跑 DeepSeek-R1 671B 完整模型的話，可以使用 llama.cpp 配虛擬記憶體，便可以慢速跑 671B 的大型模型測試效果，大家有興趣我再寫一篇文章教學。</p><p>注意以下教學示範的環境是 Linux，Windows 或 MacOS 可能會有些指令上的差異。</p><h3 id="二使用-llamacpp-部署-deepseek"><span class="me-2">二、使用 llama.cpp 部署 DeepSeek</span><a href="#二使用-llamacpp-部署-deepseek" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>llama.cpp 有提供本地建置、 <a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/docker.md" target="_blank">Docker 部屬</a> 與 <a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/install.md" target="_blank">原生套件管理器安裝(Homebrew、Nix 與 Flox)</a> ，這篇文章會以本地建置 llama.cpp 為主。我之前有寫過使用本地建置 <a href="https://github.com/ggerganov/llama.cpp/issues/11474" target="_blank">llama.cpp 跑 llama2 的教學文章</a> ，其實換成 DeepSeek 也是大同小異，不過事隔近半年， <a href="https://github.com/ggerganov/llama.cpp/issues/11474" target="_blank">llama.cpp</a> 也改版了不少，因此再重寫一次以最新版的 llama.cpp 為準。</p><h3 id="步驟1-clone-llamacpp-專案"><span class="me-2">步驟1. clone llama.cpp 專案</span><a href="#步驟1-clone-llamacpp-專案" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>把 <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a> 的 repo clone 下來</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>git clone https://github.com/ggerganov/llama.cpp.git
<span class="nb">cd </span>llama.cpp
</pre></table></code></div></div><h3 id="步驟2-使用-cmake-進行編譯"><span class="me-2">步驟2. 使用 <code class="language-plaintext highlighter-rouge">Cmake</code> 進行編譯</span><a href="#步驟2-使用-cmake-進行編譯" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>舊版的 llamp.cpp 專案是使用 <code class="language-plaintext highlighter-rouge">make</code> ，現在版本是用 <code class="language-plaintext highlighter-rouge">Cmake</code> 對 llama.cpp 專案進行編譯，更細節的操作指令可以參考 <a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md" target="_blank">官方文件</a> 。</p><p>沒有安裝過 <code class="language-plaintext highlighter-rouge">cmake</code> 的可以使用以下指令安裝：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="c"># linux</span>
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install </span>cmake
</pre></table></code></div></div><h4 id="1-cpu-build"><span class="me-2">1. CPU Build</span><a href="#1-cpu-build" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>如果你電腦只有 CPU 的話可使用以下指令：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>cmake <span class="nt">-B</span> build
cmake <span class="nt">--build</span> build <span class="nt">--config</span> Release
</pre></table></code></div></div><h4 id="2-cuda-build"><span class="me-2">2. CUDA Build</span><a href="#2-cuda-build" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>如果你有 NVIDA 的顯示卡的話，則可使用以下：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>cmake <span class="nt">-B</span> build <span class="nt">-DGGML_CUDA</span><span class="o">=</span>ON
cmake <span class="nt">--build</span> build <span class="nt">--config</span> Release
</pre></table></code></div></div><p>完成後你會在 ./build/bin/ 中看到許多操作用的工具，常用的有 <code class="language-plaintext highlighter-rouge">llama-cli</code> 、 <code class="language-plaintext highlighter-rouge">llama-server、llama-quantize</code> 等。以前版本 build 完後會在 ./ 中看到工具，現在新版則會放在 ./build/bin/ 中，可以注意一下。</p><p><a href="/assets/78f24809604f/1*okD8Oc_wMV5zNIzep6AeTA.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*okD8Oc_wMV5zNIzep6AeTA.png" alt="" loading="lazy"></a></p><h3 id="步驟3-準備環境"><span class="me-2">步驟3. 準備環境</span><a href="#步驟3-準備環境" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>我這邊使用 conda 來管理，官方建議的 python &gt;= 3.7</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>conda create <span class="nt">--name</span> llamaCpp <span class="nv">python</span><span class="o">=</span>3.9
conda activate llamaCpp
python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</pre></table></code></div></div><h3 id="步驟4-量化模型"><span class="me-2">步驟4. 量化模型</span><a href="#步驟4-量化模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="a--下載模型"><span class="me-2">a) . 下載模型</span><a href="#a--下載模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>首先我們要下載 Hugging Face 上的 <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B" target="_blank">DeepSeek-R1-Distill-Qwen-14B</a> ，不熟悉 huggingface 的人，可以參考我 <a href="../ef839026207b/">之前的文章</a> 。載下來確認裡面含有：</p><ul><li>config.json<li>model.safetensors<li>tokenizer.json<li>tokenizer_config.json</ul><p><a href="/assets/78f24809604f/1*GGdrw_hi-WojSh5xIFIbeQ.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*GGdrw_hi-WojSh5xIFIbeQ.png" alt="" loading="lazy"></a></p><h4 id="b--將模型轉為-gguf-格式"><span class="me-2">b) . 將模型轉為 GGUF 格式</span><a href="#b--將模型轉為-gguf-格式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>由於 llama.cpp 只支援 GGUF 格式的模型，但一般模型發布不會是 GGUF 格式，因此我們需要將模型轉成 GGUF 格式。</p><p>llama.cpp 提供了各種 Python 腳本 <code class="language-plaintext highlighter-rouge">convert_*.py</code> 來幫助大家操作，你可以在 步驟1. clone 下來的 llama.cpp 資料夾中看到。而從 Hugging Face 下載的模型，一般會使用 <code class="language-plaintext highlighter-rouge">convert_hf_to_gguf.py</code> 來轉成 GGUF，如以下：</p><ul><li>— outfile: 轉檔後的模型位置。</ul><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python3 convert_hf_to_gguf.py ../DeepSeek-R1-Distill-Qwen-14B <span class="nt">--outfile</span> ./models/DeepSeek-R1-Distill-Qwen-14B-gguf
</pre></table></code></div></div><h4 id="c--使用-llama-quantize-進行量化-可選"><span class="me-2">c) . 使用 llama-quantize 進行量化 (可選)</span><a href="#c--使用-llama-quantize-進行量化-可選" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>擁有 DeepSeek-R1-Distill-Qwen-14B 的 GGUF 模型後，可以進一步將其量化，以損失部分精度換取模型空間縮小，以下是 Q8_0 與 Q4_0 所能壓縮的空間，量化的方式還有很多種，可以參考 <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md" target="_blank">官方文件</a> 。</p><p><a href="/assets/78f24809604f/1*Xon-DM_s4sMsrUZci69Pww.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*Xon-DM_s4sMsrUZci69Pww.png" alt="不同模型使用不同量化方式的空間大小差異" loading="lazy"></a></p><p>不同模型使用不同量化方式的空間大小差異</p><p>這邊使用 Q4_K_M，你也可以換成你想要的量化方式：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="c"># quantize the model to 4-bits (using Q4_K_M method)</span>
./build/bin/llama-quantize ./models/DeepSeek-R1-Distill-Qwen-14B-gguf.gguf ./models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf Q4_K_M
</pre></table></code></div></div><p>我電腦大概跑了 20 分鐘左右，執行完後可以將原始 27.5 GB 的模型量化為8.37 GB。</p><p>若覺得太複雜，你也可以使用 Hugging Face 的 <a href="https://huggingface.co/spaces/ggml-org/gguf-my-repo" target="_blank">GGUF-my-repo</a> 來直接進行量化，有很好的 UI 設計，操作上更為簡便。若不想要量化，網路上也有滿多已經做好量化的模型，可以下載直接使用。</p><h3 id="步驟5-載入模型"><span class="me-2">步驟5. 載入模型</span><a href="#步驟5-載入模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>之後便可以使用 <code class="language-plaintext highlighter-rouge">llama-cli</code> 來與模型進行交談，以下附上常用的參數介紹：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>./build/bin/llama-cli <span class="nt">-m</span> ./models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf <span class="nt">-cnv</span> <span class="nt">--chat-template</span> gemma <span class="nt">-ngl</span> 100
</pre></table></code></div></div><ul><li><code class="language-plaintext highlighter-rouge">-m</code> 指定模型路徑<li><code class="language-plaintext highlighter-rouge">-n</code> 設定生成文字時預測的 token 數量<li><code class="language-plaintext highlighter-rouge">-c</code> 設定輸入提示的上下文大小，預設為 4096<li><code class="language-plaintext highlighter-rouge">-t</code> 設定生成時使用的執行緒數量，建議設定為與系統實體 CPU 核心數相同，以達最佳效能。<li><code class="language-plaintext highlighter-rouge">-cnv</code> 以對話模式運行程式，此模式下不會顯示特殊標記或前後綴<li><code class="language-plaintext highlighter-rouge">-ngl</code> 在支援 GPU 的編譯版本中，此選項可將部分層計算交由 GPU 處理</ul><p><a href="/assets/78f24809604f/1*je34hhF8060oZmEU9kFCPQ.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*je34hhF8060oZmEU9kFCPQ.png" alt="使用 llama-cli 與模型聊天的實機操作" loading="lazy"></a></p><p>使用 llama-cli 與模型聊天的實機操作</p><h3 id="步驟6-伺服器-server"><span class="me-2">步驟6. 伺服器 server</span><a href="#步驟6-伺服器-server" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>llama.cpp 也有提供相容於 OpenAI API 的 HTTP server，只要使用 <code class="language-plaintext highlighter-rouge">./llama-server</code> 就可以快速架設，預設會在 <code class="language-plaintext highlighter-rouge">http://127.0.0.1:8080/</code> 開啟一個 LLM Service，參數的部分比 <code class="language-plaintext highlighter-rouge">llama-cli</code> 多滿多的，可以參考 <a href="https://github.com/ggml-org/llama.cpp/tree/master/examples/server" target="_blank">官方文件</a> ，也可以進到 server 再調整。</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>./build/bin/llama-server <span class="nt">-m</span> ./models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf <span class="nt">-ngl</span> 100
</pre></table></code></div></div><p>開啟後你便能到 <code class="language-plaintext highlighter-rouge">http://127.0.0.1:8080/</code> 與模型對話，llama.cpp 目前聊天介面也優化很多了，推薦大家試試。</p><p><a href="/assets/78f24809604f/1*z1Nmmr2iVyVK8ir9vs1JPg.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*z1Nmmr2iVyVK8ir9vs1JPg.png" alt="" loading="lazy"></a></p><p>啟動了 HTTP server 後，也能使用 CURL 直接與 server 溝通，以便以此為接口串接其他服務。</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>curl <span class="nt">--request</span> POST <span class="se">\</span>
    <span class="nt">--url</span> http://localhost:8080/completion <span class="se">\</span>
    <span class="nt">--header</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
    <span class="nt">--data</span> <span class="s1">'{"prompt": "跟我介紹雪山"}'</span>
</pre></table></code></div></div><p><a href="/assets/78f24809604f/1*tfSCjHfrmqFwKQe6eopr-Q.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*tfSCjHfrmqFwKQe6eopr-Q.png" alt="實機畫面" loading="lazy"></a></p><p>實機畫面</p><h3 id="三使用-ollama-部署-deepseek"><span class="me-2">三、使用 ollama 部署 DeepSeek</span><a href="#三使用-ollama-部署-deepseek" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>上述的 llama.cpp 步驟較為繁雜，但是可以自訂的彈性非常多，不過如果你只是想要直接測試模型效果，不想要操作那麼多流程，可以試試建立在 llama.cpp 基礎上的 ollama。</p><h3 id="步驟1-下載-ollama"><span class="me-2">步驟1. 下載 ollama</span><a href="#步驟1-下載-ollama" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>若你是 linux 環境，首先使用以下指令下載 ollama，非 linux 環境你可以在 <a href="https://ollama.com/download" target="_blank">ollama 官方網站</a> 找到如何下載。</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>curl <span class="nt">-fsSL</span> https://ollama.com/install.sh | sh
</pre></table></code></div></div><h3 id="步驟2-ollama-基礎操作"><span class="me-2">步驟2. Ollama 基礎操作</span><a href="#步驟2-ollama-基礎操作" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>安裝完成後，先啟動 Ollama，以下有一些常用的指令：</p><div class="language-typescript highlighter-rouge"><div class="code-header"> <span data-label-text="TypeScript"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nx">ollama</span> <span class="nx">serve</span>
</pre></table></code></div></div><ul><li><code class="language-plaintext highlighter-rouge">ollama list</code> ：列出所有已下載的模型<li><code class="language-plaintext highlighter-rouge">ollama ps</code> ：顯示目前加載的模型<li><code class="language-plaintext highlighter-rouge">ollama stop &lt;model_name&gt;</code> ：停止加載模型<li><code class="language-plaintext highlighter-rouge">ollama rm &lt;model_name&gt;</code> ：刪除模型</ul><h3 id="步驟3-下載模型"><span class="me-2">步驟3. 下載模型</span><a href="#步驟3-下載模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Ollama 有把一些有名的模型整理好，方便大家直接使用 ollama 工具下載，你可以在 <a href="https://ollama.com/search" target="_blank">官方看到提供的模型列表</a> 。如果列表中沒有你想要的模型，ollama 也可以導入 GGUF 模型，因此你可以在 Hugging Face 下載並使用任何你喜歡的模型。</p><p>以下兩種方式都會示範，你可以擇一實作便可。</p><h4 id="1-使用-ollama-pull-下載模型"><span class="me-2">1. 使用 ollama pull 下載模型：</span><a href="#1-使用-ollama-pull-下載模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>ollama 有將 deepseek-r1 整合進來，並附上不同模型大小的版本，你可以在 <a href="https://ollama.com/library/deepseek-r1" target="_blank">這個網頁</a> 看到，並且有些模型已經量化過了，例如以下 14b 的模型便跟上面在 llama.cpp 示範的模型一模一樣，是經過 Q4_K_M 量化過的 DeepSeek-R1-Distill-Qwen-14B。</p><p><a href="/assets/78f24809604f/1*k8yoTU6am8D48kMgNF-ouQ.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*k8yoTU6am8D48kMgNF-ouQ.png" alt="" loading="lazy"></a></p><p>想要下載這個模型，只要輸入以下指令：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>ollama pull deepseek-r1:14b
</pre></table></code></div></div><h4 id="2-導入已有的-gguf-模型"><span class="me-2">2. 導入已有的 GGUF 模型</span><a href="#2-導入已有的-gguf-模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>這邊以在 llama.cpp 示範操作過，經 Q4_K_M 量化 DeepSeek-R1-Distill-Qwen-14B 模型為範例，上面我們已經得到一個名為 <code class="language-plaintext highlighter-rouge">DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf</code> 的模型。</p><p>a) . 建立 <code class="language-plaintext highlighter-rouge">Modelfile</code> 檔</p><p>首先建立一個檔案，並命名為 <code class="language-plaintext highlighter-rouge">Modelfile</code> ，並在裡面寫上 FROM 以及要匯入的模型路徑。</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>FROM ../llama.cpp_new/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf
</pre></table></code></div></div><p>b) . 在 ollama 建立模型</p><p>接者便可以使用 ollama 指令將模型導入， <code class="language-plaintext highlighter-rouge">DeepSeek-R1-Distill-Qwen-14B-Q4_K_M</code> 是這個模型在 ollama 的名稱，你可以隨意取名。</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>ollama create DeepSeek-R1-Distill-Qwen-14B-Q4_K_M <span class="nt">-f</span> Modelfile
</pre></table></code></div></div><h3 id="步驟4-載入模型"><span class="me-2">步驟4. 載入模型</span><a href="#步驟4-載入模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>使用以上兩種方法任一取得模型後，你可以使用 <code class="language-plaintext highlighter-rouge">ollama list</code> 確認是否有成功。</p><p><a href="/assets/78f24809604f/1*BpkLJ86S11hQTY0gqWA6FQ.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*BpkLJ86S11hQTY0gqWA6FQ.png" alt="" loading="lazy"></a></p><p>確認有模型後，我們便可以使用 <code class="language-plaintext highlighter-rouge">ollama run</code> 來運行模型：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>ollama run DeepSeek-R1-Distill-Qwen-14B-Q4_K_M:latest
</pre></table></code></div></div><p><a href="/assets/78f24809604f/1*gYvr56vl11GL3TtuQBxnXw.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*gYvr56vl11GL3TtuQBxnXw.png" alt="" loading="lazy"></a></p><p>運行的同時 HTTP server 也開著，因此跟 llama.cpp 一樣，也可以使用 CURL 來與模型溝通，後續可以使用這個接口去串聯不同的服務。</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>curl http://localhost:11434/api/generate <span class="nt">-d</span> <span class="s1">'{
  "model": "DeepSeek-R1-Distill-Qwen-14B-Q4_K_M:latest",
  "prompt":"你好，跟我介紹合歡山"
}'</span>
</pre></table></code></div></div><p><a href="/assets/78f24809604f/1*WB9X6e40CupsegSugVH2Xw.png" class="popup img-link shimmer"><img src="/assets/78f24809604f/1*WB9X6e40CupsegSugVH2Xw.png" alt="實機畫面" loading="lazy"></a></p><p>實機畫面</p><h3 id="結論"><span class="me-2">結論</span><a href="#結論" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>LLM 進步越來越快，去年初使用 llama2 + llama.cpp 示範操作時，同樣 9 G 左右的模型與現在 DeepSeek-r1 模型表現差異非常大。去年初用時，模型近乎不能使用，出錯過高且 context window 太短，現在 DeepSeek-r1 9G 的模型回答流暢度以及準確性我覺得都滿堪用的，令人非常期待開源 LLM 的領域發展。</p><p>也就是因此，才想重新寫一篇 llama.cpp 與 ollama 的教學，希望能幫助大家更輕鬆的測試開源 LLM 模型，如果有任何問題都可以留言或寄信給我，如果文章有誤，也請不吝指教。</p><div class="floating-medium-button"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-follow-button"> <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z" fill="currentColor" /><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z" fill="currentColor" /><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z" fill="currentColor" /> </svg> Follow me on Medium </a></div><hr /><p><a href="https://www.buymeacoffee.com/chichieh.huang" target="_blank" class="img-link shimmer" ><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories//"></a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/deepseek/" class="post-tag no-text-decoration" >deepseek</a> <a href="/tags/ollama/" class="post-tag no-text-decoration" >ollama</a> <a href="/tags/llama-cpp/" class="post-tag no-text-decoration" >llama-cpp</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >ai</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=DeepSeek%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E5%B1%AC%20%7C%20llama.cpp%20%E8%88%87%20ollama%20-%20ChiChieh%20Huang&url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F78f24809604f%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=DeepSeek%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E5%B1%AC%20%7C%20llama.cpp%20%E8%88%87%20ollama%20-%20ChiChieh%20Huang&u=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F78f24809604f%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F78f24809604f%2F&text=DeepSeek%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E5%B1%AC%20%7C%20llama.cpp%20%E8%88%87%20ollama%20-%20ChiChieh%20Huang" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/a1d263ce61b4/">Migrating Away From v0 | Switching to Cursor/Windsurf</a><li class="text-truncate lh-lg"> <a href="/posts/5a67f86311d3/">從 v0 搬家 | 改用Cursor/Windsurf 替代</a><li class="text-truncate lh-lg"> <a href="/posts/942b2f15bea4/">我們都在用 AI，但 AI 跟 AI 怎麼溝通？淺談 AI Agent 通訊協定</a><li class="text-truncate lh-lg"> <a href="/posts/a3476af62056/">OpenManus Tutorial: How to Build Your Custom AI Agent in 2025 (Beginner’s Guide)</a><li class="text-truncate lh-lg"> <a href="/posts/d30783070827/">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/2451807f8ba5/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1705338474" data-df="ll" > Jan 16, 2024 </time><h4 class="pt-0 my-2">用手機就能跑 LLaMA 2! llama.cpp 教學</h4><div class="text-muted"><p>使用 llama.cpp 建立屬於你的 LLM</p></div></div></a></article><article class="col"> <a href="/posts/73c870be4b10/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1741180067" data-df="ll" > Mar 5, 2025 </time><h4 class="pt-0 my-2">深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</h4><div class="text-muted"><p>推理模型 (Reasoning models) 正迅速崛起，如 OpenAI-o1、DeepSeek-R1。本文將以淺顯易懂的語言，探討什麼是 Test-Time Compute 以及其與 Reasoning models 的關係，並以 DeepSeek-R1 為例。</p></div></div></a></article><article class="col"> <a href="/posts/895bb92b0c08/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1730885246" data-df="ll" > Nov 6, 2024 </time><h4 class="pt-0 my-2">Local GraphRAG | llama.cpp: 使用地端 LLM</h4><div class="text-muted"><p>這篇文章便是希望帶大家一起操作，如何在 Microsoft GraphRAG 中使用llama.cpp 架設的地端 LLM 伺服器，主要會分成兩步驟： A. 啟動 llama.cpp 伺服器 B. Microsoft GraphRAG</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/a8e27ea274ed/" class="btn btn-outline-primary" aria-label="Older" ><p>AI Avatar | 虛擬人偶的製作與種類</p></a> <a href="/posts/73c870be4b10/" class="btn btn-outline-primary" aria-label="Newer" ><p>深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/wsxqaza12">ChiChieh Huang</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.3.0" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.<br/> Automatically sync posts from Medium with <a href="https://zhgchg.li/posts/en-medium-to-jekyll/" target="_blank">ZhgChg.Li</a>.<br/>Last updated: 2025-06-15 11:02:16 +08:00</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
