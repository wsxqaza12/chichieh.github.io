<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Understanding Reasoning Models &amp; Test-Time Compute: Insights from DeepSeek-R1" /><meta name="author" content="ChiChieh Huang" /><meta property="og:locale" content="en" /><meta name="description" content="In this article, we’ll break down Test-Time Compute and its relationship with reasoning models in a clear and engaging way. We’ll first …" /><meta property="og:description" content="In this article, we’ll break down Test-Time Compute and its relationship with reasoning models in a clear and engaging way. We’ll first …" /><link rel="canonical" href="https://chichieh-huang.com/posts/d30783070827/" /><meta property="og:url" content="https://chichieh-huang.com/posts/d30783070827/" /><meta property="og:site_name" content="ChiChieh Huang" /><meta property="og:image" content="https://chichieh-huang.com/assets/d30783070827/1*2gAteNQXK0WxvfWTSgI6Rg.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-03-18T04:12:48+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://chichieh-huang.com/assets/d30783070827/1*2gAteNQXK0WxvfWTSgI6Rg.png" /><meta property="twitter:title" content="Understanding Reasoning Models &amp; Test-Time Compute: Insights from DeepSeek-R1" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"ChiChieh Huang","url":"https://medium.com/@cch.chichieh"},"dateModified":"2025-03-18T04:12:48+08:00","datePublished":"2025-03-18T04:12:48+08:00","description":"In this article, we’ll break down Test-Time Compute and its relationship with reasoning models in a clear and engaging way. We’ll first …","headline":"Understanding Reasoning Models &amp; Test-Time Compute: Insights from DeepSeek-R1","image":"https://chichieh-huang.com/assets/d30783070827/1*2gAteNQXK0WxvfWTSgI6Rg.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://chichieh-huang.com/posts/d30783070827/"},"url":"https://chichieh-huang.com/posts/d30783070827/"}</script><title>Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1 | ChiChieh Huang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ChiChieh Huang"><meta name="application-name" content="ChiChieh Huang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">ChiChieh Huang</a><p class="site-subtitle fst-italic mb-0"><div class="medium-followers-container"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-link"> <span class="followers-count">880+ followers on&nbsp;</span> <svg class="medium-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z"/><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z"/><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z"/> </svg> <span>Medium</span> </a></div>Hi~ 我專注於 Generative AI 產品開發，熱愛桌球與奇幻小說，並希望透過中文內容與更多人分享 AI 知識，讓技術更貼近社群。</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/wsxqaza12" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="javascript:location.href = 'mailto:' + ['cch.chichieh','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</h1><p class="post-desc fw-light mb-4">In this article, we’ll break down Test-Time Compute and its relationship with reasoning models in a clear and engaging way. We’ll first ...</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1742242368" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 18, 2025 </time> </span> <span> Updated <time data-ts="1742242368" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 17, 2025 </time> </span><div class="mt-3 mb-3"> <a href="/assets/d30783070827/1*2gAteNQXK0WxvfWTSgI6Rg.png" class="popup img-link preview-img shimmer"><img src="/assets/d30783070827/1*2gAteNQXK0WxvfWTSgI6Rg.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://medium.com/@cch.chichieh">ChiChieh Huang</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="4017 words" > <em>22 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><h3 id="understanding-reasoning-models--test-time-compute-insights-from-deepseek-r1"><span class="me-2">Understanding Reasoning Models &amp; Test-Time Compute: Insights from DeepSeek-R1</span><a href="#understanding-reasoning-models--test-time-compute-insights-from-deepseek-r1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote><p>This article was originally written <a href="../73c870be4b10/">in Chinese</a> and translated into English using AI. If you notice any errors, please feel free to contact me.</p></blockquote><p><a href="/assets/d30783070827/1*2gAteNQXK0WxvfWTSgI6Rg.png" class="popup img-link shimmer"><img src="/assets/d30783070827/1*2gAteNQXK0WxvfWTSgI6Rg.png" alt="Currently known Reasoning models, surveyed up to February 28, 2025" loading="lazy"></a></p><p>Currently known Reasoning models, surveyed up to February 28, 2025</p><p>A new class of large language models (LLMs) is rapidly emerging — Reasoning Models, including OpenAI-o1, DeepSeek-R1, and Alibaba QwQ. Before these models, AI systems were typically designed for speed, generating instant responses. However, OpenAI’s introduction of the o1 model marked a major shift with the concept of “slow thinking.” This approach fundamentally changed expectations, allowing o1 to achieve remarkable feats — ranking 89th in programming competitions, placing in the top 500 of the US Math Olympiad qualifier, and even surpassing PhD-level accuracy in fields like physics, biology, and chemistry.</p><p>This breakthrough highlighted a key insight: when models are given time to process information and reason step by step rather than rushing to produce answers, their performance significantly improves. This phenomenon is closely related to an important concept known as Test-Time Compute.</p><p>In this article, we’ll break down Test-Time Compute and its relationship with reasoning models in a clear and engaging way. We’ll first explore the origins and evolution of reasoning models before introducing the concept of Test-Time Compute. Then, we’ll examine how DeepSeek-R1 utilizes this technique, analyze its impact and limitations, and finally, discuss what the future holds for this exciting area of AI development.</p><h3 id="what-are-reasoning-models"><span class="me-2">What are Reasoning models?</span><a href="#what-are-reasoning-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Reasoning models are a novel category of language models (LMs) designed to tackle complex problems by breaking them down into smaller, structured steps and solving them through explicit logical reasoning. These models, known as Reasoning Language Models (RLMs) or Large Reasoning Models (LRMs), are gaining prominence for their ability to perform systematic reasoning ( <a href="https://arxiv.org/abs/2501.11223" target="_blank">Besta et al., 2025</a> ) .</p><p>To understand the distinction between RLMs and traditional LMs, we can draw an analogy from Daniel <strong>Kahneman’s <em>Thinking, Fast and Slow</em></strong> , which describes two modes of human cognition:</p><ul><li><strong>System-1 thinking</strong> : Fast, intuitive, and automatic<li><strong>System-2 thinking</strong> : Slow, analytical, and logic-driven</ul><p><a href="/assets/d30783070827/0*Ux_wcZVJM16DMMY_.png" class="popup img-link shimmer"><img src="/assets/d30783070827/0*Ux_wcZVJM16DMMY_.png" alt="Two Modes of Human Cognition: System 1 Thinking &amp; System 2 Thinking" loading="lazy"></a></p><p>Two Modes of Human Cognition: System 1 Thinking &amp; System 2 Thinking</p><p>For <strong>System 1 tasks</strong> , such as simple question-answering, standard LMs perform remarkably well, as they can quickly generate responses using pattern recognition — something that students using ChatGPT demonstrate daily. However, <strong>System 2 tasks</strong> , like solving complex university-level math problems, demand a structured, multi-step reasoning process. Unlike conventional LMs, RLMs are specifically designed to handle these challenges by methodically deconstructing problems and applying logical inference, making them a significant step forward in AI’s ability to engage in deep reasoning.</p><h3 id="origins-of-reasoning-models"><span class="me-2">Origins of Reasoning Models</span><a href="#origins-of-reasoning-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Following impressive System-1 capabilities demonstrated by LLMs, researchers began exploring their limitations and potential improvements in System-2 tasks. A significant breakthrough was the Chain-of-Thought (CoT) experiments proposed by <a href="https://arxiv.org/abs/2201.11903" target="_blank">Wei et al. (2022)</a> . They found that simply including instructions like “Let’s think step by step” in prompts dramatically enhanced the reasoning capabilities of LLMs. This phenomenon was further validated by <a href="https://arxiv.org/abs/2205.11916" target="_blank">Kojima et al. (2022)</a> , leading to more sophisticated reasoning frameworks, such as Tree of Thoughts (ToT) .</p><p>Although OpenAI’s o1 model’s “deep reasoning” superficially resembles existing CoT techniques, there is a critical difference. While CoT prompts models to explicitly articulate reasoning steps, the intermediate steps themselves remain unverified and unweighted against alternative possibilities. Therefore, even a single incorrect intermediate step can compromise the final output. Thus, CoT-like methods merely prompt reasoning at the superficial level without enabling the models to truly internalize reasoning processes.</p><h3 id="evolution-of-reasoning-models"><span class="me-2">Evolution of Reasoning Models</span><a href="#evolution-of-reasoning-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The foundation of RLMs remains <strong>intricate and opaque</strong> , making it difficult to determine whether OpenAI o1’s advancements stem from the model itself or rely on external systems. While its technical details are still largely undisclosed, OpenAI o1 has undoubtedly played a pivotal role in the evolution of RLMs, driven by three major technological advancements:</p><ul><li>Reinforcement Learning (RL) -based model design, such as AlphaZero<li>Advancements in Large Language Models (LLMs) and Transformer architectures, exemplified by GPT-4o<li>The continuous growth of supercomputing and High-Performance Computing (HPC) capabilities</ul><p><a href="/assets/d30783070827/1*nhRPBDxaFX-6nVd5DzV5og.png" class="popup img-link shimmer"><img src="/assets/d30783070827/1*nhRPBDxaFX-6nVd5DzV5og.png" alt="[Besta et al. (2025)](https://arxiv.org/abs/2501.11223){:target=&quot;_blank&quot;} explore the history and evolution of RLM through three key elements." loading="lazy"></a></p><p><a href="https://arxiv.org/abs/2501.11223" target="_blank">Besta et al. (2025)</a> explore the history and evolution of RLM through three key elements.</p><p>Following OpenAI o1’s success, researchers have attempted to reconstruct its reasoning mechanisms. Studies by <a href="https://arxiv.org/abs/2501.11223" target="_blank">Besta et al. (2025)</a> and <a href="https://arxiv.org/abs/2410.09671" target="_blank">Wang et al. (2024)</a> suggest that RLMs may incorporate sophisticated techniques such as Monte Carlo Tree Search (MCTS) and Beam Search to enhance decision-making, alongside reinforcement learning (RL) for ongoing optimization. Furthermore, Process-Based Supervision (PBS), Chain-of-Thought (CoT) and Tree-of-Thought (ToT) reasoning methods, as well as Retrieval-Augmented Generation (RAG), are considered integral to their functionality.</p><p>In the open-source domain, initiatives such as <a href="https://github.com/openreasoner/openr" target="_blank">OpenR (Wang et al., 2024)</a> , <a href="https://github.com/THUDM/ReST-MCTS?tab=readme-ov-file" target="_blank">Rest-MCTS (Zhang et al., 2024)</a> , <a href="https://github.com/GAIR-NLP/O1-Journey" target="_blank">Journey Learning (Qin et al., 2024)</a> , and <a href="https://arxiv.org/abs/2410.02884" target="_blank">LLaMA-Berry (Zhang et al., 2024)</a> have pursued various approaches. The following section delves into the research progress of Journey Learning.</p><p><a href="/assets/d30783070827/0*cPHzQAg2b1po86PY.png" class="popup img-link shimmer"><img src="/assets/d30783070827/0*cPHzQAg2b1po86PY.png" alt="[Qin et al. (2024](https://github.com/GAIR-NLP/O1-Journey){:target=&quot;_blank&quot;} ) explore the research trajectory of OpenAI o1 technology, published on October 8, 2024." loading="lazy"></a></p><p><a href="https://github.com/GAIR-NLP/O1-Journey" target="_blank">Qin et al. (2024</a> ) explore the research trajectory of OpenAI o1 technology, published on October 8, 2024.</p><h3 id="reasoning-models-and-deepseek-r1"><span class="me-2">Reasoning Models and DeepSeek-R1</span><a href="#reasoning-models-and-deepseek-r1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>However, none of the aforementioned methods have matched the reasoning performance of OpenAI o1 until the emergence of DeepSeek-R1. As described in their paper ( <a href="https://arxiv.org/abs/2501.12948" target="_blank">DeepSeek-AI et al. (2025</a> ), DeepSeek follows the same fundamental principle championed by OpenAI o1: deep, step-by-step reasoning is essential for solving complex tasks. Their key objective was to push models to “think” more deeply during inference, leading them to explore enhancements in Test-Time Compute.</p><p>OpenAI had already laid the foundation for this approach. In their 2024 technical report ( <a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank">OpenAI, 2024</a> ), they introduced the idea of extending inference time to improve Test-Time Compute. Similarly, research by DeepMind ( <a href="https://arxiv.org/abs/2408.03314" target="_blank">Snell et al., 2024</a> ) confirmed that the Scaling Law, originally formulated for training, applies equally to inference. This principle has been validated across benchmarks in fields like mathematics, physics, and chemistry, bringing AI reasoning closer to human expert cognition.</p><p><a href="/assets/d30783070827/1*veenUtAeZ6DFTKYFe4DNUA.png" class="popup img-link shimmer"><img src="/assets/d30783070827/1*veenUtAeZ6DFTKYFe4DNUA.png" alt="The performance of o1 improves steadily with Train-Time Compute and Test-Time Compute (OpenAI, 2024)" loading="lazy"></a></p><p>The performance of o1 improves steadily with Train-Time Compute and Test-Time Compute (OpenAI, 2024)</p><p>DeepSeek, however, brings its own distinctive innovations to model architecture and training, with a strong emphasis on reinforcement learning (RL) and Test-Time Compute extensions. In the following sections, we will explore how DeepSeek constructs its RLMs step by step. But first, we need to understand what Test-Time Compute really means.</p><h3 id="what-is-test-time-compute"><span class="me-2">What is Test-Time Compute?</span><a href="#what-is-test-time-compute" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Test-Time Compute refers to the computational resources required during a model’s inference phase — the processing power and time needed to generate responses after training is complete. More than just a technical metric, it represents a <strong>shift in how AI systems distribute computing resources</strong> .</p><p>Before mid-2024, improving the performance of LLMs was primarily driven by scaling up three key factors, a practice known as Train-Time Compute. While this approach was highly effective, the growing size of pre-trained models made training costs prohibitively expensive, sometimes reaching billions of dollars. These factors included:</p><ul><li>The number of model parameters<li>The size of training datasets (measured in tokens)<li>The computational power required (FLOPs)</ul><p>Interestingly, the scaling laws that governed Train-Time Compute were found to apply to Test-Time Compute as well. Research by <a href="https://arxiv.org/abs/2104.03113" target="_blank">Jones (2021)</a> on board games revealed a linear relationship between the two: increasing Train-Time Compute leads to a proportional reduction in Test-Time Compute.</p><p><a href="/assets/d30783070827/1*2SIIlNycBVcAkBk_bazA-w.png" class="popup img-link shimmer"><img src="/assets/d30783070827/1*2SIIlNycBVcAkBk_bazA-w.png" alt="The relationship between Train-Time Compute and Test-Time Compute across different benchmarks ( [Jones, 2021](https://arxiv.org/abs/2104.03113){:target=&quot;_blank&quot;} )" loading="lazy"></a></p><p>The relationship between Train-Time Compute and Test-Time Compute across different benchmarks ( <a href="https://arxiv.org/abs/2104.03113" target="_blank">Jones, 2021</a> )</p><p>This discovery has led AI developers to rethink computational resource allocation. Instead of focusing solely on pre-training, more attention is now being placed on optimizing inference. By investing more in Test-Time Compute, LLMs can engage in deeper reasoning, effectively “thinking” through complex problems. This shift is particularly crucial for developing autonomous AI agents capable of self-optimization and handling open-ended, high-intensity reasoning and decision-making tasks.</p><h3 id="how-does-deepseek-r1-apply-test-time-compute"><span class="me-2">How Does DeepSeek-R1 Apply Test-Time Compute?</span><a href="#how-does-deepseek-r1-apply-test-time-compute" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>On January 20, 2025, DeepSeek introduced three models: <strong>DeepSeek-R1-Zero</strong> , <strong>DeepSeek-R1</strong> , and <strong>DeepSeek-R1-Distill</strong> . From the evolution of these models, we can clearly see how DeepSeek constructs its RLM.</p><p>Below is an overview of these models, illustrating the evolution clearly:</p><ol><li><strong>DeepSeek-R1-Zero:</strong> Pure RL model.<li><strong>DeepSeek-R1:</strong> Initially fine-tuned with a few multi-step reasoning examples, followed by RL.<li><strong>Distillation:</strong> Transfers DeepSeek-R1’s reasoning capabilities to smaller-scale AI models, equipping them with powerful reasoning performance.</ol><p><a href="/assets/d30783070827/0*LsCseumS39nkwxV-.png" class="popup img-link shimmer"><img src="/assets/d30783070827/0*LsCseumS39nkwxV-.png" alt="The development process of three different models in the DeepSeek R1 technical report ( [Raschka, 2025](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms){:target=&quot;_blank&quot;} )" loading="lazy"></a></p><p>The development process of three different models in the DeepSeek R1 technical report ( <a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" target="_blank">Raschka, 2025</a> )</p><h4 id="1-deepseek-r1-zero-a-pure-rl-model-from-scratch"><span class="me-2">1. DeepSeek-R1-Zero: A Pure RL Model from Scratch</span><a href="#1-deepseek-r1-zero-a-pure-rl-model-from-scratch" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>At this stage, the model solely relies on RL to learn reasoning skills without any initial supervised data. It acquires rewards or penalties through its actions (the model’s outputs) and continuously learns multi-step reasoning through trial and error.<li>During inference, DeepSeek-R1-Zero continuously evolves by revisiting previous thoughts, gaining deeper “reflection” capabilities as reasoning steps increase. This process, termed “self-evolution,” leads the model from producing hundreds of reasoning tokens initially to thousands or tens of thousands over time, directly translating to increased Test-Time Compute.<li>While DeepSeek-R1-Zero demonstrates remarkable effectiveness in mathematics and logical reasoning tasks, it struggles with outputs prone to mixed languages and disorganized formats due to the absence of supervised fine-tuning.<li>Note: The RL algorithm employed here is <strong>Group Relative Policy Optimization (GRPO)</strong> , an improved version of PPO (Proximal Policy Optimization) . GRPO uniquely does not require an additional value function model, reducing training costs and enhancing performance through action comparisons within a group and using average rewards as baselines ( <a href="https://arxiv.org/html/2501.12948v1" target="_blank">DeepSeek-AI et al., 2025</a> ) .</ul><p><a href="/assets/d30783070827/1*yOO5NK_OFIEreFHTXzwFaQ.png" class="popup img-link shimmer"><img src="/assets/d30783070827/1*yOO5NK_OFIEreFHTXzwFaQ.png" alt="The reasoning time of DeepSeek-R1-Zero continuously improves as the RL training process progresses." loading="lazy"></a></p><p>The reasoning time of DeepSeek-R1-Zero continuously improves as the RL training process progresses.</p><h4 id="2-deepseek-r1-multi-stage-optimization-pipeline-for-enhanced-reasoning"><span class="me-2">2. DeepSeek-R1: Multi-stage Optimization Pipeline for Enhanced Reasoning</span><a href="#2-deepseek-r1-multi-stage-optimization-pipeline-for-enhanced-reasoning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Due to the formatting and language mixing issues of DeepSeek-R1-Zero, DeepSeek made further refinements in subsequent models:</p><ul><li><strong>Cold Start Fine-Tuning:</strong> The model is initially “warmed up” using a small number of high-quality Chain-of-Thought (CoT) examples and highly readable data, helping it quickly achieve deep reasoning without becoming unstable or divergent, laying the groundwork for subsequent multi-step reasoning.<li><strong>Reasoning-Oriented RL:</strong> RL training specifically targets fields such as mathematics and programming requiring multi-step reasoning, introducing a “language consistency reward” to penalize language mixing and promote readable, human-friendly outputs. This naturally necessitates additional Test-Time Compute, allowing deeper iterative thinking and verification for complex problems.<li><strong>Rejection Sampling + Supervised Fine-Tuning (SFT):</strong> After the initial RL phase stabilizes, the model generates large-scale data filtered through rejection sampling, retaining high-quality examples for further fine-tuning. This helps extend reasoning capabilities to broader applications and ensures adaptive reasoning during inference.<li><strong>Second RL Phase:</strong> Following the fine-tuning, a second RL phase aligns the model better with user instructions, enhancing both user-friendliness and reasoning accuracy.</ul><h4 id="3-distillation-transferring-large-model-reasoning-capabilities-to-smaller-models"><span class="me-2">3. Distillation: Transferring Large Model Reasoning Capabilities to Smaller Models</span><a href="#3-distillation-transferring-large-model-reasoning-capabilities-to-smaller-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Given that DeepSeek-R1 contains <strong>671B parameters</strong> , operational costs and hardware requirements are significant. To democratize its advanced reasoning capabilities, DeepSeek researchers distilled its reasoning capabilities into smaller models:</p><p><strong>1. Distillation Training Data: 800,000 High-Quality Samples:</strong></p><ul><li>Researchers collected approximately 800,000 high-quality examples (600,000 with explicit CoT reasoning steps, 200,000 general Q&amp;A) .<li>Smaller “student” models learn to replicate probability distributions produced by the “teacher” model (DeepSeek-R1) given the same prompts.</ul><p><strong>2. Learning “Reasoning Patterns,” not Just Answers:</strong></p><ul><li>By aligning outputs closely with the teacher model’s distributions, the student models gradually internalize DeepSeek-R1’s reasoning strategies.<li>The resulting smaller models, such as Qwen-32B, demonstrate high accuracy across various reasoning tasks while being deployable on consumer-grade hardware, significantly reducing deployment barriers.</ul><p>The core idea behind this research is simple yet powerful: <strong>enabling deeper, more flexible thinking during inference — an approach known as Test-Time Compute.</strong> This journey begins with the validation of DeepSeek-R1-Zero, advances to DeepSeek-R1 employing multi-step reasoning during inference, and culminates in the distillation of advanced reasoning techniques from large models into smaller ones. By incorporating additional reasoning steps and computational resources, DeepSeek-R1 excels at solving complex problems with precision and efficiency. Meanwhile, through distillation, these capabilities are shared, allowing more developers and users to leverage cutting-edge inference technology.</p><p>Test-Time Compute is not just about extending inference tokens — it provides the model with the necessary time and resources to iteratively verify, refine, and optimize its reasoning process. This adaptability enables the model to dynamically adjust its strategies at different stages, maximizing its reasoning potential and delivering outstanding performance across multiple benchmarks.</p><h3 id="deepseeks-failed-attempts"><span class="me-2">DeepSeek’s Failed Attempts</span><a href="#deepseeks-failed-attempts" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Nevertheless, DeepSeek’s effort to enhance its Test-Time Compute capabilities has not been entirely smooth. As discussed earlier in the “Evolution of Reasoning Models” section, RLMs likely integrate multiple techniques, including Monte Carlo Tree Search (MCTS), Beam Search, Reinforcement Learning (RL), and Process Reward Models (PRM), many of which are key to Test-Time Compute applications. In its early research phase, DeepSeek explored these techniques to enhance its model’s reasoning capabilities. However, two notable attempts proved unsuccessful:</p><ol><li><strong>Process Reward Model (PRM)</strong> : Although theoretically appealing due to its capacity to evaluate the correctness of each reasoning step individually, PRM faced practical challenges. Precise assessment of intermediate reasoning steps was problematic because many reasoning tasks lack explicit intermediate standards. Furthermore, PRM frequently led to Reward Hacking, where models exploit scoring criteria without genuinely improving outcomes. Additionally, PRM required extensive manual annotation, limiting scalability.<li><strong>Monte Carlo Tree Search (MCTS)</strong> : Despite its success in board games, applying MCTS to LLMs encountered exponential expansion in token-generation search spaces, causing inefficiency. Even when imposing strict limitations on node expansion, models easily fell into local optima. Although MCTS could theoretically enhance exploration efficiency in certain contexts, it proved unstable and impractical for large-scale RL training scenarios.</ol><p>DeepSeek’s experience underscores an important reality: while these advanced Test-Time Compute techniques are theoretically compelling, their high computational costs, complexity, and convergence challenges often make them impractical for real-world applications. Recognizing these limitations, DeepSeek-R1 opted for a more straightforward approach to training its reasoning models, avoiding these overly intricate techniques.</p><p><strong>Note</strong> : DeepSeek has introduced numerous other innovations, which are not covered in this discussion.</p><h3 id="test-time-compute-effects-on-small-models"><span class="me-2">Test-Time Compute Effects on Small Models</span><a href="#test-time-compute-effects-on-small-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Previously, we discussed how DeepSeek-R1 uses distillation to transfer the reasoning power of a massive 671B model to a smaller one. However, distillation is not the only method for enhancing smaller models. Research has shown that simply increasing Test-time Compute — the computational effort allocated during inference — can enable small models to outperform much larger ones, particularly in mathematical and programming tasks.</p><p>For instance, <a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute" target="_blank">Beeching et al. (2024)</a> reported that after 256 iterations, the relatively small Llama-3.2 3B model managed to surpass the performance of the Llama-3.1 70B model, which has over 20 times more parameters. Similarly, DeepMind’s study ( <a href="https://arxiv.org/abs/2408.03314" target="_blank">Snell et al., 2024</a> ) found that when additional compute was provided during inference, the PaLM 2-S small model outperformed a model 14 times its size. <a href="https://arxiv.org/abs/2407.21787" target="_blank">Brown et al. (2024)</a> explored a different approach using Repeated Sampling, where a model generates multiple responses to the same problem and selects the best one. Their results were striking: a mid-sized model that could solve only 15.9% of problems in a single attempt saw its accuracy soar to 56% after 250 attempts, surpassing the previous best single-pass score of 43%.</p><p><a href="/assets/d30783070827/1*R22Ymhk78o0Dxc2plwCDdQ.png" class="popup img-link shimmer"><img src="/assets/d30783070827/1*R22Ymhk78o0Dxc2plwCDdQ.png" alt="[Beeching et al. (2024)](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute){:target=&quot;_blank&quot;} compare the performance of the small models Llama-3.2 1B and 3B against Llama-3.1 70B across different iteration counts." loading="lazy"></a></p><p><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute" target="_blank">Beeching et al. (2024)</a> compare the performance of the small models Llama-3.2 1B and 3B against Llama-3.1 70B across different iteration counts.</p><p>These findings suggest an important shift in perspective: rather than solely relying on scaling up model size and training data, we should consider increasing a model’s ability to “think” more deeply during inference. <strong>Could this be the key to unlocking superior AI performance?</strong></p><h3 id="limitations-of-test-time-compute"><span class="me-2">Limitations of Test-Time Compute</span><a href="#limitations-of-test-time-compute" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>At this stage, <strong>Test-time Compute should be viewed as a complement rather than a replacement for large-scale pre-training</strong> . While increasing Test-time Compute can significantly enhance the performance of smaller models, research indicates that it cannot fully substitute for extensive pre-training. Snell et al. (2024) found that for particularly challenging problems, even when small models perform numerous reasoning steps, their improvements remain limited. In such cases, only expanding the model’s parameters and investing more in Train-Time compute can lead to substantial performance gains. In other words, <strong>Test-time Compute is only effective when the model has a “non-zero starting point”</strong> — if it lacks any prior understanding of a problem type, no amount of additional reasoning will produce the correct answer.</p><p>Similarly, <a href="https://arxiv.org/abs/2407.21787" target="_blank">Brown et al. (2024)</a> observed that when tasks cannot be automatically verified (such as those beyond mathematics and programming), majority voting as a selection strategy eventually reaches a plateau after a few hundred iterations, showing no further gains with additional Test-time Compute. These insights emphasize <strong>the distinct yet complementary roles of Train-Time compute and Test-time Compute</strong> — Train-Time compute lays the foundation of the model’s knowledge and abilities, while Test-time Compute builds upon and enhances these foundations.</p><h3 id="future-directions"><span class="me-2">Future Directions</span><a href="#future-directions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Since 2024, we have seen a rapid emergence of RLMs, and this trend shows no signs of slowing down. With ongoing research pushing the limits of Test-time Compute, we can anticipate even more advancements in the near future. Based on the latest studies and findings, here are some key directions that may shape the future of RLMs:</p><h4 id="1-hybrid-training-and-the-rise-of-rlms"><span class="me-2"><strong>1. Hybrid Training and the Rise of RLMs</strong></span><a href="#1-hybrid-training-and-the-rise-of-rlms" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Test-time Compute has demonstrated its ability to enhance a model’s performance beyond its initial training. In the coming years, we are likely to see a surge in hybrid strategies that integrate Train-time Compute with Test-time Compute, enabling models to achieve superior performance even under constrained computational resources. This shift could make RLMs a fundamental component of next-generation LLMs.</p><h4 id="2-adaptive-reasoning-smarter-and-more-efficient-computation"><span class="me-2">2. Adaptive Reasoning: Smarter and More Efficient Computation</span><a href="#2-adaptive-reasoning-smarter-and-more-efficient-computation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Not all questions require extensive reasoning — some can be answered instantly, while others demand deeper logical processing. Researchers are working on models that can intelligently assess a question’s complexity and allocate computing power accordingly. Future RLM APIs may introduce an “inference budget” setting or dynamically adjust the depth of reasoning to optimize both efficiency and cost, ensuring that users only pay for additional computation when necessary.</p><h4 id="3-balancing-quick-responses-and-deep-thought"><span class="me-2">3. Balancing Quick Responses and Deep Thought</span><a href="#3-balancing-quick-responses-and-deep-thought" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>While this concept overlaps with adaptive reasoning, it focuses on the synergy between standard LLMs and RLMs. A promising direction is the development of a dual-tier AI system: a fast, conventional LLM handles straightforward queries with minimal computation, while an RLM is activated for more complex tasks that require deeper reasoning. This approach ensures both efficiency and enhanced problem-solving capabilities.</p><h4 id="4-ai-that-learns-during-inference"><span class="me-2">4. AI That Learns During Inference</span><a href="#4-ai-that-learns-during-inference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>One of the most cutting-edge research areas is <strong>Test-time Training</strong> , which blurs the boundary between training and inference. This technique allows models to make real-time micro-adjustments to their parameters while solving problems. Imagine an AI model that, like a student cramming key facts just before an exam, refines its responses dynamically based on the task at hand. While still in its early stages, this capability could revolutionize AI adaptability, making models more flexible and responsive even after deployment.</p><p>As RLMs continue to evolve, these advancements could redefine how AI balances efficiency, reasoning, and learning, paving the way for more intelligent and adaptive systems in the future.</p><h3 id="references"><span class="me-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Beeching, Edward, and Tunstall, Lewis, and Rush, Sasha, “Scaling test-time compute with open models.”, 2024.</p><p>Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q.V., R’e, C., &amp; Mirhoseini, A. (2024) . Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. <em>ArXiv, abs/2407.21787</em> .</p><p>Besta, M., Barth, J., Schreiber, E., Kubíček, A., Catarino, A.C., Gerstenberger, R., Nyczyk, P., Iff, P., Li, Y., Houliston, S., Sternal, T., Copik, M., Kwa’sniewski, G., Muller, J., Flis, L., Eberhard, H., Niewiadomski, H., &amp; Hoefler, T. (2025) . Reasoning Language Models: A Blueprint.</p><p>DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z.F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R.J., Jin, R.L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S.S., Zhou, S., Wu, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W.L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X.Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y.K., Wang, Y.Q., Wei, Y.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y.X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., &amp; Zhang, Z. (2025) . DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. <em>ArXiv, abs/2501.12948</em> .</p><p>Jones, A. (2021) . Scaling Scaling Laws with Board Games. <em>ArXiv, abs/2104.03113</em> .</p><p>Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., &amp; Iwasawa, Y. (2022) . Large Language Models are Zero-Shot Reasoners. <em>ArXiv, abs/2205.11916</em> .</p><p>OpenAI. (2024) . <em>Learning to reason with llms</em> . <a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank">https://openai.com/index/learning-to-reason-with-llms/</a></p><p>Plaat, A., Wong, A., Verberne, S., Broekens, J., Stein, N.V., &amp; Back, T.H. (2024) . Reasoning with Large Language Models, a Survey. <em>ArXiv, abs/2407.11511</em> .</p><p>Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., &amp; Liu, P. (2024) . O1 Replication Journey: A Strategic Progress Report — Part 1. <em>ArXiv, abs/2410.18982</em> .</p><p>Raschka, S. (2025) . <em>Understanding reasoning in LLMs</em> . Sebastian Raschka’s Newsletter. <a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" target="_blank">https://magazine.sebastianraschka.com/p/understanding-reasoning-llms</a></p><p>Snell, C., Lee, J., Xu, K., &amp; Kumar, A. (2024) . Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. <em>ArXiv, abs/2408.03314</em> .</p><p>Se, K., &amp; Vert, A. (2025, February 6) . <em>What is test-time compute and how to scale it?</em> [Community Article] . Retrieved from <a href="https://huggingface.co/blog/Kseniase/testtimecompute" target="_blank">https://huggingface.co/blog/Kseniase/testtimecompute</a></p><p>Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., &amp; Sun, H. (2022) . Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. <em>Annual Meeting of the Association for Computational Linguistics</em> .</p><p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.H., Xia, F., Le, Q., &amp; Zhou, D. (2022) . Chain of Thought Prompting Elicits Reasoning in Large Language Models. <em>ArXiv, abs/2201.11903</em> .</p><p>Wang, J., Fang, M., Wan, Z., Wen, M., Zhu, J., Liu, A., Gong, Z., Song, Y., Chen, L., Ni, L.M., Yang, L., Wen, Y., &amp; Zhang, W. (2024) . OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models. <em>ArXiv, abs/2410.09671</em> .</p><p>Wang, J. (2025) . A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1.</p><p>Zhang, D., Zhoubian, S., Yue, Y., Dong, Y., &amp; Tang, J. (2024) . ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search. <em>ArXiv, abs/2406.03816</em> .</p><p>Zhang, D., Wu, J., Lei, J., Che, T., Li, J., Xie, T., Huang, X., Zhang, S., Pavone, M., Li, Y., Ouyang, W., &amp; Zhou, D. (2024) . LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning. <em>ArXiv, abs/2410.02884</em> .</p><h3 id="support--encouragement"><span class="me-2"><strong>Support &amp; Encouragement</strong></span><a href="#support--encouragement" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>If you found this article helpful or would like to support my work, feel free to clap for the article or buy me a coffee via the link below. Thank you for your support!</p><p><a href="/assets/d30783070827/1*QCQqlZr6doDP-cszzpaSpw.png" class="popup img-link shimmer"><img src="/assets/d30783070827/1*QCQqlZr6doDP-cszzpaSpw.png" alt="" loading="lazy"></a></p><div class="floating-medium-button"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-follow-button"> <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z" fill="currentColor" /><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z" fill="currentColor" /><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z" fill="currentColor" /> </svg> Follow me on Medium </a></div><hr /><p><a href="https://www.buymeacoffee.com/chichieh.huang" target="_blank" class="img-link shimmer" ><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories//"></a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/artificial-intelligence/" class="post-tag no-text-decoration" >artificial-intelligence</a> <a href="/tags/research/" class="post-tag no-text-decoration" >research</a> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/large-reasoning-models/" class="post-tag no-text-decoration" >large-reasoning-models</a> <a href="/tags/trends/" class="post-tag no-text-decoration" >trends</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Understanding%20Reasoning%20Models%20&%20Test-Time%20Compute:%20Insights%20from%20DeepSeek-R1%20-%20ChiChieh%20Huang&url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2Fd30783070827%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Understanding%20Reasoning%20Models%20&%20Test-Time%20Compute:%20Insights%20from%20DeepSeek-R1%20-%20ChiChieh%20Huang&u=https%3A%2F%2Fchichieh-huang.com%2Fposts%2Fd30783070827%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2Fd30783070827%2F&text=Understanding%20Reasoning%20Models%20&%20Test-Time%20Compute:%20Insights%20from%20DeepSeek-R1%20-%20ChiChieh%20Huang" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/a1d263ce61b4/">Migrating Away From v0 | Switching to Cursor/Windsurf</a><li class="text-truncate lh-lg"> <a href="/posts/5a67f86311d3/">從 v0 搬家 | 改用Cursor/Windsurf 替代</a><li class="text-truncate lh-lg"> <a href="/posts/942b2f15bea4/">我們都在用 AI，但 AI 跟 AI 怎麼溝通？淺談 AI Agent 通訊協定</a><li class="text-truncate lh-lg"> <a href="/posts/a3476af62056/">OpenManus Tutorial: How to Build Your Custom AI Agent in 2025 (Beginner’s Guide)</a><li class="text-truncate lh-lg"> <a href="/posts/d30783070827/">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/42628a4362f7/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1712949889" data-df="ll" > Apr 13, 2024 </time><h4 class="pt-0 my-2">LLM 評估教學 | EleutherAI LM Evaluation Harness</h4><div class="text-muted"><p>在上一篇文章中，我們探討了評估大型語言模型評估時應考慮的各項指標和細節。而這篇文章中，我們將深入探討如何具體操作去評估 LLM。這篇我們使用的工具框架是 EleutherAI 的 lm-evaluation-harness，以下會帶你一起實機操作。</p></div></div></a></article><article class="col"> <a href="/posts/73c870be4b10/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1741180067" data-df="ll" > Mar 5, 2025 </time><h4 class="pt-0 my-2">深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</h4><div class="text-muted"><p>推理模型 (Reasoning models) 正迅速崛起，如 OpenAI-o1、DeepSeek-R1。本文將以淺顯易懂的語言，探討什麼是 Test-Time Compute 以及其與 Reasoning models 的關係，並以 DeepSeek-R1 為例。</p></div></div></a></article><article class="col"> <a href="/posts/ab70d7117480/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1717580275" data-df="ll" > Jun 5, 2024 </time><h4 class="pt-0 my-2">深入解析 RAG 評估框架：TruLens, RGAR, 與 RAGAs 的比較</h4><div class="text-muted"><p>隨著 RAG 日益發展，有許多 RAG 的變形架構出現，使其成為一個越來越複雜的系統，需要全面性的評估方可監控其效能，提供後續的商業價值。因此，本文旨在探討我們如何全面且廣泛性的評估 RAG 系統，以及 RAG 評估框架的未來方向。</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/8918612ba642/" class="btn btn-outline-primary" aria-label="Older" ><p>OpenManus 教學：讓通用型 AI Agent 走進大眾</p></a> <a href="/posts/a3476af62056/" class="btn btn-outline-primary" aria-label="Newer" ><p>OpenManus Tutorial: How to Build Your Custom AI Agent in 2025 (Beginner’s Guide)</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/wsxqaza12">ChiChieh Huang</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.3.0" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.<br/> Automatically sync posts from Medium with <a href="https://zhgchg.li/posts/en-medium-to-jekyll/" target="_blank">ZhgChg.Li</a>.<br/>Last updated: 2025-06-15 11:02:16 +08:00</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
