<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="用手機就能跑 LLaMA 2! llama.cpp 教學" /><meta name="author" content="ChiChieh Huang" /><meta property="og:locale" content="en" /><meta name="description" content="使用 llama.cpp 建立屬於你的 LLM" /><meta property="og:description" content="使用 llama.cpp 建立屬於你的 LLM" /><link rel="canonical" href="https://chichieh-huang.com/posts/2451807f8ba5/" /><meta property="og:url" content="https://chichieh-huang.com/posts/2451807f8ba5/" /><meta property="og:site_name" content="ChiChieh Huang" /><meta property="og:image" content="https://chichieh-huang.com/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-01-16T01:07:54+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://chichieh-huang.com/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png" /><meta property="twitter:title" content="用手機就能跑 LLaMA 2! llama.cpp 教學" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"ChiChieh Huang","url":"https://medium.com/@cch.chichieh"},"dateModified":"2025-02-20T18:58:49+08:00","datePublished":"2024-01-16T01:07:54+08:00","description":"使用 llama.cpp 建立屬於你的 LLM","headline":"用手機就能跑 LLaMA 2! llama.cpp 教學","image":"https://chichieh-huang.com/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://chichieh-huang.com/posts/2451807f8ba5/"},"url":"https://chichieh-huang.com/posts/2451807f8ba5/"}</script><title>用手機就能跑 LLaMA 2! llama.cpp 教學 | ChiChieh Huang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ChiChieh Huang"><meta name="application-name" content="ChiChieh Huang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">ChiChieh Huang</a><p class="site-subtitle fst-italic mb-0"><div class="medium-followers-container"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-link"> <span class="followers-count">880+ followers on&nbsp;</span> <svg class="medium-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z"/><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z"/><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z"/> </svg> <span>Medium</span> </a></div>Hi~ 我專注於 Generative AI 產品開發，熱愛桌球與奇幻小說，並希望透過中文內容與更多人分享 AI 知識，讓技術更貼近社群。</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/wsxqaza12" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="javascript:location.href = 'mailto:' + ['cch.chichieh','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>用手機就能跑 LLaMA 2! llama.cpp 教學</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>用手機就能跑 LLaMA 2! llama.cpp 教學</h1><p class="post-desc fw-light mb-4">使用 llama.cpp 建立屬於你的 LLM</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1705338474" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 16, 2024 </time> </span> <span> Updated <time data-ts="1740049129" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Feb 20, 2025 </time> </span><div class="mt-3 mb-3"> <a href="/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png" class="popup img-link preview-img shimmer"><img src="/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://medium.com/@cch.chichieh">ChiChieh Huang</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="1374 words" > <em>7 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">用手機就能跑 LLaMA 2! llama.cpp 教學</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">用手機就能跑 LLaMA 2! llama.cpp 教學</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><h3 id="用筆電就能跑-llama--llamacpp-教學"><span class="me-2">用筆電就能跑 LLaMA ! llama.cpp 教學</span><a href="#用筆電就能跑-llama--llamacpp-教學" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png" class="popup img-link shimmer"><img src="/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png" alt="" loading="lazy"></a></p><blockquote><p>llama.cpp 更新非常多了，有興趣的朋友請參考我於 2025/2/17 寫的新 <a href="../78f24809604f/">文章</a> 。</p></blockquote><p>隨著人工智能的快速發展，大型語言模型（LLM）如 Llama 2, 3 已成為技術前沿的熱點。然而，Llama 2 最小的模型有7B(Llama 3 最小為 8B)，需要 14G 左右的記憶體，這不是一般消費級顯卡跑得動的，因此目前有很多方法在研究如何減少其資源使用，例如 <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a> ，號稱可以樹莓派上進行推理； 還有 <a href="https://github.com/juncongmoo/pyllama" target="_blank">pyllama</a> 等。經過處理，最低只用 4G 的 RAM 就可以推理。</p><p>這篇文章將向你展示如何使用 llama.cpp 量化模型並架設 server，使用你專屬的 LLM。</p><h3 id="步驟1-clone-llamacpp-專案"><span class="me-2">步驟1. clone llama.cpp 專案</span><a href="#步驟1-clone-llamacpp-專案" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>把 <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a> 的 repo clone 下來</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>git clone https://github.com/ggerganov/llama.cpp.git
</pre></table></code></div></div><p><a href="/assets/2451807f8ba5/1*iPz_zGuFtUEFxtCs4P-eUQ.png" class="popup img-link shimmer"><img src="/assets/2451807f8ba5/1*iPz_zGuFtUEFxtCs4P-eUQ.png" alt="" loading="lazy"></a></p><h3 id="步驟2-使用-makefile-進行編譯"><span class="me-2">步驟2. 使用 Makefile 進行編譯</span><a href="#步驟2-使用-makefile-進行編譯" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>使用 <code class="language-plaintext highlighter-rouge">make</code> 對 llama.cpp 專案進行編譯，過程中會生成很多檔案，如：</p><ul><li>./llama-cli：用於指令操作<li>./llama-quantize：用於量化二進位檔案<li>./llama-server：啟動伺服器</ul><p><a href="/assets/2451807f8ba5/1*0N8YWwej3vU7qe7lZIGS5Q.png" class="popup img-link shimmer"><img src="/assets/2451807f8ba5/1*0N8YWwej3vU7qe7lZIGS5Q.png" alt="" loading="lazy"></a></p><p>上述的方式只能使用 CPU 運行，如果你要使用 GPU：</p><ul><li>Windows/Linux用戶：建議與BLAS（或cuBLAS如果有GPU）一起編譯，可以提高prompt處理速度</ul><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>make <span class="nv">GGML_CUDA</span><span class="o">=</span>1 <span class="c"># LLAMA_CUBLAS 已棄用</span>
</pre></table></code></div></div><ul><li>macOS用戶：無需額外操作，llama.cpp已對ARM NEON做優化， 並且已自動啟用BLAS。 M系列晶片建議使用Metal啟用GPU推理，顯著提升速度。</ul><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">LLAMA_METAL</span><span class="o">=</span>1 make
</pre></table></code></div></div><p>— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p>如果 <code class="language-plaintext highlighter-rouge">make</code> 時遇到以下錯誤：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>: not foundld-info.sh: 2:
: not foundld-info.sh: 4:
: not foundld-info.sh: 9:
scripts/build-info.sh: 31: Syntax error: end of file unexpected <span class="o">(</span>expecting <span class="s2">"then"</span><span class="o">)</span>
make: <span class="k">***</span> <span class="o">[</span>Makefile:676: common/build-info.cpp] Error 2
</pre></table></code></div></div><p>這是由於檔案結尾字元不一樣導致，wins Git clone下載得到資料夾(CRLF)後，上傳至 linux 系統中(LF)，便會造成這種錯誤，可以使用以下方式修復：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nb">sudo </span>apt <span class="nb">install </span>dos2unix
find <span class="nt">-type</span> f <span class="nt">-print0</span> | xargs <span class="nt">-0</span> dos2unix
</pre></table></code></div></div><h3 id="步驟4-準備環境"><span class="me-2">步驟4. 準備環境</span><a href="#步驟4-準備環境" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>建議使用 conda 來管理</p><div class="language-lua highlighter-rouge"><div class="code-header"> <span data-label-text="Lua"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">conda</span> <span class="n">create</span> <span class="c1">--name llamaCpp python=3.9</span>
<span class="n">conda</span> <span class="n">activate</span> <span class="n">llamaCpp</span>
<span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="p">.</span><span class="n">txt</span>
</pre></table></code></div></div><h3 id="步驟5-量化模型"><span class="me-2">步驟5. 量化模型</span><a href="#步驟5-量化模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>使用 <code class="language-plaintext highlighter-rouge">convert.py</code> 來對模型進行量化。這邊示範如何把 Llama2-chat 7B 量化：</p><h3 id="a--下載-meta-llama-27b-chat"><span class="me-2">a) . 下載 meta Llama 27B chat</span><a href="#a--下載-meta-llama-27b-chat" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>這邊可以參考我之前的 <a href="../d4374ed248d9/">文章</a> ，你也可以依照你的需求下載 Llama 3。假設你照著我之前的做法做，那麼你會有一個 clone 下來的 llama 專案，並且裡面載了 llama-2–7b-chat/，看一下這個資料夾中有哪些檔案：</p><p><a href="/assets/2451807f8ba5/1*QGecv1vIc47jxb08lERjMA.png" class="popup img-link shimmer"><img src="/assets/2451807f8ba5/1*QGecv1vIc47jxb08lERjMA.png" alt="" loading="lazy"></a></p><h3 id="b--使用-convert-hf-to-ggufpy-轉換為-gguf-格式"><span class="me-2">b) . 使用 convert-hf-to-gguf.py 轉換為 GGUF 格式</span><a href="#b--使用-convert-hf-to-ggufpy-轉換為-gguf-格式" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>llama.cpp 能夠執行的模型文件格式被稱為 GGUF 格式。可以使用 llama.cpp 提供的 <code class="language-plaintext highlighter-rouge">convert-hf-to-gguf.py</code> 工具，來將 llama-2–7b-chat 模型轉換為 GGUF 格式。</p><p><code class="language-plaintext highlighter-rouge">convert-hf-to-gguf.py</code> 已經在上面準備環境的步驟中安裝，因此只需要執行以下指令。</p><p>註：舊版 llama.cpp 使用 <code class="language-plaintext highlighter-rouge">convert.py</code></p><ul><li>注意要使用 <code class="language-plaintext highlighter-rouge">— vocab-dir</code> 來指定 tokenizer.model 的位置，如果沒有指定會遇到 Error <code class="language-plaintext highlighter-rouge">FileNotFoundError: spm tokenizer.model not found.</code><li>使用 <code class="language-plaintext highlighter-rouge">--outtype</code> 參數來決定資料的型態，一般來說會自動偵測可以不指定。!注意這不是量化<li>使用 <code class="language-plaintext highlighter-rouge">— outfile</code> 來指定轉換後模型的位置</ul><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nb">cd </span>llama.cpp
python3 convert-hf-to-gguf.py models/llama-2-7b-chat <span class="nt">--vocab-dir</span> ../llama <span class="nt">--outtype</span> fP16
 <span class="nt">--outfile</span> models/llama-2-7b-chat/llama-2-model.gguf
</pre></table></code></div></div><p>執行後我遇到一個 Error：</p><pre><code class="language-vbnet">ValueError: The model's vocab size is set to -1 in params.json. Please update it manually. Maybe 32000?
</code></pre><p>這個問題需要改 llama-2–7b-chat/ 中的 <code class="language-plaintext highlighter-rouge">params.json</code> ，將 vocab_size 從 -1 改成 32000。</p><div class="language-yaml highlighter-rouge"><div class="code-header"> <span data-label-text="YAML"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="pi">{</span><span class="s2">"</span><span class="s">dim"</span><span class="pi">:</span> <span class="nv">4096</span><span class="pi">,</span> <span class="s2">"</span><span class="s">multiple_of"</span><span class="pi">:</span> <span class="nv">256</span><span class="pi">,</span> <span class="s2">"</span><span class="s">n_heads"</span><span class="pi">:</span> <span class="nv">32</span><span class="pi">,</span> <span class="s2">"</span><span class="s">n_layers"</span><span class="pi">:</span> <span class="nv">32</span><span class="pi">,</span> <span class="s2">"</span><span class="s">norm_eps"</span><span class="pi">:</span> <span class="nv">1e-06</span><span class="pi">,</span> <span class="s2">"</span><span class="s">vocab_size"</span><span class="pi">:</span> <span class="nv">-1</span><span class="pi">}</span> <span class="c1"># 原始</span>
<span class="pi">{</span><span class="s2">"</span><span class="s">dim"</span><span class="pi">:</span> <span class="nv">4096</span><span class="pi">,</span> <span class="s2">"</span><span class="s">multiple_of"</span><span class="pi">:</span> <span class="nv">256</span><span class="pi">,</span> <span class="s2">"</span><span class="s">n_heads"</span><span class="pi">:</span> <span class="nv">32</span><span class="pi">,</span> <span class="s2">"</span><span class="s">n_layers"</span><span class="pi">:</span> <span class="nv">32</span><span class="pi">,</span> <span class="s2">"</span><span class="s">norm_eps"</span><span class="pi">:</span> <span class="nv">1e-06</span><span class="pi">,</span> <span class="s2">"</span><span class="s">vocab_size"</span><span class="pi">:</span> <span class="nv">32000</span><span class="pi">}</span> <span class="c1"># 更改</span>
</pre></table></code></div></div><p>再執行一次：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>python3 convert.py models/llama-2-7b-chat <span class="nt">--vocab-dir</span> ../llama <span class="nt">--outtype</span> fP16
 <span class="nt">--outfile</span> models/llama-2-7b-chat/llama-2-model.gguf
</pre></table></code></div></div><p>便會在 <code class="language-plaintext highlighter-rouge">— outfile</code> 中生成 llama-2-model.gguf。</p><h3 id="c--使用-llama-quantize-進行量化"><span class="me-2">c) . 使用 llama-quantize 進行量化</span><a href="#c--使用-llama-quantize-進行量化" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>使用 ./llama-quantize 可以對模型進行量化，可以選擇的量化尺度有： 8-Bit、6-Bit 到 2-Bit 都可以進行。使用方法如下：</p><p>註：舊版 llama.cpp 使用 <code class="language-plaintext highlighter-rouge">quantize</code></p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>./llama-quantize models/llama-2-7b-chat/llama-2-model.gguf models/llama-2-7b-chat/llama-2_q4.gguf Q4_K
</pre></table></code></div></div><p>檔案大小由原始的12.5 GB 變成 Q8: 6.66 GB；Q4: 3.79 GB。若不想要量化，網路上滿多已經幫你做好的量化模型，可以下載直接使用。</p><h3 id="步驟6-載入模型"><span class="me-2">步驟6. 載入模型</span><a href="#步驟6-載入模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>之後便可以使用 llama.cpp 的 llama-cli，來做一些最基礎的用法：</p><p>註：舊版 llama.cpp 使用 <code class="language-plaintext highlighter-rouge">main</code></p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>./llama-cli <span class="nt">-m</span> models/llama-2-7b-chat/llama-2_q4.gguf <span class="nt">--color</span> <span class="nt">-f</span> prompts/alpaca.txt <span class="nt">-ins</span> <span class="nt">-c</span> 2048 <span class="nt">--temp</span> 0.2 <span class="nt">-n</span> 256 <span class="nt">--repeat_penalty</span> 1.3
./llama-cli <span class="nt">-m</span> models/llama-2-7b-chat/llama-2_q4.gguf <span class="nt">-n</span> 256 <span class="nt">--repeat_penalty</span> 1.0 <span class="nt">--color</span> <span class="nt">-i</span> <span class="nt">-r</span> <span class="s2">"User:"</span> <span class="nt">-f</span> prompts/chat-with-bob.txt
</pre></table></code></div></div><ul><li><code class="language-plaintext highlighter-rouge">-c</code> 控制上下文的長度，數值越大越能參考較長的對話歷史，default=512<li><code class="language-plaintext highlighter-rouge">-f</code> 指定prompt模板<li><code class="language-plaintext highlighter-rouge">-n</code> 控制回復產生的最大長度，default=128<li><code class="language-plaintext highlighter-rouge">-b</code> 控制batch size，default=512<li><code class="language-plaintext highlighter-rouge">-t</code> 控制線程數量，default=8<li><code class="language-plaintext highlighter-rouge">— repeat_penalty</code> 控制中對重複文字的懲罰力度<li><code class="language-plaintext highlighter-rouge">— temp</code> 溫度係數，數值越低迴復的隨機性越小，反之越大<li><code class="language-plaintext highlighter-rouge">— top_p</code> , <code class="language-plaintext highlighter-rouge">top_k</code> 控制解碼取樣的相關參數<li><code class="language-plaintext highlighter-rouge">-m</code> 指定模型路徑<li><code class="language-plaintext highlighter-rouge">-ngl</code> 指定 GPU 讀取層數</ul><p><a href="/assets/2451807f8ba5/1*glhfW0REtdNlEsgiR415uQ.png" class="popup img-link shimmer"><img src="/assets/2451807f8ba5/1*glhfW0REtdNlEsgiR415uQ.png" alt="" loading="lazy"></a></p><h3 id="步驟7-伺服器-server"><span class="me-2">步驟7. 伺服器 server</span><a href="#步驟7-伺服器-server" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>llama.cpp 也提供架設 server 的功能，通過 HTTP API 來存取模型，只要使用 <code class="language-plaintext highlighter-rouge">./llama-server</code> 就可以快速架設，預設會在 <code class="language-plaintext highlighter-rouge">http://127.0.0.1:8080/</code> 開啟一個 LLM Service，參數的部分與 <code class="language-plaintext highlighter-rouge">.main/</code> 差不多，也可以進到 server 再調整。</p><p>註：舊版 llama.cpp 使用 <code class="language-plaintext highlighter-rouge">server</code></p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>./llama-server <span class="nt">-m</span> models/llama-2-7b-chat/llama-2_q4.gguf <span class="nt">-ngl</span> 100 <span class="c">#使用 GPU 版</span>
</pre></table></code></div></div><p><a href="/assets/2451807f8ba5/1*aqeRgnAj_mdn0PAdXhO67Q.png" class="popup img-link shimmer"><img src="/assets/2451807f8ba5/1*aqeRgnAj_mdn0PAdXhO67Q.png" alt="" loading="lazy"></a></p><p><a href="/assets/2451807f8ba5/1*sCJeqhRELoCRb1pvtqJWIA.png" class="popup img-link shimmer"><img src="/assets/2451807f8ba5/1*sCJeqhRELoCRb1pvtqJWIA.png" alt="" loading="lazy"></a></p><div class="floating-medium-button"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-follow-button"> <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z" fill="currentColor" /><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z" fill="currentColor" /><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z" fill="currentColor" /> </svg> Follow me on Medium </a></div><hr /><p><a href="https://www.buymeacoffee.com/chichieh.huang" target="_blank" class="img-link shimmer" ><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/llm-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B/">LLM (大型語言模型)</a>, <a href="/categories/tutorial/">Tutorial</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/llama-2/" class="post-tag no-text-decoration" >llama-2</a> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/llama-cpp/" class="post-tag no-text-decoration" >llama-cpp</a> <a href="/tags/chatbots/" class="post-tag no-text-decoration" >chatbots</a> <a href="/tags/%E4%B8%AD%E6%96%87/" class="post-tag no-text-decoration" >中文</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%E7%94%A8%E6%89%8B%E6%A9%9F%E5%B0%B1%E8%83%BD%E8%B7%91%20LLaMA%202!%20llama.cpp%20%E6%95%99%E5%AD%B8%20-%20ChiChieh%20Huang&url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F2451807f8ba5%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%E7%94%A8%E6%89%8B%E6%A9%9F%E5%B0%B1%E8%83%BD%E8%B7%91%20LLaMA%202!%20llama.cpp%20%E6%95%99%E5%AD%B8%20-%20ChiChieh%20Huang&u=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F2451807f8ba5%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F2451807f8ba5%2F&text=%E7%94%A8%E6%89%8B%E6%A9%9F%E5%B0%B1%E8%83%BD%E8%B7%91%20LLaMA%202!%20llama.cpp%20%E6%95%99%E5%AD%B8%20-%20ChiChieh%20Huang" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/a1d263ce61b4/">Migrating Away From v0 | Switching to Cursor/Windsurf</a><li class="text-truncate lh-lg"> <a href="/posts/5a67f86311d3/">從 v0 搬家 | 改用Cursor/Windsurf 替代</a><li class="text-truncate lh-lg"> <a href="/posts/942b2f15bea4/">我們都在用 AI，但 AI 跟 AI 怎麼溝通？淺談 AI Agent 通訊協定</a><li class="text-truncate lh-lg"> <a href="/posts/a3476af62056/">OpenManus Tutorial: How to Build Your Custom AI Agent in 2025 (Beginner’s Guide)</a><li class="text-truncate lh-lg"> <a href="/posts/d30783070827/">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/d4374ed248d9/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1705252407" data-df="ll" > Jan 15, 2024 </time><h4 class="pt-0 my-2">建立屬於你的 LLM | Llama2 教學</h4><div class="text-muted"><p>一步一步下載使用 Llama2 教學</p></div></div></a></article><article class="col"> <a href="/posts/d6838febf8c4/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1705777720" data-df="ll" > Jan 21, 2024 </time><h4 class="pt-0 my-2">RAG實作教學，LangChain + Llama2 |創造你的個人LLM</h4><div class="text-muted"><p>在這篇文章中，我們將帶你使用 LangChain + Llama2，一步一步架設自己的 RAG（Retrieval-Augmented Generation）的系統，讓你可以上傳自己的 PDF，並且詢問 LLM 關於 PDF 的訊息。</p></div></div></a></article><article class="col"> <a href="/posts/42628a4362f7/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1712949889" data-df="ll" > Apr 13, 2024 </time><h4 class="pt-0 my-2">LLM 評估教學 | EleutherAI LM Evaluation Harness</h4><div class="text-muted"><p>在上一篇文章中，我們探討了評估大型語言模型評估時應考慮的各項指標和細節。而這篇文章中，我們將深入探討如何具體操作去評估 LLM。這篇我們使用的工具框架是 EleutherAI 的 lm-evaluation-harness，以下會帶你一起實機操作。</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/ef839026207b/" class="btn btn-outline-primary" aria-label="Older" ><p>如何從 HuggingFace 下載模型</p></a> <a href="/posts/fced76fdb8b9/" class="btn btn-outline-primary" aria-label="Newer" ><p>RAG (Retrieval Augmented Generation): 為自然語言處理揭開新篇章</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/wsxqaza12">ChiChieh Huang</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.3.0" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.<br/> Automatically sync posts from Medium with <a href="https://zhgchg.li/posts/en-medium-to-jekyll/" target="_blank">ZhgChg.Li</a>.<br/>Last updated: 2025-06-15 11:02:16 +08:00</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
