<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="記憶體不夠? 來看 LLM 的壓縮技術" /><meta name="author" content="ChiChieh Huang" /><meta property="og:locale" content="en" /><meta name="description" content="本篇探討 LLM 的 4 種類型的壓縮技術：剪枝(Pruning)、知識蒸餾(Knowledge Distillation)、量化(Quantization)、低秩因子分解(Low-Rank Factorization)" /><meta property="og:description" content="本篇探討 LLM 的 4 種類型的壓縮技術：剪枝(Pruning)、知識蒸餾(Knowledge Distillation)、量化(Quantization)、低秩因子分解(Low-Rank Factorization)" /><link rel="canonical" href="https://chichieh-huang.com/posts/e2d02dd25fd9/" /><meta property="og:url" content="https://chichieh-huang.com/posts/e2d02dd25fd9/" /><meta property="og:site_name" content="ChiChieh Huang" /><meta property="og:image" content="https://chichieh-huang.com/assets/e2d02dd25fd9/1*1sk17L66iVBmconz50QTkg.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-01-28T03:46:09+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://chichieh-huang.com/assets/e2d02dd25fd9/1*1sk17L66iVBmconz50QTkg.png" /><meta property="twitter:title" content="記憶體不夠? 來看 LLM 的壓縮技術" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"ChiChieh Huang","url":"https://medium.com/@cch.chichieh"},"dateModified":"2025-02-20T18:57:49+08:00","datePublished":"2024-01-28T03:46:09+08:00","description":"本篇探討 LLM 的 4 種類型的壓縮技術：剪枝(Pruning)、知識蒸餾(Knowledge Distillation)、量化(Quantization)、低秩因子分解(Low-Rank Factorization)","headline":"記憶體不夠? 來看 LLM 的壓縮技術","image":"https://chichieh-huang.com/assets/e2d02dd25fd9/1*1sk17L66iVBmconz50QTkg.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://chichieh-huang.com/posts/e2d02dd25fd9/"},"url":"https://chichieh-huang.com/posts/e2d02dd25fd9/"}</script><title>記憶體不夠? 來看 LLM 的壓縮技術 | ChiChieh Huang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ChiChieh Huang"><meta name="application-name" content="ChiChieh Huang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">ChiChieh Huang</a><p class="site-subtitle fst-italic mb-0"><div class="medium-followers-container"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-link"> <span class="followers-count">880+ followers on&nbsp;</span> <svg class="medium-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z"/><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z"/><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z"/> </svg> <span>Medium</span> </a></div>Hi~ 我專注於 Generative AI 產品開發，熱愛桌球與奇幻小說，並希望透過中文內容與更多人分享 AI 知識，讓技術更貼近社群。</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/wsxqaza12" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="javascript:location.href = 'mailto:' + ['cch.chichieh','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>記憶體不夠? 來看 LLM 的壓縮技術</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>記憶體不夠? 來看 LLM 的壓縮技術</h1><p class="post-desc fw-light mb-4">本篇探討 LLM 的 4 種類型的壓縮技術：剪枝(Pruning)、知識蒸餾(Knowledge Distillation)、量化(Quantization)、低秩因子分解(Low-Rank Factorization)</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1706384769" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 28, 2024 </time> </span> <span> Updated <time data-ts="1740049069" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Feb 20, 2025 </time> </span><div class="mt-3 mb-3"> <a href="/assets/e2d02dd25fd9/1*1sk17L66iVBmconz50QTkg.png" class="popup img-link preview-img shimmer"><img src="/assets/e2d02dd25fd9/1*1sk17L66iVBmconz50QTkg.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://medium.com/@cch.chichieh">ChiChieh Huang</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="3580 words" > <em>19 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">記憶體不夠? 來看 LLM 的壓縮技術</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">記憶體不夠? 來看 LLM 的壓縮技術</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><h3 id="記憶體不夠-來看-llm-的壓縮技術"><span class="me-2">記憶體不夠? 來看 LLM 的壓縮技術</span><a href="#記憶體不夠-來看-llm-的壓縮技術" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/assets/e2d02dd25fd9/1*1sk17L66iVBmconz50QTkg.png" class="popup img-link shimmer"><img src="/assets/e2d02dd25fd9/1*1sk17L66iVBmconz50QTkg.png" alt="" loading="lazy"></a></p><p>大型語言模型（LLMs）所佔的記憶體非常大：</p><ul><li>GPT-175B 用半精度 FP16(2 bytes) 儲存，需要 175*2 ~ 320 GB<li>LLaMA2–65B 用全精度 FP32(4 bytes) 儲存，需要 65*4 ~ 260 GB</ul><p>對比一張 A100 只有 80G，因此想在常見硬體設備上進行 fine tuning 或預測，是一個極大的挑戰。因此當前 LLMs 的在壓縮技術領域中，有豐富多樣的研究，針對不同的挑戰和應用提出解決方案，目前主要的研究領域有 4 大塊，如下圖可以看到不同領域的模型壓縮技術。</p><p><a href="/assets/e2d02dd25fd9/0*0_YnZEAmS0ZHCAjW.jpeg" class="popup img-link shimmer"><img src="/assets/e2d02dd25fd9/0*0_YnZEAmS0ZHCAjW.jpeg" alt="Canwen and Julian (2022) . A Survey on Model Compression for Large Language Models." loading="lazy"></a></p><p>Canwen and Julian (2022) . A Survey on Model Compression for Large Language Models.</p><h3 id="1-剪枝-pruning"><span class="me-2">1. 剪枝 (Pruning)</span><a href="#1-剪枝-pruning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>剪枝透過移除語言模型中的非關鍵或多餘組件（例如權重參數）來提升模型效率，這是一種最佳化的方法。此方法透過裁剪對模型效能貢獻較小的參數，減少了儲存需求，又優化了記憶體和運算效率，同時盡量保持模型效能的穩定性。剪枝主要分為兩大類：非結構化剪枝和結構化剪枝。</p><h3 id="1-a--非結構化剪枝--unstructured-pruning-"><span class="me-2"><strong>1-a. ) 非結構化剪枝 (</strong> Unstructured Pruning <strong>)</strong></span><a href="#1-a--非結構化剪枝--unstructured-pruning-" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>主要指隨機移除單一參數，不考慮模型的整體架構。</p><p>此方法透過將那些低於特定閾值的參數設為零來實現對個別權重或神經元的處理，因此容易使模型產生一種不規則的稀疏結構，而這種不規則性則需要特別的壓縮技術來有效儲存和計算。</p><p>非結構化剪枝通常要求對大型語言模型進行廣泛的再訓練以恢復效能，這對於LLM來說成本極高，因此有些研究在探討這個領域，以下提出 3 種不同的方法。</p><ol><li><strong>SparseGPT</strong> (Frantar and Alistarh, 2023) 提出了一種無需重新訓練的一次性剪枝方法。此方法將剪枝問題轉化為廣義的稀疏迴歸問題，並採用近似稀疏迴歸求解器來實現顯著的非結構化稀疏性。<li><strong>LoRAPrune</strong> (Zhang <em>et al.</em> , 2023) 結合了參數高效調整 (PEFT) 策略和剪枝技術，旨在提升特定下游任務的性能。 此方法採用了獨特的參數重要性評估標準，此標準基於Low-Rank Adaption（LoRA）的值和梯度資訊。<li><strong>Wanda</strong> (Sun <em>et al.</em> , 2023) 則提出了一個新型的剪枝度量方法。此方法透過計算每個權重的大小與對應輸入活化範數的乘積來進行評估，此計算透過使用小型校準資料集來近似實現。 這種度量方式用於模型的線性層輸出中的局部比較，幫助從LLM中剔除相對較不重要的權重。</ol><h3 id="1-b--結構化剪枝-structured-pruning"><span class="me-2">1-b. ) 結構化剪枝 (Structured Pruning)</span><a href="#1-b--結構化剪枝-structured-pruning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>結構化剪枝著重於依照預設規則批量移除連接或層次結構，保持模型的整體架構不變。這種方法的優點在於，它透過一次性處理整組權重，降低了模型的複雜度和記憶體佔用，同時保持LLM的整體結構完整。</p><ol><li><strong>LLM-Pruner</strong> (Ma <em>et al.</em> , 2023) 提出了一種多元化方法來壓縮大型語言模型，並保留其多任務處理和語言生成的能力。它採用了依賴檢測演算法來識別模型內部的依賴結構，並實現了一種高效的重要性評估方法，該方法考慮了一階資訊和近似 Hessian 資訊。</ol><h3 id="2-知識蒸餾--knowledge-distillation-"><span class="me-2">2. 知識蒸餾 ( <strong>Knowledge Distillation</strong> )</span><a href="#2-知識蒸餾--knowledge-distillation-" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>核心思想為 “教師-學生網路”。目的在於將一個大型複雜模型 (教師模型) 的知識，傳遞給一個更小、更簡單的模型 (學生模型)。透過這種方式，學生模型能夠學習並模仿教師模型的行為，從而在保持較高性能的同時減少計算資源的需求。這些方法分為兩大類： <strong>Black-boxKD</strong> ，其中僅可訪問教師的預測；和 <strong>White-boxKD</strong> ，可以利用教師的參數。</p><h3 id="2-a--white-box-kd"><span class="me-2">2-a. ) <strong>White-box KD</strong></span><a href="#2-a--white-box-kd" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>White-box KD 中，不僅可以訪問教師 LLM 的預測，還允許訪問和利用教師LLM的參數。這種方法使學生LM能夠更深入地了解教師LLM的內部結構和知識表達，通常能帶來更高層次的性能改進。White-box KD 通常用於幫助較小的學生 LM 學習和複製更大、更強大的教師 LLM 的知識和能力。</p><ol><li>一個典型的例子是 <strong>MINILLM (</strong> Gu <em>et al.</em> , 2023)，它探討了來自 White-box生成 LLM 的蒸餾。它發現了一個挑戰，即最小化前向Kullback-Leibler 散度 (KLD) 可能導致教師分布不太可能的區域出現過高的概率，從而在自由生成過程中產生不可能的樣本。為了解決這個問題，MINILLM 選擇最小化反向 KLD。這種方法防止學生模型在教師分布中低概率區域的估計過高，從而改善生成樣本的質量。<li><strong>GKD</strong> (Agarwal <em>et al.</em> , 2023) 則探索了來自自回歸模型的蒸餾，其中 White-box 生成 LLM 是一個子集。該方法確定了兩個關鍵問題：訓練期間輸出序列與學生部署期間生成的序列之間的分布不匹配，以及模型規格不足，其中學生模型可能缺乏與教師分布匹配的表達能力。GKD 通過在訓練期間從學生模型抽樣輸出序列來處理分布不匹配。它還通過優化像反向 KL這樣的替代散度來解決模型規格不足問題。</ol><h3 id="2-b--black-box-kd"><span class="me-2">2-b. ) Black-box KD</span><a href="#2-b--black-box-kd" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Black-box KD 僅可訪問教師 LLM 的預測結果。近期的研究強調，像 GPT-3 (175B) 和PaLM (540B參數) 等較大型的 LLM，與較小的模型如BERT (330M) 和 GPT-2(1.5B) 相比，會展現出獨特的行為。這些能力被稱為新興能力，主要有三種：</p><ol><li><strong>上下文學習 (In-Context Learning, ICL)</strong></ol><p>ICL 運用結構化自然語言提示，其中包含任務描述和可能的幾個任務示例作為演示。通過這些任務示例，LLMs可以把握並執行新任務，而無需明確的梯度更新。</p><p><strong>2. 思緒鏈 (Chain of Thought, CoT)</strong></p><p>與 ICL 不同，CoT 採用了不同的方法，將中間推理步驟 (可以導致最終輸出)納入提示中，而不是僅使用簡單的輸入輸出對。 <strong>MT-COT</strong> (Li <em>et al.</em> , 2022) 旨在利用LLMs產生的解釋來增強小型推理器的訓練。它利用多任務學習框架來賦予較小模型強大的推理能力以及產生解釋的能力。</p><p><strong>3. 指令遵循 (Instruction Following, IF)</strong></p><p>IF 旨在提升語言模型僅憑閱讀任務描述就能執行新任務的能力，而無需依賴少樣本示例。通過使用一系列以指令形式表達的任務進行微調，語言模型展示了準確執行以前未見指令描述中的任務的能力。例如，Lion (Jiang <em>et al.</em> , 2023) 利用LLMs的適應性來提升學生模型的性能。它促使 LLM 識別並生成 “難” 指令，然後利用這些指令來增強學生模型的能力。這種方法利用了LLM的多功能性，指導學生模型在處理複雜的指令和任務時進行學習。</p><h3 id="3-量化-quantization"><span class="me-2">3. 量化 (Quantization)</span><a href="#3-量化-quantization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>核心在於將 Float 轉換為整數或其他離散形式，減輕儲存與計算負擔。根據應用量化壓縮模型的階段，量化方法可分為以下三種：</p><h3 id="3-a--量化感知訓練-quantization-aware-training-qat"><span class="me-2">3-a. ) 量化感知訓練 (Quantization-Aware Training, QAT)</span><a href="#3-a--量化感知訓練-quantization-aware-training-qat" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>量化感知訓練是在訓練過程中直接將量化的影響納入考慮的方法。通過模擬訓練過程中的量化效果，QAT 能夠使模型對量化產生的誤差更具韌性。這樣，模型在量化後的性能下降會較小，甚至在某些情況下可以達到與未量化模型相近的性能。</p><p><strong>LLM-QAT</strong> (Liu <em>et al.</em> , 2023) 利用預訓練模型產生的結果來實現無數據蒸餾，並量化權重、activation value 以及 KV Cache，提升吞吐量並支持更長的序列依賴，能夠將帶有量化權重和 KV Cache 的大型LLaMA模型蒸餾為僅有4bits 的模型，證明了製造精準的4 bits 量化 LLM 的可行性。</p><h3 id="3-b--量化感知微調-quantization-aware-fine-tuning-qaf"><span class="me-2">3-b. ) 量化感知微調 (Quantization-aware Fine-tuning, QAF)</span><a href="#3-b--量化感知微調-quantization-aware-fine-tuning-qaf" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>QAF 是指在量化過程後對模型進行微調，以恢復因量化導致的性能損失。QAF 通常在 PTQ 之後執行，通過微調量化模型的參數，進一步優化模型的表現。</p><p>相關的技術如 <strong>PEQA</strong> (Kim <em>et al.,</em> 2023) 和 <strong>QLORA</strong> (Dettmers <em>et al.,</em> 2023) ，它們都專注於促進模型壓縮和加速推理。PEQA 通過雙階段過程，先將全連接層參數矩陣量化為低位整數矩陣和標量向量，再對每個特定下游任務的標量向量進行微調。QLORA 則引入新的數據類型、雙重量化和分頁優化器等創新概念，旨在節省內存並保持性能，使大型模型可在單GPU上微調，在Vicuna基準測試上達到先進水平。值得注意的是這些方法都是屬於量化感知參數高效微調(Parameter-Efficient Fine-Tuning, PEFT) 的技術範疇。</p><h3 id="3-c--後訓練量化-post-training-quantization-ptq"><span class="me-2">3-c. ) 後訓練量化 (Post-Training Quantization, PTQ)</span><a href="#3-c--後訓練量化-post-training-quantization-ptq" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>PTQ 是在模型訓練完成後進行的量化過程。這種方法不需要重新訓練模型，可以直接在訓練好的模型上進行。PTQ 的優勢在於操作簡單、快速，但缺點是可能會因為無法根據量化後的表現進行優化，而導致模型精度有所下降，這部分方法盛行，以下列舉一些近期發表的方法。</p><ol><li><strong>LUT-GEMM</strong> (Park <em>et al.,</em> 2022) 通過對權重進行量化和使用BCQ格式來優化LLM中的矩陣乘法，提升計算效率，降低延遲，同時維持良好性能。<li><strong>LLM.int8( )</strong> (Dettmers <em>et al.,</em> 2022) 採用 8-bit 量化方法，有效減少GPU內存使用，同時保持模型性能，其可以在具有多達 1750B 個參數的模型中進行推理，而不會影響效能<li><strong>ZeroQuant</strong> (Yao <em>et al.,</em> 2022) 結合硬件友好的量化方案、逐層知識蒸餾和優化技術，將基於 Transformer 的模型權重和激活精度減至 INT8，對精度影響極小。<li><strong>GPTQ (</strong> Frantar <em>et al.,</em> 2022) 則提出一種基於近似二階信息的創新層級量化技術，將權重位寬減至3或4 bits，而幾乎不損失精度。<li><strong>AWQ (</strong> Lin <em>et al.,</em> 2023) 發現權重對於 LLM 的影響不一致，因此通過僅保護1%的關鍵權重，可以顯著減少量化誤差。<li><strong>OWQ (</strong> Lee <em>et al.,</em> 2023) 分析了激活異常如何放大權重量化的誤差，並引入混合精度量化策略。<li><strong>SpQR (</strong> Dettmers <em>et al.,</em> 2023) 則專注於識別和隔離異常權重，並將其以較高精度存儲，同時將其他權重壓縮至3–4bits。<li><strong>SmoothQuant (</strong> Xiao <em>et al.,</em> 2022) 針對量化激活過程中的挑戰，引入逐通道縮放轉換，有效地平滑幅度，便於量化，實現高達 3bits 超低精度的無損壓縮。</ol><p><a href="/assets/e2d02dd25fd9/1*jLQthhVrX_XoNecD9Z9e8A.png" class="popup img-link shimmer"><img src="/assets/e2d02dd25fd9/1*jLQthhVrX_XoNecD9Z9e8A.png" alt="LLM 量化方法 Canwen and Julian (2022) . A Survey on Model Compression for Large Language Models." loading="lazy"></a></p><p>LLM 量化方法 Canwen and Julian (2022) . A Survey on Model Compression for Large Language Models.</p><h3 id="4-低秩因子分解-low-rank-factorization"><span class="me-2">4. 低秩因子分解 (Low-Rank Factorization)</span><a href="#4-低秩因子分解-low-rank-factorization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>核心思想是將一個大的矩陣分解為兩個或多個較小、低秩的矩陣的乘積，從而達到數據壓縮和特徵提取的目的。這種方法在降維、噪聲過濾、數據壓縮以及模型參數壓縮等領域有廣泛應用。</p><p>簡單來說，這個方法將大矩陣 W 分解為兩個矩陣 U 和 V，使得 W ≈ UV，其中 U 是 m×k 矩陣，V 是 k×n 矩陣，k 比 m 和 n 小很多。低秩因子分解在數學上應用很久，大家也常使用如奇異值分解 (Singular Value Decomposition, SVD)、主成分分析 (Principal Component Analysis, PCA) 或非負矩陣分解 (Non-negative Matrix Factorization, NMF) 等。</p><p>在 LLM 中，低秩因子分解已被廣泛應用於有效地 fine tuning LLM，例如著名的如 LORA (Hu <em>et al.,</em> 2022) 及其許許多多的變體。而將這個方法應用在模型壓縮上，近期的研究有以下：</p><ol><li><strong>TensorGPT</strong> (Xu <em>et al.,</em> 2023) 以低秩張量格式存儲大型嵌入(embeddings)，減少了LLMs的空間複雜性，使其能夠在邊緣設備上運行。具體來說，TensorGPT利用張量列車分解 (Tensor-Train Decomposition, TTD) 有效壓縮LLMs中的嵌入層。通過將每個 token 嵌入視為一個矩陣積態 (Matrix Product State, MPS)，嵌入層的壓縮比可達到高達38.40倍，同時仍然維持或甚至提升模型相比原始LLM的性能。</ol><h3 id="總結"><span class="me-2">總結</span><a href="#總結" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>總結來說，LLM 的壓縮技術正迅速進步，重點在於在保持準確性和效率的同時，解決多任務處理、語言多樣性和魯棒性等挑戰，希望這篇文章能幫助到你了解 LLM 的相關壓縮技術。</p><h3 id="參考資料"><span class="me-2">參考資料</span><a href="#參考資料" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Canwen and Julian (2022) . A Survey on Model Compression for Large Language Models.</p><div class="floating-medium-button"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-follow-button"> <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z" fill="currentColor" /><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z" fill="currentColor" /><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z" fill="currentColor" /> </svg> Follow me on Medium </a></div><hr /><p><a href="https://www.buymeacoffee.com/chichieh.huang" target="_blank" class="img-link shimmer" ><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/llm-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B/">LLM (大型語言模型)</a>, <a href="/categories/research/">Research</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/compression/" class="post-tag no-text-decoration" >compression</a> <a href="/tags/%E4%B8%AD%E6%96%87/" class="post-tag no-text-decoration" >中文</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%E8%A8%98%E6%86%B6%E9%AB%94%E4%B8%8D%E5%A4%A0?%20%E4%BE%86%E7%9C%8B%20LLM%20%E7%9A%84%E5%A3%93%E7%B8%AE%E6%8A%80%E8%A1%93%20-%20ChiChieh%20Huang&url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2Fe2d02dd25fd9%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%E8%A8%98%E6%86%B6%E9%AB%94%E4%B8%8D%E5%A4%A0?%20%E4%BE%86%E7%9C%8B%20LLM%20%E7%9A%84%E5%A3%93%E7%B8%AE%E6%8A%80%E8%A1%93%20-%20ChiChieh%20Huang&u=https%3A%2F%2Fchichieh-huang.com%2Fposts%2Fe2d02dd25fd9%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2Fe2d02dd25fd9%2F&text=%E8%A8%98%E6%86%B6%E9%AB%94%E4%B8%8D%E5%A4%A0?%20%E4%BE%86%E7%9C%8B%20LLM%20%E7%9A%84%E5%A3%93%E7%B8%AE%E6%8A%80%E8%A1%93%20-%20ChiChieh%20Huang" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/a1d263ce61b4/">Migrating Away From v0 | Switching to Cursor/Windsurf</a><li class="text-truncate lh-lg"> <a href="/posts/5a67f86311d3/">從 v0 搬家 | 改用Cursor/Windsurf 替代</a><li class="text-truncate lh-lg"> <a href="/posts/942b2f15bea4/">我們都在用 AI，但 AI 跟 AI 怎麼溝通？淺談 AI Agent 通訊協定</a><li class="text-truncate lh-lg"> <a href="/posts/a3476af62056/">OpenManus Tutorial: How to Build Your Custom AI Agent in 2025 (Beginner’s Guide)</a><li class="text-truncate lh-lg"> <a href="/posts/d30783070827/">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/ab70d7117480/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1717580275" data-df="ll" > Jun 5, 2024 </time><h4 class="pt-0 my-2">深入解析 RAG 評估框架：TruLens, RGAR, 與 RAGAs 的比較</h4><div class="text-muted"><p>隨著 RAG 日益發展，有許多 RAG 的變形架構出現，使其成為一個越來越複雜的系統，需要全面性的評估方可監控其效能，提供後續的商業價值。因此，本文旨在探討我們如何全面且廣泛性的評估 RAG 系統，以及 RAG 評估框架的未來方向。</p></div></div></a></article><article class="col"> <a href="/posts/e81616d30e53/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1711745115" data-df="ll" > Mar 30, 2024 </time><h4 class="pt-0 my-2">LLM 評估方法指南：趨勢、指標與未來方向</h4><div class="text-muted"><p>在過去的幾年裡，LLM 在自然語言處理 NLP 領域取得了驚人的進步，成為許多應用的核心技術，包括自動回答系統、文本生成、翻譯等等。隨著這些模型能力日益增強，確保模型既準確又公正就顯得非常重要，而這就引伸出一個根本性的問題：我們怎麼評估模型好不好?</p></div></div></a></article><article class="col"> <a href="/posts/c9ac7835adb3/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1710784696" data-df="ll" > Mar 19, 2024 </time><h4 class="pt-0 my-2">LLM 規格比較 | 盤點 ChatGPT, Gemini, Claude, Mistral, llama 等模型</h4><div class="text-muted"><p>最近，Gemini 1.5 和 Claude 3 先後發布使的各種 LLM 的規格變得越來越複雜，因此我決定花時間來整理一份最新的規格比較表，其中包含 OpenAI、Google、Anthropic、Meta 以及 Mistral AI 的模型。</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/c7d1dac2494e/" class="btn btn-outline-primary" aria-label="Older" ><p>RAG實作教學，Streamlit+LangChain+Llama2</p></a> <a href="/posts/0e4ac8adc6df/" class="btn btn-outline-primary" aria-label="Newer" ><p>RAG 優化技巧| 7 大挑戰與解決方式 | 增進你的 LLM</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/wsxqaza12">ChiChieh Huang</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.3.0" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.<br/> Automatically sync posts from Medium with <a href="https://zhgchg.li/posts/en-medium-to-jekyll/" target="_blank">ZhgChg.Li</a>.<br/>Last updated: 2025-06-15 11:02:16 +08:00</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
