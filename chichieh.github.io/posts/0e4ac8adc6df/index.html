<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="RAG 優化技巧 7 大挑戰與解決方式 增進你的 LLM" /><meta name="author" content="ChiChieh Huang" /><meta property="og:locale" content="en" /><meta name="description" content="儘管 LLM + RAG 的能力已經令人驚嘆，但我們在使用 RAG 優化 LLM 的過程中，還是會遇到了許多挑戰與難題，包括但不限於檢索器返回不準確或不相關的資料，以及LLM基於錯誤或過時資訊生成答案的情況，因此本文旨在提出 RAG 常見的 7 大挑戰，與其各自的優化方案。" /><meta property="og:description" content="儘管 LLM + RAG 的能力已經令人驚嘆，但我們在使用 RAG 優化 LLM 的過程中，還是會遇到了許多挑戰與難題，包括但不限於檢索器返回不準確或不相關的資料，以及LLM基於錯誤或過時資訊生成答案的情況，因此本文旨在提出 RAG 常見的 7 大挑戰，與其各自的優化方案。" /><link rel="canonical" href="https://chichieh-huang.com/posts/0e4ac8adc6df/" /><meta property="og:url" content="https://chichieh-huang.com/posts/0e4ac8adc6df/" /><meta property="og:site_name" content="ChiChieh Huang" /><meta property="og:image" content="https://chichieh-huang.com/assets/0e4ac8adc6df/1*MSP2W2EhexEADFhA1_S8uw.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-03-07T18:35:47+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://chichieh-huang.com/assets/0e4ac8adc6df/1*MSP2W2EhexEADFhA1_S8uw.png" /><meta property="twitter:title" content="RAG 優化技巧 7 大挑戰與解決方式 增進你的 LLM" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"ChiChieh Huang","url":"https://medium.com/@cch.chichieh"},"dateModified":"2025-02-20T18:57:36+08:00","datePublished":"2024-03-07T18:35:47+08:00","description":"儘管 LLM + RAG 的能力已經令人驚嘆，但我們在使用 RAG 優化 LLM 的過程中，還是會遇到了許多挑戰與難題，包括但不限於檢索器返回不準確或不相關的資料，以及LLM基於錯誤或過時資訊生成答案的情況，因此本文旨在提出 RAG 常見的 7 大挑戰，與其各自的優化方案。","headline":"RAG 優化技巧 7 大挑戰與解決方式 增進你的 LLM","image":"https://chichieh-huang.com/assets/0e4ac8adc6df/1*MSP2W2EhexEADFhA1_S8uw.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://chichieh-huang.com/posts/0e4ac8adc6df/"},"url":"https://chichieh-huang.com/posts/0e4ac8adc6df/"}</script><title>RAG 優化技巧| 7 大挑戰與解決方式 | 增進你的 LLM | ChiChieh Huang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ChiChieh Huang"><meta name="application-name" content="ChiChieh Huang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">ChiChieh Huang</a><p class="site-subtitle fst-italic mb-0"><div class="medium-followers-container"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-link"> <span class="followers-count">880+ followers on&nbsp;</span> <svg class="medium-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z"/><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z"/><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z"/> </svg> <span>Medium</span> </a></div>Hi~ 我專注於 Generative AI 產品開發，熱愛桌球與奇幻小說，並希望透過中文內容與更多人分享 AI 知識，讓技術更貼近社群。</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/wsxqaza12" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="javascript:location.href = 'mailto:' + ['cch.chichieh','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>RAG 優化技巧| 7 大挑戰與解決方式 | 增進你的 LLM</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>RAG 優化技巧| 7 大挑戰與解決方式 | 增進你的 LLM</h1><p class="post-desc fw-light mb-4">儘管 LLM + RAG 的能力已經令人驚嘆，但我們在使用 RAG 優化 LLM 的過程中，還是會遇到了許多挑戰與難題，包括但不限於檢索器返回不準確或不相關的資料，以及LLM基於錯誤或過時資訊生成答案的情況，因此本文旨在提出 RAG 常見的 7 大挑戰，與其各自的優化方案。</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1709807747" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 7, 2024 </time> </span> <span> Updated <time data-ts="1740049056" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Feb 20, 2025 </time> </span><div class="mt-3 mb-3"> <a href="/assets/0e4ac8adc6df/1*MSP2W2EhexEADFhA1_S8uw.png" class="popup img-link preview-img shimmer"><img src="/assets/0e4ac8adc6df/1*MSP2W2EhexEADFhA1_S8uw.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://medium.com/@cch.chichieh">ChiChieh Huang</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="4242 words" > <em>23 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">RAG 優化技巧| 7 大挑戰與解決方式 | 增進你的 LLM</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">RAG 優化技巧| 7 大挑戰與解決方式 | 增進你的 LLM</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><h3 id="rag-優化技巧-7-大挑戰與解決方式--增進你的-llm"><span class="me-2">RAG 優化技巧| 7 大挑戰與解決方式 | 增進你的 LLM</span><a href="#rag-優化技巧-7-大挑戰與解決方式--增進你的-llm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>在當今快速演進的人工智慧領域中，大型語言模型（LLM）已經成為無處不在的技術，它們不僅改變了我們與機器交流的方式，還在各行各業中發揮著革命性的影響。</p><p>然而，儘管 LLM + RAG 的能力已經令人驚嘆，但我們在使用 RAG 優化 LLM 的過程中，還是會遇到了許多挑戰與難題，包括但不限於檢索器返回不準確或不相關的資料，以及LLM基於錯誤或過時資訊生成答案的情況，因此本文旨在提出 RAG 常見的 7 大挑戰，與其各自的優化方案，期望能夠幫助到你改善 RAG。</p><p>下圖展示了 RAG 系統的兩個主要流程：檢索和查詢，紅色方框代表過程中會遇到的挑戰，主要有7點：</p><ol><li>Missing Content: 缺失內容<li>Missed Top Ranked: 錯誤排序內容，導致正確答案沒有被成功 Retrieve<li>Not in Context: 上限文限制，導致正確答案沒有被採用<li>Wrong Format: 格式錯誤<li>Incomplete: 回答不全面<li>Not Extracted: 未能檢索資訊<li>Incorrect Specificity: 不適合的詳細回答</ol><p><a href="/assets/0e4ac8adc6df/1*MSP2W2EhexEADFhA1_S8uw.png" class="popup img-link shimmer"><img src="/assets/0e4ac8adc6df/1*MSP2W2EhexEADFhA1_S8uw.png" alt="Barnett _et al. (2024)_ Seven Failure Points When Engineering a Retrieval Augmented Generation System." loading="lazy"></a></p><p>Barnett <em>et al. (2024)</em> Seven Failure Points When Engineering a Retrieval Augmented Generation System.</p><p>這些挑戰不僅關係到系統的可用性和準確性，還直接影響到用戶對技術的信任度。為了解決這些問題，以下是針對每個挑戰的優化方案：</p><h3 id="1-missing-content缺失內容"><span class="me-2">1. Missing Content（缺失內容）</span><a href="#1-missing-content缺失內容" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>當 RAG 系統面對的問題無法從現有文件中得到答案時，就會出現這種情況。在最佳情況下，我們會希望 RAG 系統直接回答「我不知道」。然而，實務上RAG 系統常常會編造或錯誤回答問題。</p><p>針對這個問題，目前有兩大解決策略：</p><p><strong>1. 資料清理</strong></p><p>俗話說「garbage in garbage out」。原始資料品質對於資訊處理系統準確性的重要性，如果輸入資料錯誤或矛盾，或預處理步驟不當，那麼無論檢索增強生成（RAG）系統多麼先進，都無法從混亂資料中提取有價值的資訊。這意味著我們必須在資料來源選擇、資料清洗、預處理等環節投入資源和技術，以確保輸入資料盡可能的準確和一致。這個策略不僅適用於本文討論的問題，也適用於所有資料處理流程中，資料品質始終是關鍵。</p><p><strong>2. Prompt Engineering</strong></p><p>在知識庫缺乏相關訊息，導致系統可能給出看似合理但實際上錯誤的答案的情況下，使用 Prompt Engineering 是一個非常有幫助的解決方式。例如透過設定 Prompt：「如果你對答案不確定，就直接告訴我你不知道」，如此可以鼓勵模型採取更謹慎和誠實的回應態度，從而避免誤導用戶。雖然不能確保系統回答的絕對準確性，但透過這樣的 Prompt，確實能提高回答品質。</p><h3 id="2-missed-top-ranked錯誤排序內容"><span class="me-2">2. Missed Top Ranked（錯誤排序內容）</span><a href="#2-missed-top-ranked錯誤排序內容" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>這個挑戰主要的問題在於「答案在文件中，但因為排名靠前，而未能提供給用戶」。理論上，所有文件在檢索系統中都會被賦予一個排名，而這個排名會決定其在後續處理中的使用程度。但在實際操作中，由於性能和資源的限制，通常只有排名最高的前 K 個文檔會被選中並展示給用戶，這裡的 K 是一個基於性能考慮的參數。</p><p>針對這個問題，有兩種解決方式：</p><p><strong>1. 調整參數優化搜尋效果</strong></p><p>這部分提出兩個面向調整增加 RAG 的效率和準確性： <code class="language-plaintext highlighter-rouge">chunk_size</code> 和 <code class="language-plaintext highlighter-rouge">k</code></p><p>在 <code class="language-plaintext highlighter-rouge">chunk_size</code> 方面， <a href="../fced76fdb8b9/">之前文章</a> 中有介紹到，為了建立準確語意空間， <code class="language-plaintext highlighter-rouge">chunk_size</code> 非常重要，同時 <strong>Chunk optimization</strong> 也是 RAG 領域中很熱門的研究領域，常見的方法如 <strong>Sliding window technology, small2big</strong> 與 <strong>Abstract embedding</strong> 等。</p><p>如果想在 langchain 直接調整 chunk size，可以使用以下 code：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">all_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">PDF_data</span><span class="p">)</span>
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">k</code> 則涉及檢索器應該回復多少答案，我們可以選擇更多的回覆答案，來確保正確答案不會沒有被送給 LLM：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="n">retriever</span> <span class="o">=</span> <span class="n">vectordb</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">k</span><span class="sh">"</span><span class="p">:</span> <span class="mi">8</span><span class="p">})</span>

<span class="n">qa</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="p">.</span><span class="nf">from_chain_type</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> 
    <span class="n">chain_type</span><span class="o">=</span><span class="sh">"</span><span class="s">stuff</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">,</span> 
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</pre></table></code></div></div><p><strong>2. 優化檢索文件的排序</strong></p><p>在把檢索到的文件送到 LLM 前，先對文件進行最佳化排序，能大幅提升 RAG 系統的效能，因為初始排序無法反映件與查詢的真實相關性。這系列的論文可以看 <a href="https://arxiv.org/abs/2307.03172" target="_blank">Liu et al. 2023</a> ，論文中指出， <strong>將最相似的文檔放在開頭或結尾時，效能通常最高，因為模型容易迷失在中間</strong> 。</p><p>在 langchain 中，我們可以使用 langchain 原生的 Long-Context Reorder 或 Cohere Reranker 來實作，以下兩者都會示範：</p><p><strong>2.1 Long-Context Reorder</strong></p><p>可以參考 <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/long_context_reorder" target="_blank">官方文件</a> 。</p><div class="language-makefile highlighter-rouge"><div class="code-header"> <span data-label-text="Makefile"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="nv">retriever</span> <span class="o">=</span> vectordb.as_retriever<span class="o">(</span><span class="nv">search_kwargs</span><span class="o">={</span><span class="s2">"k"</span>: 8<span class="o">})</span>
<span class="nv">query</span> <span class="o">=</span> <span class="s2">"What can you tell me about the Celtics?"</span>

<span class="c"># Get relevant documents ordered by relevance score
</span><span class="nv">docs</span> <span class="o">=</span> retriever.get_relevant_documents<span class="o">(</span>query<span class="o">)</span>

<span class="c"># Reorder the documents:
# Less relevant document will be at the middle of the list and more
# relevant elements at beginning / end.
</span><span class="nv">reordering</span> <span class="o">=</span> LongContextReorder<span class="o">()</span>
<span class="nv">reordered_docs</span> <span class="o">=</span> reordering.transform_documents<span class="o">(</span>docs<span class="o">)</span>

<span class="c"># Confirm that the 4 relevant documents are at beginning and end.
</span><span class="err">reordered_docs</span>
</pre></table></code></div></div><p><strong>2.2 Cohere Reranker</strong></p><p>可以參考 <a href="https://python.langchain.com/docs/integrations/retrievers/cohere-reranker" target="_blank">官方文件</a> 。</p><div class="language-makefile highlighter-rouge"><div class="code-header"> <span data-label-text="Makefile"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="err">from</span> <span class="err">langchain.retrievers</span> <span class="err">import</span> <span class="err">ContextualCompressionRetriever</span>
<span class="err">from</span> <span class="err">langchain.retrievers.document_compressors</span> <span class="err">import</span> <span class="err">CohereRerank</span>
<span class="err">from</span> <span class="err">langchain_community.llms</span> <span class="err">import</span> <span class="err">Cohere</span>

<span class="nv">retriever</span> <span class="o">=</span> vectordb.as_retriever<span class="o">(</span><span class="nv">search_kwargs</span><span class="o">={</span><span class="s2">"k"</span>: 8<span class="o">})</span>
<span class="nv">query</span> <span class="o">=</span> <span class="s2">"What can you tell me about the Celtics?"</span>

<span class="c"># Get relevant documents ordered by relevance score
</span><span class="nv">docs</span> <span class="o">=</span> retriever.get_relevant_documents<span class="o">(</span>query<span class="o">)</span>

<span class="c"># Uses the Cohere rerank endpoint to rerank the returned results
</span><span class="nv">llm</span> <span class="o">=</span> Cohere<span class="o">(</span><span class="nv">temperature</span><span class="o">=</span>0<span class="o">)</span>
<span class="nv">compressor</span> <span class="o">=</span> CohereRerank<span class="o">()</span>
<span class="nv">compression_retriever</span> <span class="o">=</span> ContextualCompressionRetriever<span class="o">(</span>
    <span class="nv">base_compressor</span><span class="o">=</span>compressor, <span class="nv">base_retriever</span><span class="o">=</span>retriever
<span class="err">)</span>

<span class="nv">compressed_docs</span> <span class="o">=</span> compression_retriever.get_relevant_documents<span class="o">(</span>
    <span class="s2">"What did the president say about Ketanji Jackson Brown"</span>
<span class="err">)</span>
<span class="err">pretty_print_docs(compressed_docs)</span>
</pre></table></code></div></div><h3 id="3-not-in-context上下文限制"><span class="me-2">3. Not in Context（上下文限制）</span><a href="#3-not-in-context上下文限制" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>論文有提到：「答案所在的文檔雖從資料庫中檢索出來，但並未包含在生成答案的上下文中。」這種情況通常發生在返回的文檔太多，需透過一個整合過程來選取答案的情境。</p><p>為了解決這個問題，擴大上下文的處理範圍是一種方式，此外也建議可以嘗試以下方法：</p><p><strong>1. 調整檢索策略</strong></p><p>Langchain 中提供許多檢索的方法，確保我們在 RAG 中能拿到最符合問題的文件，詳細的列表可以 <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/" target="_blank">參考官網</a> ，其中包含：</p><ol><li>Vectorstore: 之前範例中提到的<li>ParentDocument<li>Multi Vector<li>Self Query<li>Contextual Compression<li>Time-Weighted Vectorstore<li>Multi-Query Retriever<li>Ensemble<li>Long-Context Reorder: 上一步驟有介紹</ol><p>這些策略為我們提供了一種靈活多樣的方式，能夠根據不同的檢索需求和應用場景進行調整，以此提升檢索過程中的準確性和效率。</p><p><strong>2. Finetune Embeddings</strong></p><p>針對特定任務對 Embedding model 進行 Finetune，是提升檢索準確性的有效方法。如果你的 Embedding model 是開源的，那可以使用 LlamaIndex 的功能來實做，對比 Langchian，LlamaIndex 是針對檢索資料進行最佳化的套件，LlamaIndex 在這塊提供了詳細的教學，Langchian 則沒有對應的功能。</p><p>下面示範如何設定一個微調框架、執行微調操作，並取得經過微調的模型，也可以參考 <a href="https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html" target="_blank">官網文件</a> 。</p><div class="language-makefile highlighter-rouge"><div class="code-header"> <span data-label-text="Makefile"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="nv">finetune_engine</span> <span class="o">=</span> SentenceTransformersFinetuneEngine<span class="o">(</span>
    <span class="err">train_dataset,</span>
    <span class="nv">model_id</span><span class="o">=</span><span class="s2">"BAAI/bge-small-en"</span>,
    <span class="nv">model_output_path</span><span class="o">=</span><span class="s2">"test_model"</span>,
    <span class="nv">val_dataset</span><span class="o">=</span>val_dataset,
<span class="err">)</span>

<span class="err">finetune_engine.finetune()</span>

<span class="nv">embed_model</span> <span class="o">=</span> finetune_engine.get_finetuned_model<span class="o">()</span>
</pre></table></code></div></div><h3 id="4-wrong-format-格式錯誤"><span class="me-2">4. Wrong Format （格式錯誤）</span><a href="#4-wrong-format-格式錯誤" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>當你使用 prompt 要求 LLM 以特定格式（如表格或清單）提取資訊，但卻被而被 LLM 忽略時，可以嘗試以下 3 種解決策略：</p><p><strong>1. 改進 prompt</strong></p><p>你可以採用以下策略來改進你的 prompt，解決這個問題：</p><p>A. 明確說明指令</p><p>B. 簡化請求並使用關鍵字</p><p>C. 提供範例</p><p>D. 採用迭代提示，提出後續問題</p><p><strong>2. Output Parsers</strong></p><p>Output Parsers 負責取得 LLM 的輸出，並將其轉換為更合適的格式，因此當你想使用 LLM 產生任何形式的結構化資料時，這非常有用。他主要是在以下方面幫助確保獲得期望的輸出：</p><p>A. 為任何提示/查詢提供格式化指令</p><p>B. 對大語言模型的輸出進行「解析」。</p><p>Langchain 有提供許多不同類型 Output Parsers 的串流接口，以下是示範 code，細節可以看 <a href="https://python.langchain.com/docs/modules/model_io/output_parsers/" target="_blank">官網文件</a> 。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">PydanticOutputParser</span>
<span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="n">langchain_core.pydantic_v1</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo-instruct</span><span class="sh">"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>


<span class="c1"># Define your desired data structure.
</span><span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">question to set up a joke</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">punchline</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">answer to resolve the joke</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># You can add custom validation logic easily with Pydantic.
</span>    <span class="nd">@validator</span><span class="p">(</span><span class="sh">"</span><span class="s">setup</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">question_ends_with_question_mark</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">field</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">field</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">?</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Badly formed question!</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">field</span>


<span class="c1"># Set up a parser + inject instructions into the prompt template.
</span><span class="n">parser</span> <span class="o">=</span> <span class="nc">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">Joke</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="sh">"</span><span class="s">Answer the user query.</span><span class="se">\n</span><span class="s">{format_instructions}</span><span class="se">\n</span><span class="s">{query}</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">format_instructions</span><span class="sh">"</span><span class="p">:</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()},</span>
<span class="p">)</span>

<span class="c1"># And a query intended to prompt a language model to populate the data structure.
</span><span class="n">prompt_and_model</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">prompt_and_model</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Tell me a joke.</span><span class="sh">"</span><span class="p">})</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>3. Pydantic parser</strong></p><p>Pydantic 是一個多功能框架，它能夠將輸入的文本字符串轉化為結構化的 Pydantic 物件。Langchain 有提供此功能，歸類在 Output Parsers 中，以下是示範 code，可以參考 <a href="https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic" target="_blank">官方文件</a> 。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="kn">from</span> <span class="n">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">PydanticOutputParser</span>
<span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="n">langchain_core.pydantic_v1</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Define your desired data structure.
</span><span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">question to set up a joke</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">punchline</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">answer to resolve the joke</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># You can add custom validation logic easily with Pydantic.
</span>    <span class="nd">@validator</span><span class="p">(</span><span class="sh">"</span><span class="s">setup</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">question_ends_with_question_mark</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">field</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">field</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">?</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Badly formed question!</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">field</span>


<span class="c1"># And a query intented to prompt a language model to populate the data structure.
</span><span class="n">joke_query</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Tell me a joke.</span><span class="sh">"</span>

<span class="c1"># Set up a parser + inject instructions into the prompt template.
</span><span class="n">parser</span> <span class="o">=</span> <span class="nc">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">Joke</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="sh">"</span><span class="s">Answer the user query.</span><span class="se">\n</span><span class="s">{format_instructions}</span><span class="se">\n</span><span class="s">{query}</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">format_instructions</span><span class="sh">"</span><span class="p">:</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()},</span>
<span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="n">parser</span>

<span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="n">joke_query</span><span class="p">})</span>
</pre></table></code></div></div><h3 id="5-incomplete回答不全面"><span class="me-2">5. Incomplete（回答不全面）</span><a href="#5-incomplete回答不全面" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>有時候 LLM 的回答並不完全錯誤，但會遺漏了一些細節。這些細節雖然在上下文中有所體現，但並未被充分呈現出來。例如，如果有人詢問「文檔A、B和C主要討論了哪些方面？」對於每個文檔分別提問可能會更加適合，這樣可以確保獲得更詳細的答案。</p><ol><li><strong>Query Transformations</strong></ol><p>提高 RAG 系統效能的一個策略是添加一層查詢理解層，也就是在實際進行檢索前，先進行一系列的 Query Rewriting。具體而言，我們可以採用以下四種轉換方法：</p><p>1.1 <strong>Routing</strong> ：在不改變原始查詢的基礎上，識別並導向相關的工具子集，並將這些工具確定為處理該查詢的首選。</p><p>1.2 <strong>Query Rewriting</strong> ：在保留選定工具的同時，透過多種方式重構查詢語句，以便跨相同的工具集進行應用。</p><p>1.3 <strong>Sub-Questions</strong> ：將原查詢拆解為若干個更小的問題，每個問題都針對特定的工具進行定向，這些工具是根據它們的元數據來選定的。</p><p>1.4 <strong>ReAct Agent Tool Picking</strong> ：根據原始查詢判斷最適用的工具，並為在該工具上運行而特別構造的查詢。</p><p>Llamaindex 有針對這個問題有整理成一系列功能方便操作，細節看 <a href="https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook.html" target="_blank">官方文件</a> ；Langchain 則大部分功能散落在 <a href="https://python.langchain.com/docs/templates/" target="_blank">Templates</a> 裡面，如 <a href="https://python.langchain.com/docs/templates/hyde" target="_blank">HyDE 實作</a> 與 paper內容實作等。以下示範使用 Langchain 進行 HyDE：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">LLMChain</span><span class="p">,</span> <span class="n">HypotheticalDocumentEmbedder</span>
<span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">base_embeddings</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">()</span>
<span class="n">llm</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">()</span>

<span class="c1"># Load with `web_search` prompt
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">HypotheticalDocumentEmbedder</span><span class="p">.</span><span class="nf">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">base_embeddings</span><span class="p">,</span> <span class="sh">"</span><span class="s">web_search</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Now we can use it as any embedding class!
</span><span class="n">result</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">embed_query</span><span class="p">(</span><span class="sh">"</span><span class="s">Where is the Taj Mahal?</span><span class="sh">"</span><span class="p">)</span>
</pre></table></code></div></div><h3 id="6-not-extracted-未能檢索資訊"><span class="me-2">6. Not Extracted （未能檢索資訊）</span><a href="#6-not-extracted-未能檢索資訊" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>當 RAG 系統面對眾多資訊時，往往難以準確提取出所需的答案，關鍵資訊的遺漏降低了回答的品質。研究顯示，這種情況通常發生在上下文中存在過多干擾或矛盾資訊時。</p><p>以下是針對這一問題提出的三種解決策略：</p><p><strong>1. 數據清洗</strong></p><p>數據的品質直接影響到檢索的效果，這個痛點再次突顯了優質數據的重要性。在責備你的 RAG 系統之前，確保你已經投入足夠的精力去清洗數據。</p><p><strong>2. 訊息壓縮</strong></p><p>提示信息壓縮技術在長上下文場景下，首次由 <a href="https://arxiv.org/abs/2310.06839" target="_blank">LongLLMLingua</a> 研究項目提出，並已在 LlamaIndex 中得到應用，相對 Langchain 的資源則較零散。現在，我們可以將 LongLLMLingua 作為節點後處理器來實施，這一步會在檢索後對上下文進行壓縮，然後再送入 LLM 處理。</p><p><a href="/assets/0e4ac8adc6df/1*Esj6fNb4jR5zhgdfz5cgnw.png" class="popup img-link shimmer"><img src="/assets/0e4ac8adc6df/1*Esj6fNb4jR5zhgdfz5cgnw.png" alt="Jiang et al. (2023) LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression." loading="lazy"></a></p><p>Jiang et al. (2023) LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression.</p><p>以下是在 LlamaIndex 中使用 LongLLMLingua 的示範，其他細節可以參考 <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LongLLMLingua.html#longllmlingua" target="_blank">官方文件</a> ：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">llama_index.query_engine</span> <span class="kn">import</span> <span class="n">RetrieverQueryEngine</span>
<span class="kn">from</span> <span class="n">llama_index.response_synthesizers</span> <span class="kn">import</span> <span class="n">CompactAndRefine</span>
<span class="kn">from</span> <span class="n">llama_index.postprocessor</span> <span class="kn">import</span> <span class="n">LongLLMLinguaPostprocessor</span>
<span class="kn">from</span> <span class="n">llama_index.schema</span> <span class="kn">import</span> <span class="n">QueryBundle</span>

<span class="n">node_postprocessor</span> <span class="o">=</span> <span class="nc">LongLLMLinguaPostprocessor</span><span class="p">(</span>
    <span class="n">instruction_str</span><span class="o">=</span><span class="sh">"</span><span class="s">Given the context, please answer the final question</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">target_token</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
    <span class="n">rank_method</span><span class="o">=</span><span class="sh">"</span><span class="s">longllmlingua</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">additional_compress_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">condition_compare</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">condition_in_question</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">after</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">context_budget</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">+100</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">reorder_context</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">sort</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># enable document reorder
</span>    <span class="p">},</span>
<span class="p">)</span>

<span class="n">retrieved_nodes</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">retrieve</span><span class="p">(</span><span class="n">query_str</span><span class="p">)</span>
<span class="n">synthesizer</span> <span class="o">=</span> <span class="nc">CompactAndRefine</span><span class="p">()</span>

<span class="c1">## outline steps in RetrieverQueryEngine for clarity:
## postprocess (compress), synthesize
</span><span class="n">new_retrieved_nodes</span> <span class="o">=</span> <span class="n">node_postprocessor</span><span class="p">.</span><span class="nf">postprocess_nodes</span><span class="p">(</span>
    <span class="n">retrieved_nodes</span><span class="p">,</span> <span class="n">query_bundle</span><span class="o">=</span><span class="nc">QueryBundle</span><span class="p">(</span><span class="n">query_str</span><span class="o">=</span><span class="n">query_str</span><span class="p">)</span>
<span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">n</span><span class="p">.</span><span class="nf">get_content</span><span class="p">()</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">new_retrieved_nodes</span><span class="p">]))</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">synthesizer</span><span class="p">.</span><span class="nf">synthesize</span><span class="p">(</span><span class="n">query_str</span><span class="p">,</span> <span class="n">new_retrieved_nodes</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>3. LongContextReorder</strong></p><p>這在第二個挑戰，Missed Top Ranked 中有提到，為了解決 LLM 在文件中間會有「迷失」的問題，它通過重新排序檢索到的節點來優化處理，特別適用於需要處理大量頂級結果的情形。細節示範可以參考上面的內文。</p><h3 id="7-incorrect-specificity不適合的詳細回答"><span class="me-2">7. Incorrect Specificity（不適合的詳細回答）</span><a href="#7-incorrect-specificity不適合的詳細回答" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>有時候，LLM 的回答會不夠詳細或具體，可能需要進行多次追問才能得到清楚的解答。這些答案或許過於籠統，無法有效滿足用戶的實際需求。</p><p>因此，我們需要採取更高級的檢索策略來尋找解決方案。</p><p>當你發現答案的詳細程度未達預期時，通過優化檢索策略，可以顯著提升資訊獲取的精確度。LlamaIndex 提供了許多高級檢索技巧，Langchain 在這方面的資源則較少，因此以下舉幾個 LlamaIndex 中，能夠有效緩解此類問題的高級檢索技巧：</p><ul><li><a href="https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever.html" target="_blank">Auto Merging Retriever</a><li><a href="https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever.html" target="_blank">Metadata Replacement + Node Sentence Window</a><li><a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/pdf_tables/recursive_retriever.html" target="_blank">Recursive Retriever</a></ul><h3 id="結論"><span class="me-2">結論</span><a href="#結論" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>在本文中，我們探討了使用 RAG 技術會遇到的七大挑戰，而對於每個挑戰，我們提出了具體的優化方案，旨在提升系統的準確性和用戶體驗。</p><ol><li><strong>Missing Content</strong> : 解決方案包括資料清理和 Prompt Engineering，確保輸入資料的品質和引導模型更加精確地回答問題。<li><strong>Missed Top Ranked:</strong> 可通過調整檢索參數和優化文件排序來解決，以確保最相關的資訊被呈現給用戶。<li><strong>Not in Context:</strong> 擴大處理範圍和調整檢索策略是關鍵，以包含更廣泛的相關信息。<li><strong>Wrong Format:</strong> 可以通過改進 prompt、使用 Output Parsers 和 Pydantic parser 來達成，這有助於以用戶期望的格式獲得信息。<li><strong>Incomplete:</strong> 可以通過 Query Transformations 來解決，確保對問題的全面理解和回答。<li><strong>Not Extracted:</strong> 數據清洗、訊息壓縮、和 LongContextReorder 是有效的解決策略。<li><strong>Incorrect Specificity:</strong> 可以通過更高級的檢索策略來解決，如 Auto Merging Retriever、Metadata Replacement 等技巧，進一步精細化信息檢索。</ol><p>綜合以上，通過針對 RAG 系統的挑戰進行細致的分析和優化，我們不僅可以提升LLM的準確性和可靠性，還可以大大提高用戶對技術的信任度和滿意度。希望這篇能幫助到你改善你的 RAG 系統。</p><div class="floating-medium-button"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-follow-button"> <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z" fill="currentColor" /><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z" fill="currentColor" /><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z" fill="currentColor" /> </svg> Follow me on Medium </a></div><hr /><p><a href="https://www.buymeacoffee.com/chichieh.huang" target="_blank" class="img-link shimmer" ><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/rag-%E6%AA%A2%E7%B4%A2%E5%A2%9E%E5%BC%B7%E7%94%9F%E6%88%90/">RAG (檢索增強生成)</a>, <a href="/categories/research/">Research</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/langchain/" class="post-tag no-text-decoration" >langchain</a> <a href="/tags/llamaindex/" class="post-tag no-text-decoration" >llamaindex</a> <a href="/tags/rag/" class="post-tag no-text-decoration" >rag</a> <a href="/tags/%E4%B8%AD%E6%96%87/" class="post-tag no-text-decoration" >中文</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RAG%20%E5%84%AA%E5%8C%96%E6%8A%80%E5%B7%A7%7C%207%20%E5%A4%A7%E6%8C%91%E6%88%B0%E8%88%87%E8%A7%A3%E6%B1%BA%E6%96%B9%E5%BC%8F%20%7C%20%E5%A2%9E%E9%80%B2%E4%BD%A0%E7%9A%84%20LLM%20-%20ChiChieh%20Huang&url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F0e4ac8adc6df%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RAG%20%E5%84%AA%E5%8C%96%E6%8A%80%E5%B7%A7%7C%207%20%E5%A4%A7%E6%8C%91%E6%88%B0%E8%88%87%E8%A7%A3%E6%B1%BA%E6%96%B9%E5%BC%8F%20%7C%20%E5%A2%9E%E9%80%B2%E4%BD%A0%E7%9A%84%20LLM%20-%20ChiChieh%20Huang&u=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F0e4ac8adc6df%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F0e4ac8adc6df%2F&text=RAG%20%E5%84%AA%E5%8C%96%E6%8A%80%E5%B7%A7%7C%207%20%E5%A4%A7%E6%8C%91%E6%88%B0%E8%88%87%E8%A7%A3%E6%B1%BA%E6%96%B9%E5%BC%8F%20%7C%20%E5%A2%9E%E9%80%B2%E4%BD%A0%E7%9A%84%20LLM%20-%20ChiChieh%20Huang" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/a1d263ce61b4/">Migrating Away From v0 | Switching to Cursor/Windsurf</a><li class="text-truncate lh-lg"> <a href="/posts/5a67f86311d3/">從 v0 搬家 | 改用Cursor/Windsurf 替代</a><li class="text-truncate lh-lg"> <a href="/posts/942b2f15bea4/">我們都在用 AI，但 AI 跟 AI 怎麼溝通？淺談 AI Agent 通訊協定</a><li class="text-truncate lh-lg"> <a href="/posts/a3476af62056/">OpenManus Tutorial: How to Build Your Custom AI Agent in 2025 (Beginner’s Guide)</a><li class="text-truncate lh-lg"> <a href="/posts/d30783070827/">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/d6838febf8c4/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1705777720" data-df="ll" > Jan 21, 2024 </time><h4 class="pt-0 my-2">RAG實作教學，LangChain + Llama2 |創造你的個人LLM</h4><div class="text-muted"><p>在這篇文章中，我們將帶你使用 LangChain + Llama2，一步一步架設自己的 RAG（Retrieval-Augmented Generation）的系統，讓你可以上傳自己的 PDF，並且詢問 LLM 關於 PDF 的訊息。</p></div></div></a></article><article class="col"> <a href="/posts/fced76fdb8b9/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1705428247" data-df="ll" > Jan 17, 2024 </time><h4 class="pt-0 my-2">RAG (Retrieval Augmented Generation): 為自然語言處理揭開新篇章</h4><div class="text-muted"><p>近期 RAG 的研究發展</p></div></div></a></article><article class="col"> <a href="/posts/c7d1dac2494e/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1706206252" data-df="ll" > Jan 26, 2024 </time><h4 class="pt-0 my-2">RAG實作教學，Streamlit+LangChain+Llama2</h4><div class="text-muted"><p>我們將重點放在如何使用 Streamlit 來建立一個視覺化的操作介面，以便 Demo 整個RAG（Retrieval-Augmented Generation）的工作流程。</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/e2d02dd25fd9/" class="btn btn-outline-primary" aria-label="Older" ><p>記憶體不夠? 來看 LLM 的壓縮技術</p></a> <a href="/posts/6ac4201a4cbe/" class="btn btn-outline-primary" aria-label="Newer" ><p>LLM 各種技巧 | Prompt Engineering 指南</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/wsxqaza12">ChiChieh Huang</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.3.0" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.<br/> Automatically sync posts from Medium with <a href="https://zhgchg.li/posts/en-medium-to-jekyll/" target="_blank">ZhgChg.Li</a>.<br/>Last updated: 2025-06-15 11:02:16 +08:00</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
