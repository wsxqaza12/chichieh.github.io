<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="深入探討 Reasoning models 與 Test-Time Compute DeepSeek-R1 的建構" /><meta name="author" content="ChiChieh Huang" /><meta property="og:locale" content="en" /><meta name="description" content="推理模型 (Reasoning models) 正迅速崛起，如 OpenAI-o1、DeepSeek-R1。本文將以淺顯易懂的語言，探討什麼是 Test-Time Compute 以及其與 Reasoning models 的關係，並以 DeepSeek-R1 為例。" /><meta property="og:description" content="推理模型 (Reasoning models) 正迅速崛起，如 OpenAI-o1、DeepSeek-R1。本文將以淺顯易懂的語言，探討什麼是 Test-Time Compute 以及其與 Reasoning models 的關係，並以 DeepSeek-R1 為例。" /><link rel="canonical" href="https://chichieh-huang.com/posts/73c870be4b10/" /><meta property="og:url" content="https://chichieh-huang.com/posts/73c870be4b10/" /><meta property="og:site_name" content="ChiChieh Huang" /><meta property="og:image" content="https://chichieh-huang.com/assets/73c870be4b10/1*2gAteNQXK0WxvfWTSgI6Rg.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-03-05T21:07:47+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://chichieh-huang.com/assets/73c870be4b10/1*2gAteNQXK0WxvfWTSgI6Rg.png" /><meta property="twitter:title" content="深入探討 Reasoning models 與 Test-Time Compute DeepSeek-R1 的建構" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"ChiChieh Huang","url":"https://medium.com/@cch.chichieh"},"dateModified":"2025-03-05T21:07:47+08:00","datePublished":"2025-03-05T21:07:47+08:00","description":"推理模型 (Reasoning models) 正迅速崛起，如 OpenAI-o1、DeepSeek-R1。本文將以淺顯易懂的語言，探討什麼是 Test-Time Compute 以及其與 Reasoning models 的關係，並以 DeepSeek-R1 為例。","headline":"深入探討 Reasoning models 與 Test-Time Compute DeepSeek-R1 的建構","image":"https://chichieh-huang.com/assets/73c870be4b10/1*2gAteNQXK0WxvfWTSgI6Rg.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://chichieh-huang.com/posts/73c870be4b10/"},"url":"https://chichieh-huang.com/posts/73c870be4b10/"}</script><title>深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構 | ChiChieh Huang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ChiChieh Huang"><meta name="application-name" content="ChiChieh Huang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">ChiChieh Huang</a><p class="site-subtitle fst-italic mb-0"><div class="medium-followers-container"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-link"> <span class="followers-count">880+ followers on&nbsp;</span> <svg class="medium-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z"/><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z"/><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z"/> </svg> <span>Medium</span> </a></div>Hi~ 我專注於 Generative AI 產品開發，熱愛桌球與奇幻小說，並希望透過中文內容與更多人分享 AI 知識，讓技術更貼近社群。</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/wsxqaza12" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a> <a href="javascript:location.href = 'mailto:' + ['cch.chichieh','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</h1><p class="post-desc fw-light mb-4">推理模型 (Reasoning models) 正迅速崛起，如 OpenAI-o1、DeepSeek-R1。本文將以淺顯易懂的語言，探討什麼是 Test-Time Compute 以及其與 Reasoning models 的關係，並以 DeepSeek-R1 為例。</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1741180067" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 5, 2025 </time> </span> <span> Updated <time data-ts="1741180067" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 5, 2025 </time> </span><div class="mt-3 mb-3"> <a href="/assets/73c870be4b10/1*2gAteNQXK0WxvfWTSgI6Rg.png" class="popup img-link preview-img shimmer"><img src="/assets/73c870be4b10/1*2gAteNQXK0WxvfWTSgI6Rg.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://medium.com/@cch.chichieh">ChiChieh Huang</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="6439 words" > <em>35 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><h3 id="深入探討-reasoning-models-與-test-time-compute--deepseek-r1-的建構"><span class="me-2">深入探討 Reasoning models 與 Test-Time Compute | DeepSeek-R1 的建構</span><a href="#深入探討-reasoning-models-與-test-time-compute--deepseek-r1-的建構" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/assets/73c870be4b10/1*2gAteNQXK0WxvfWTSgI6Rg.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/1*2gAteNQXK0WxvfWTSgI6Rg.png" alt="目前已知的推理模型 (Reasoning models)，調查範圍至2025 年 2 月 28 號" loading="lazy"></a></p><p>目前已知的推理模型 (Reasoning models)，調查範圍至2025 年 2 月 28 號</p><p>最近，一種全新類型的 LLM 正在迅速崛起 — — 推理模型 (Reasoning models)，如 OpenAI-o1、DeepSeek-R1 與 Alibaba QwQ 等。在這種 Reasoning models 出來之前，許多 AI 研究人員以及使用這些技術的人都更傾向於讓模型「立刻」產生輸出。然而，OpenAI 推出的 o1 模型所引入的「慢思考」 理念，徹底改變了這一切，o1 在程式測驗競賽的表現突出 (第 89 位)，也在 US math olympiad qualifier 排進前 500 名，甚至在物理、生物與化學等多項 benchmark 中，達到了超越博士級別的準確率。</p><p>自從這一突破出現之後，我們清楚地看到，當模型並不「匆忙」而是有時間「思考」逐步推理時 ，模型的推理能力有多麼強大。而這些都與一個很有意思的主題息息相關：Test-Time Compute。</p><p>因此，本文將以淺顯易懂的語言，探討什麼是 Test-Time Compute 以及其與 Reasoning models 的關係。我們會先從 Reasoning models 的起源與演進說起，並探討什麼是 Test-Time Compute；在了解這一切基礎後，我們會以 DeepSeek-R1 為例，探究其是如何應用 Test-Time Compute；緊接著討論 Test-time Compute 的效果與限制；最後則是展望這塊領域未來的發展發向。</p><h3 id="什麼是-reasoning-models-"><span class="me-2">什麼是 Reasoning models ?</span><a href="#什麼是-reasoning-models-" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Reasoning models 是一種新的 LM 分類，這種模型的特色在於將複雜的問題分解為更小、更易於管理的步驟，並透過明確的邏輯推理來解決，也成被稱為 Reasoning Language Models (RLMs) 或 Large Reasoning Models (LRMs) ( <a href="https://arxiv.org/abs/2501.11223" target="_blank">Besta et al. ,2025</a> )。</p><p>RLMs 與 一般 LM 的差異可以用 <strong>Kahneman 的暢銷書籍《快思慢想》(Thinking, Fast and Slow)</strong> 中的概念作比喻。書籍中提到人類有兩種認知模式系統：</p><ol><li>系統 1 思維 (System-1 thinking)：更快、更直覺<li>系統 2 思維 (System-2 thinking)：相對更為緩慢、深思熟慮且邏輯性強</ol><p><a href="/assets/73c870be4b10/0*Ux_wcZVJM16DMMY_.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/0*Ux_wcZVJM16DMMY_.png" alt="人類的兩種認知模式：System 1 thinking &amp; System 2 thinking" loading="lazy"></a></p><p>人類的兩種認知模式：System 1 thinking &amp; System 2 thinking</p><p>System 1 任務，一般的 LM 就能透過一些技巧輕鬆解決，世界各地每天使用 ChatGPT 的許多學童都可以證明這一點；然而 System 2 任務，如大學數學題，對 LM 來說就沒那麼簡單，為了解決數學應用題，模型需要將問題分解為多個推理步驟，而這就是我們 RLMs 所在做的事。</p><h3 id="reasoning-models-起源"><span class="me-2">Reasoning models 起源</span><a href="#reasoning-models-起源" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>在 LLM 展現出優異的 System 1 任務能力後，研究者開始探索其在 System 2 任務上的瓶頸與改進方法。其中， <a href="https://arxiv.org/abs/2201.11903" target="_blank">Wei et al. (2022)</a> 所提出的 Chain-of-Thought (CoT) 實驗成為一大突破，該研究發現，僅需在 Prompt 中加入簡單的指令，例如「讓我們一步步思考 (Let’s think step by step)」，即可促使 LLM 進行推理步驟，大幅提升其推理能力。隨後， <a href="https://arxiv.org/abs/2205.11916" target="_blank">Kojima et al. (2022)</a> 進一步驗證了這一現象，並推動了一系列相關研究的發展，如 ToT (Tree of Thoughts) 等更複雜的推理框架。</p><p>表面上，o1 所展現的「多思考」似乎與現有的 CoT 技術相似，但二者有著關鍵差異。CoT 這類技術鼓勵模型將推理過程和思路闡述出來，但推理過程的中間步驟並未經過驗證，或與其他可能性進行權重比較。因此，即使大部分步驟正確，若其中某一步出錯，也容易會導致最終答案錯誤。也就是說，這些方法僅是 Prompt 的層面讓 LLM 展現這個能力，並未真正讓模型本身學到推理的能力，換句話說，LLM 仍無法真正「內化」這種推理流程。</p><h3 id="reasoning-models-演進"><span class="me-2">Reasoning models 演進</span><a href="#reasoning-models-演進" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>目前建立 RLM 的技術基礎仍然 <strong>不透明且複雜</strong> ，同時也仍不清楚 OpenAI o1 的創新是否來自於模型本身，或還是仰賴外部系統。雖然 OpenAI o1 的技術還披著一層面紗，但不可否認的是它帶動了 RLM 演進，而這一切的發展都離不開以下三個關鍵的技術進步：</p><ol><li>基於 RL 的模型設計，如 AlphaZero<li>基於 LLM 和 Transformer 的模型進步，例如 GPT-4o<li>超級電腦、高效能運算 (HPC) 能力的不斷增長</ol><p><a href="/assets/73c870be4b10/1*nhRPBDxaFX-6nVd5DzV5og.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/1*nhRPBDxaFX-6nVd5DzV5og.png" alt="[Besta et al. (2025)](https://arxiv.org/abs/2501.11223){:target=&quot;_blank&quot;} 探討 RLM 的歷史與演進的三大要素" loading="lazy"></a></p><p><a href="https://arxiv.org/abs/2501.11223" target="_blank">Besta et al. (2025)</a> 探討 RLM 的歷史與演進的三大要素</p><p>有鑑於 OpenAI o1 的成功，有一系列研究試圖探討重現 OpenAI o1 的思考系統。如 <a href="https://arxiv.org/abs/2501.11223" target="_blank">Besta et al. (2025)</a> 與 <a href="https://arxiv.org/abs/2410.09671" target="_blank">Wang et al. (2024)</a> 皆提到，RLM 的設計可能結合了多種技術，如蒙地卡羅樹搜尋 (Monte Carlo Tree Search, MCTS) 或 Beam Search 這樣的搜索算法來提升決策品質，並利用強化學習 (Reinforcement Learning, RL) 進行調整。此外，Process-Based Supervision (PBS)、上述提到的 CoT 與 ToT，甚至檢索增強生成 (Retrieval-Augmented Generation, RAG) 也被認為是其核心機制之一。</p><p>而在開源領域 <a href="https://github.com/openreasoner/openr" target="_blank">OpenR (Wang et al., 2024)</a> 、 <a href="https://github.com/THUDM/ReST-MCTS?tab=readme-ov-file" target="_blank">Rest-MCTS (Zhang et al., 2024)</a> 、 <a href="https://github.com/GAIR-NLP/O1-Journey" target="_blank">Journey Learning (Qin et al., 2024)</a> 、 <a href="https://arxiv.org/abs/2410.02884" target="_blank">LLaMA-Berry (Zhang et al., 2024)</a> 也做了諸多嘗試，如以下是 Journey Learning 的研究歷程。</p><p><a href="/assets/73c870be4b10/0*cPHzQAg2b1po86PY.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/0*cPHzQAg2b1po86PY.png" alt="[Qin et al. (2024](https://github.com/GAIR-NLP/O1-Journey){:target=&quot;_blank&quot;} ) 探索 OpenAI o1 技術的研究歷程，發佈於 2024 年 10 月 8 日" loading="lazy"></a></p><p><a href="https://github.com/GAIR-NLP/O1-Journey" target="_blank">Qin et al. (2024</a> ) 探索 OpenAI o1 技術的研究歷程，發佈於 2024 年 10 月 8 日</p><h3 id="reasoning-models-與-deepseek-r1"><span class="me-2">Reasoning models 與 DeepSeek-R1</span><a href="#reasoning-models-與-deepseek-r1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>然而，以上這些方法均未能達到與 OpenAI o1 相當的推理效能。直到 DeepSeek-R1 的出現才打破了這個僵局，根據 DeepSeek 在其論文中所述 ( <a href="https://arxiv.org/abs/2501.12948" target="_blank">DeepSeek-AI et al., 2025</a> )，他們的理念明確地承襲 OpenAI o1 提倡的核心 — — 深度、逐步的推理過程在解決複雜任務時至關重要。而驅動他們的動力是：如何讓模型在推理階段更深入的「思考」，因此他們著手研究對 Test-Time Compute 的擴展。</p><p>其實回頭來看，OpenAI o1 在其技術報告 ( <a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank">OpenAI, 2024</a> ) 便首次引入了通過延長推理過程來進行 Test-Time Compute 的擴展，且該報告與 DeepMind 的研究 ( <a href="https://arxiv.org/abs/2408.03314" target="_blank">Snell et al., 2024</a> ) 皆顯示，傳統應用於訓練階段的 Scaling Law 也同樣適用於推理階段。這一點不僅在數學、物理與化學等 benchmark 上得到驗證，也使得模型在實際應用中更接近人類專家的思考模式。</p><p><a href="/assets/73c870be4b10/1*veenUtAeZ6DFTKYFe4DNUA.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/1*veenUtAeZ6DFTKYFe4DNUA.png" alt="o1 效能隨著 Train-Time compute 和 Test-Time Compute 而平穩提升 ( [OpenAI, 2024](https://openai.com/index/learning-to-reason-with-llms/){:target=&quot;_blank&quot;} )" loading="lazy"></a></p><p>o1 效能隨著 Train-Time compute 和 Test-Time Compute 而平穩提升 ( <a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank">OpenAI, 2024</a> )</p><p>DeepSeek 在模型架構與訓練方法上有自己的設計特色，其特別強調 RL 以及Test-Time Compute 的擴展。因此接下來我們隨著 DeepSeek 的步伐，來一步一步探討他們是如何搭建 RLM，但在這之前，我們得先來了解什麼是 Test-Time Compute。</p><h3 id="什麼是-test-time-compute-"><span class="me-2">什麼是 Test-Time Compute ?</span><a href="#什麼是-test-time-compute-" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Test-Time Compute 是指的是模型在推理階段所使用的計算量 — — 也就是模型在訓練完成後，為產生答案所投入的資源與時間。從根本上來講，Test-Time Compute 代表了在人工智慧系統中 <strong>分配計算資源方式的轉變</strong> 。</p><p>在 2024 年上半年之前，為了提升 LLM 的效能，開發者通常會增加以下三個面向的規模來提升性能，這也就是以前常說的 Train-Time compute。儘管這種模式已被證明非常有效，但隨著 pre-training 的模型越來越大，其所需的資源變得越來越昂貴，甚至已經出現需要花費數十億美元狀況。</p><ul><li>模型參數 (parameters)<li>資料集 (tokens)<li>計算量 (FLOPs)</ul><p>而就剛上文提到的，Train-Time compute 的 Scaling Law 也同樣適用於 Test-Time Compute，這種規律早在 <a href="https://arxiv.org/abs/2104.03113" target="_blank">Jones (2021)</a> 對棋盤遊戲研究就發現，Train-Time compute 的增加大致與 Test-Time Compute 的減少呈線性關係，且減少的倍數相同。</p><p><a href="/assets/73c870be4b10/1*2SIIlNycBVcAkBk_bazA-w.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/1*2SIIlNycBVcAkBk_bazA-w.png" alt="不同基準下 Train-Time compute 與 Test-Time Compute 的關係 ( [Jones, 2021](https://arxiv.org/abs/2104.03113){:target=&quot;_blank&quot;} )" loading="lazy"></a></p><p>不同基準下 Train-Time compute 與 Test-Time Compute 的關係 ( <a href="https://arxiv.org/abs/2104.03113" target="_blank">Jones, 2021</a> )</p><p>這個新的規律發現，讓開發者重新思考計算資源的分配，不再只關注於 pre-training 過程，而更注重推理階段的運用。在 Test-Time Compute 投入更多的資源，意味著 LLM 能進行更深層的推理與「思考」，這對打造能自行優化、自主處理開放式高強度推理與決策任務的代理 (agent) 來說至關重要。</p><h3 id="deepseek-r1-如何應用-test-time-compute-"><span class="me-2">DeepSeek-R1 如何應用 Test-Time Compute ?</span><a href="#deepseek-r1-如何應用-test-time-compute-" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>2025 年 1 月 20 日 DeepSeek 推出三種不同的模型 DeepSeek-R1-Zero、DeepSeek-R1 和 DeepSeek-R1-Distill，從這 <strong>三種模型的演進</strong> ，我們可以清楚的看到 DeepSeek 是如何建構 RLM。</p><p>以下是三種模型的簡介 ，由下圖可以很清楚看到整個流程是如何變化：</p><ol><li><strong>DeepSeek-R1-Zero</strong> ：純 RL 模型。<li><strong>DeepSeek-R1</strong> ：先使用少量多步驟推理的範例進行微調，再應用 RL。<li><strong>蒸餾 (Distillation)</strong> ：將DeepSeek-R1 的推理能力傳遞到較小規模的AI 模型中，讓它們也具備強大的推理表現。</ol><p><a href="/assets/73c870be4b10/0*LsCseumS39nkwxV-.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/0*LsCseumS39nkwxV-.png" alt="DeepSeek R1 技術報告中三種不同模型的開發過程 ( [Raschka, 2025](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms){:target=&quot;_blank&quot;} )" loading="lazy"></a></p><p>DeepSeek R1 技術報告中三種不同模型的開發過程 ( <a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" target="_blank">Raschka, 2025</a> )</p><h4 id="1-deepseek-r1-zero-從零開始的純-rl-模型"><span class="me-2"><strong>1. DeepSeek-R1-Zero:</strong> 從零開始的純 RL 模型</span><a href="#1-deepseek-r1-zero-從零開始的純-rl-模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>這個階段的模型完全依靠 RL 來學習推理技巧，沒有任何初始的監督式數據。它透過自身行為 (模型的回答) 獲得獎勵或懲罰，並透過持續試錯來掌握多步驟推理。<li>在推論時，DeepSeek-R1-Zero 會透過回顧先前的思路不斷自我演進，隨著推理步驟的增加而具備更深層的「反思」能力，如下圖所示，模型從最初的數百個推理 tokens 增加到數千數萬個，這一行為被稱為「自進化(self-evolution)」。而這些額外的推論 tokens 就意味著更多的運算量，也即所謂的 Test-Time Compute。<li>雖然 DeepSeek-R1-Zero 在數學與邏輯推理任務上取得了顯著效果，但因為沒有任何監督微調，仍存在輸出易 <strong>語言混雜、格式雜亂</strong> 等問題。<li>註：這邊 RL 採用的演算法為 Group Relative Policy Optimization (GRPO)，這是對 PPO (Proximal Policy Optimization) 的改進。GRPO 的特點是無需額外的價值函數模型，而是透過在一個組內比較動作並使用平均獎勵作為基線，從而降低訓練成本並提升表現。有興趣可以看 <a href="https://arxiv.org/html/2501.12948v1" target="_blank">DeepSeek-AI et al. (2025)</a> 的技術報告。</ul><p><a href="/assets/73c870be4b10/1*yOO5NK_OFIEreFHTXzwFaQ.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/1*yOO5NK_OFIEreFHTXzwFaQ.png" alt="DeepSeek-R1-Zero 的思考時間隨著 RL 訓練過程越長而持續提升" loading="lazy"></a></p><p>DeepSeek-R1-Zero 的思考時間隨著 RL 訓練過程越長而持續提升</p><h4 id="2-deepseek-r1多階段優化管線進一步強化推理"><span class="me-2"><strong>2. DeepSeek-R1：多階段優化管線，進一步強化推理</strong></span><a href="#2-deepseek-r1多階段優化管線進一步強化推理" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>由於 DeepSeek-R1-Zero 的輸出容易混雜語言且格式雜亂，因此 DeepSeek 在後續版本中引入了以下調整：</p><ul><li><strong>冷啟動微調 (Cold Start Fine-Tuning)</strong> ：首先利用少量高品質的 CoT 範例和可讀性高的數據來讓模型「暖身」。這一步讓模型在推理時可更快進入深度推理，而不會像從零開始一樣容易發散或失控，並為後續的多步推理奠定了基礎。<li><strong>面向推理的強化學習 (Reasoning-Oriented RL)</strong> ：接著針對數學、程式碼等需要多步推理的領域進行 RL 訓練，同時引入「語言一致性獎勵」，懲罰混雜語言、鼓勵可讀性更高的人類可理解輸出。在推理時，因為模型能夠在遇到複雜問題時花更多時間做多輪思考和檢查，便自然需要更多 Test-Time Compute。<li><strong>拒絕採樣 + 監督微調 (Rejection Sampling + SFT)</strong> ：在模型經過一輪 RL 並收斂後，使用其生成的大量數據進行拒絕採樣，保留優質樣本來微調模型，擴展到其他應用領域。這一步也讓模型在推理階段對不同任務都能調整思考步驟。<li><strong>第二階段 RL (Second RL Phase)</strong> ：在完成上述微調後，再進一步用 RL 進行對齊與性能提升，使模型對於使用者指令更友好、推理更精準。</ul><h4 id="3-蒸餾--distillation-將龐大模型的推理能力傳遞給更小模型"><span class="me-2">3. 蒸餾 ( <strong>Distillation)</strong> ：將龐大模型的推理能力傳遞給更小模型</span><a href="#3-蒸餾--distillation-將龐大模型的推理能力傳遞給更小模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>由於 DeepSeek-R1 擁有 <strong>671B</strong> 參數，運行成本與硬體門檻十分高。為了讓更多人能使用其強大的推理能力，研究人員也將 DeepSeek-R1 作為「教師模型」，對更小規模的「學生模型」進行 Distillation。</p><ol><li><strong>蒸餾訓練資料：80 萬高品質樣本</strong></ol><ul><li>研究團隊收集了總數約 80 萬條高品質示例 (其中 60 萬個包含 Chain-of-Thought 推理步驟，另外 20 萬則是一般問答)。<li>學生模型與教師模型在面對相同提示時，教師先輸出一組機率分佈，學生則學習去模擬、逼近該分佈。</ul><p><strong>2. 學生模型學到的，不僅是輸出內容，還有「思路」</strong></p><ul><li>透過比較自己的輸出與教師模型輸出的機率分佈差異，學生模型漸漸掌握 DeepSeek-R1 所展示的推理模式。<li>這讓最終蒸餾出來的 Qwen-32B 等小模型，能在各種推理任務中展現高準確率，同時可在消費級硬體上運行，大幅降低了部署門檻。</ul><p>總結來說，從 DeepSeek-R1-Zero 的驗證性研究開始，到 DeepSeek-R1 在推理階段充分運用多步思考，再到將大型模型的推理思維「蒸餾」給更小規模模型，這一系列工作都圍繞著一個核心理念 — — <strong>在推理階段進行更深、更靈活的思考，即 Test-Time Compute</strong> 。這些額外的推理步驟與運算資源，使得 DeepSeek-R1 能在面對複雜問題時展現出強大的解題能力與高品質輸出，同時也能透過蒸餾技術擴散其成果，讓更多開發者與用戶也能享受到高階推理技術。</p><p>在這個過程中，Test-Time Compute 不僅是單純延長推理 tokens，而是讓模型在推理階段有更多時間與資源進行反覆檢查、反思和優化其解題過程。這使得模型能夠在不同階段動態調整策略，充分發揮其潛在的推理能力，進而能在多個 benchmark 上取得優異成績。</p><h3 id="deepseek-失敗的嘗試"><span class="me-2">DeepSeek 失敗的嘗試</span><a href="#deepseek-失敗的嘗試" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>然而，DeepSeek 在追求更深層次的 Test-Time Compute 旅程上也並非一帆風順。還記得在上面「Reasoning models 演進」的篇幅中，我們提到 RLM 的設計可能結合了多種技術，如蒙地卡羅樹搜尋 (MCTS)、Beam Search、RL 與 PRM 等，這些技術大部分也是 Test-Time Compute 應用的一環。而在 DeepSeek 的早期的研究階段，也嘗試使用過這些技術來訓練模型的推理能力，以下是在論文提到的兩個不成功的嘗試：</p><ol><li><strong>Process Reward Model (PRM)</strong> ：理論上能夠逐步評估每個推理過程的正確性，從而找到更好的解決方案，但他們發現難以對推理過程中每個細微步驟進行精確評估，因為一般推理問題中的中間步驟往往缺乏明確的標準，且容易導致模型依賴而出現 Reward hacking 現象。此外，這種方法在實際應用中需要大量人工標註，無法有效擴展。<li><strong>MCTS：</strong> 雖然該方法在棋類遊戲中取得了成功，但 LLM 的 token 生成的解空間呈指數級擴張，即使設置了節點延伸的最大限制，模型仍容易陷入局部最優解。因此，儘管 MCTS 在某些情境下能夠改善推理時的探索效率，但在實際大規模的 RL 訓練中，效果並不穩定。</ol><p>根據 DeepSeek 學到的經驗我們可知，這些複雜的 Test-Time Compute 方法雖然在理論上充滿吸引力，但在實際應用中，往往會因計算資源巨大、過於複雜，或不易收斂等因素而未能達到預期效果。也正是因為如此，DeepSeek-R1 的構建並未採用上述方法，而是選擇了更直接的方式訓練模型的推理能力。</p><p>註：DeepSeek 還有許多創新技術，這邊便不贅述。</p><h3 id="test-time-compute-在小模型的效果"><span class="me-2">Test-time Compute 在小模型的效果</span><a href="#test-time-compute-在小模型的效果" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>在前文中，我們已經看到 DeepSeek-R1 能夠通過蒸餾技術，將一個 671B 的大模型的推理技術交給更小的模型，而除了蒸餾技術外，也有許多研究證明，在數學與程式的 benchmark 中，單純擴展的 Test-time Compute，便能使小模型擁有打敗大模型的能力，如 <a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute" target="_blank">Beeching et al. (2024)</a> 在 HF 上的文章便提到，Llama-3.2 3B 在經過 256 次迭代後，其表現竟可以超越了參數量超過 20 倍的 Llama-3.1 70B 模型；DeepMind 的研究 ( <a href="https://arxiv.org/abs/2408.03314" target="_blank">Snell et al., 2024</a> ) 也指出PaLM 2-S small 模型在推理時加入額外算力，其表現可以超過參數量大 14 倍的模型； <a href="https://arxiv.org/abs/2407.21787" target="_blank">Brown et al. (2024)</a> 採用 Repeated Sampling 策略，讓模型對同一問題產生多個答案再挑選最佳，結果顯示一個中型模型單次推理只能解出 15.9% 的問題，但允許其嘗試 250 次後，解題率飆升至 56%，超越了當時單次推理的最高分數 43%。</p><p><a href="/assets/73c870be4b10/1*R22Ymhk78o0Dxc2plwCDdQ.png" class="popup img-link shimmer"><img src="/assets/73c870be4b10/1*R22Ymhk78o0Dxc2plwCDdQ.png" alt="[Beeching et al. (2024)](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute){:target=&quot;_blank&quot;} 小模型 Llama-3.2 1B 與 3B 在不同迭代次數下對比 Llama-3.1 70B 的表現" loading="lazy"></a></p><p><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute" target="_blank">Beeching et al. (2024)</a> 小模型 Llama-3.2 1B 與 3B 在不同迭代次數下對比 Llama-3.1 70B 的表現</p><p>以上研究皆顯示，擴展 Test-time Compute 讓小模型擁有更多「思考」，可以有效地激發小模型的潛力，甚至讓其在某些領域超越大模型。那麼這樣的效果讓我們不禁開始思考：這是否意味著，為了獲得更優的模型，我們應始終讓模型「思考」更多，而不是用更大規模、更多數據去 pre-training 模型呢？</p><h3 id="test-time-compute-的限制"><span class="me-2">Test-time Compute 的限制</span><a href="#test-time-compute-的限制" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>我認為現階段是 <strong>互補而非完全替代</strong> ，雖然擴展 Test-time Compute 顯著提升了小模型的表現，但研究指出它無法完全取代大規模 pre-training。 <a href="https://arxiv.org/abs/2408.03314" target="_blank">Snell et al. (2024)</a> 發現到，對於極具挑戰性的難題，小模型即使用大量推理步驟改進答案，效果仍非常有限；此時只有增加模型參數，並投入更多 Train-Time compute 才能帶來明顯提升。換而言之， <strong>Test-time Compute 的前提是模型對任務有「非零的起點」</strong> — — 若模型對某類問題毫無頭緒，再多的推理嘗試也難以產生正確結果。</p><p><a href="https://arxiv.org/abs/2407.21787" target="_blank">Brown et al. (2024)</a> 也觀察到，當任務形式並不能自動驗證時(如非數學與程式任務)，多數投票這種選擇策略的效能，會在超過數百次重複後產生停滯，無法隨著更多 Test-time Compute 而持續提高。這些結果凸顯了 <strong>Train-Time compute 與 Test-time Compute 各有其不可替代的作用</strong> — — Train-Time compute 塑造了模型的基礎知識與能力，而 Test-time Compute 則在此基礎上進行延伸和強化。</p><h3 id="未來方向"><span class="me-2">未來方向</span><a href="#未來方向" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>2024 至今年，RLM 開始一個接一個出現，因此顯而易見，今後還會出現大量 RLM 以及與擴展 Test-time Compute 有關的新研究，根據現在的論文與研究，我整理了幾個未來可能發展的方向：</p><ol><li><strong>混合式訓練與 RLM 普及</strong> ：由上文我們可以看出，Test-time Compute 可以讓模型的能力在原有的基礎上更上一層樓，因此未來有望看到更多將 Train-Time compute 與 Test-time Compute 結合的混合策略，在資源有限的情況下最大程度的提升模型表現，並且 RLM 也會成為 LLM 的趨勢之一。<li><strong>動態調整推理計算：</strong> 畢竟並非所有問題都需要千字長的推理；簡單問題只需較短的思考，而複雜問題才需更深度的計算。有研究在嘗試讓模型學會決定應該啟用多少推理步驟，也就是讓模型先判斷題目難度，再選擇是否需要深度「思考」，避免資源浪費。有鑒於此，未來 RLM 的 API 可能會提供「推理預算」參數，或模型會根據難度動態調整推理步數，確保使用者只在必要時付出較高的運算費用。<li><strong>兼顧「快思」與「慢想」</strong> ：乍看與上面那一點有點像，不過這邊探討的是普通 LLM 與 RLM 的整合，未來應可建立兩階層模型系統的思路：能快速回答簡單查詢的LLM，以及面對困難問題時才啟用深度推理的 RLM。<li><strong>推理階段的「訓練」與自我提升</strong> ：更前瞻的研究試圖模糊 Train-Time compute 與 Test-time Compute 的界線，探討 <strong>Test-Time Training</strong> ，即模型在推理階段可以繼續針對該問題做小幅度的即時微調。也就是說，模型在部署之後權重仍有機會根據當下題目的特殊需求改變，類似考試時先臨時背誦資料再回答。不過目前這個領域尚不成熟。</ol><h3 id="參考文獻"><span class="me-2">參考文獻</span><a href="#參考文獻" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Beeching, Edward, and Tunstall, Lewis, and Rush, Sasha, “Scaling test-time compute with open models.”, 2024.</p><p>Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q.V., R’e, C., &amp; Mirhoseini, A. (2024) . Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. <em>ArXiv, abs/2407.21787</em> .</p><p>Besta, M., Barth, J., Schreiber, E., Kubíček, A., Catarino, A.C., Gerstenberger, R., Nyczyk, P., Iff, P., Li, Y., Houliston, S., Sternal, T., Copik, M., Kwa’sniewski, G., Muller, J., Flis, L., Eberhard, H., Niewiadomski, H., &amp; Hoefler, T. (2025) . Reasoning Language Models: A Blueprint.</p><p>DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z.F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R.J., Jin, R.L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S.S., Zhou, S., Wu, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W.L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X.Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y.K., Wang, Y.Q., Wei, Y.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y.X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., &amp; Zhang, Z. (2025) . DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. <em>ArXiv, abs/2501.12948</em> .</p><p>Jones, A. (2021) . Scaling Scaling Laws with Board Games. <em>ArXiv, abs/2104.03113</em> .</p><p>Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., &amp; Iwasawa, Y. (2022) . Large Language Models are Zero-Shot Reasoners. <em>ArXiv, abs/2205.11916</em> .</p><p>OpenAI. (2024) . <em>Learning to reason with llms</em> . <a href="https://openai.com/index/learning-to-reason-with-llms/" target="_blank">https://openai.com/index/learning-to-reason-with-llms/</a></p><p>Plaat, A., Wong, A., Verberne, S., Broekens, J., Stein, N.V., &amp; Back, T.H. (2024) . Reasoning with Large Language Models, a Survey. <em>ArXiv, abs/2407.11511</em> .</p><p>Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., &amp; Liu, P. (2024) . O1 Replication Journey: A Strategic Progress Report — Part 1. <em>ArXiv, abs/2410.18982</em> .</p><p>Raschka, S. (2025) . <em>Understanding reasoning in LLMs</em> . Sebastian Raschka’s Newsletter. <a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms" target="_blank">https://magazine.sebastianraschka.com/p/understanding-reasoning-llms</a></p><p>Snell, C., Lee, J., Xu, K., &amp; Kumar, A. (2024) . Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. <em>ArXiv, abs/2408.03314</em> .</p><p>Se, K., &amp; Vert, A. (2025, February 6) . <em>What is test-time compute and how to scale it?</em> [Community Article] . Retrieved from <a href="https://huggingface.co/blog/Kseniase/testtimecompute" target="_blank">https://huggingface.co/blog/Kseniase/testtimecompute</a></p><p>Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., &amp; Sun, H. (2022) . Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. <em>Annual Meeting of the Association for Computational Linguistics</em> .</p><p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.H., Xia, F., Le, Q., &amp; Zhou, D. (2022) . Chain of Thought Prompting Elicits Reasoning in Large Language Models. <em>ArXiv, abs/2201.11903</em> .</p><p>Wang, J., Fang, M., Wan, Z., Wen, M., Zhu, J., Liu, A., Gong, Z., Song, Y., Chen, L., Ni, L.M., Yang, L., Wen, Y., &amp; Zhang, W. (2024) . OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models. <em>ArXiv, abs/2410.09671</em> .</p><p>Wang, J. (2025) . A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1.</p><p>Zhang, D., Zhoubian, S., Yue, Y., Dong, Y., &amp; Tang, J. (2024) . ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search. <em>ArXiv, abs/2406.03816</em> .</p><p>Zhang, D., Wu, J., Lei, J., Che, T., Li, J., Xie, T., Huang, X., Zhang, S., Pavone, M., Li, Y., Ouyang, W., &amp; Zhou, D. (2024) . LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning. <em>ArXiv, abs/2410.02884</em> .</p><div class="floating-medium-button"> <a href="https://medium.com/@cch.chichieh" target="_blank" class="medium-follow-button"> <svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M13 12C13 15.3137 10.3137 18 7 18C3.68629 18 1 15.3137 1 12C1 8.68629 3.68629 6 7 6C10.3137 6 13 8.68629 13 12Z" fill="currentColor" /><path d="M23 12C23 14.7614 22.5523 17 22 17C21.4477 17 21 14.7614 21 12C21 9.23858 21.4477 7 22 7C22.5523 7 23 9.23858 23 12Z" fill="currentColor" /><path d="M17 12C17 14.7614 16.5523 17 16 17C15.4477 17 15 14.7614 15 12C15 9.23858 15.4477 7 16 7C16.5523 7 17 9.23858 17 12Z" fill="currentColor" /> </svg> Follow me on Medium </a></div><hr /><p><a href="https://www.buymeacoffee.com/chichieh.huang" target="_blank" class="img-link shimmer" ><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/llm-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B/">LLM (大型語言模型)</a>, <a href="/categories/research/">Research</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/reasoning-model/" class="post-tag no-text-decoration" >reasoning-model</a> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >ai</a> <a href="/tags/research/" class="post-tag no-text-decoration" >research</a> <a href="/tags/test-time-compute/" class="post-tag no-text-decoration" >test-time-compute</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%A8%8E%20Reasoning%20models%20%E8%88%87%20Test-Time%20Compute%20%7C%20DeepSeek-R1%20%E7%9A%84%E5%BB%BA%E6%A7%8B%20-%20ChiChieh%20Huang&url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F73c870be4b10%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%A8%8E%20Reasoning%20models%20%E8%88%87%20Test-Time%20Compute%20%7C%20DeepSeek-R1%20%E7%9A%84%E5%BB%BA%E6%A7%8B%20-%20ChiChieh%20Huang&u=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F73c870be4b10%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fchichieh-huang.com%2Fposts%2F73c870be4b10%2F&text=%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%A8%8E%20Reasoning%20models%20%E8%88%87%20Test-Time%20Compute%20%7C%20DeepSeek-R1%20%E7%9A%84%E5%BB%BA%E6%A7%8B%20-%20ChiChieh%20Huang" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/a1d263ce61b4/">Migrating Away From v0 | Switching to Cursor/Windsurf</a><li class="text-truncate lh-lg"> <a href="/posts/5a67f86311d3/">從 v0 搬家 | 改用Cursor/Windsurf 替代</a><li class="text-truncate lh-lg"> <a href="/posts/942b2f15bea4/">我們都在用 AI，但 AI 跟 AI 怎麼溝通？淺談 AI Agent 通訊協定</a><li class="text-truncate lh-lg"> <a href="/posts/a3476af62056/">OpenManus Tutorial: How to Build Your Custom AI Agent in 2025 (Beginner’s Guide)</a><li class="text-truncate lh-lg"> <a href="/posts/d30783070827/">Understanding Reasoning Models & Test-Time Compute: Insights from DeepSeek-R1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/ab70d7117480/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1717580275" data-df="ll" > Jun 5, 2024 </time><h4 class="pt-0 my-2">深入解析 RAG 評估框架：TruLens, RGAR, 與 RAGAs 的比較</h4><div class="text-muted"><p>隨著 RAG 日益發展，有許多 RAG 的變形架構出現，使其成為一個越來越複雜的系統，需要全面性的評估方可監控其效能，提供後續的商業價值。因此，本文旨在探討我們如何全面且廣泛性的評估 RAG 系統，以及 RAG 評估框架的未來方向。</p></div></div></a></article><article class="col"> <a href="/posts/c9ac7835adb3/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1710784696" data-df="ll" > Mar 19, 2024 </time><h4 class="pt-0 my-2">LLM 規格比較 | 盤點 ChatGPT, Gemini, Claude, Mistral, llama 等模型</h4><div class="text-muted"><p>最近，Gemini 1.5 和 Claude 3 先後發布使的各種 LLM 的規格變得越來越複雜，因此我決定花時間來整理一份最新的規格比較表，其中包含 OpenAI、Google、Anthropic、Meta 以及 Mistral AI 的模型。</p></div></div></a></article><article class="col"> <a href="/posts/78f24809604f/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1739699407" data-df="ll" > Feb 16, 2025 </time><h4 class="pt-0 my-2">DeepSeek 本地部屬 | llama.cpp 與 ollama</h4><div class="text-muted"><p>DeepSeek 近期在 LLM 領域的發展炙手可熱，吸引了全球關注。在這篇文章我們使用較具彈性 llama.cpp，以及易於安裝 Ollama，一步一步帶大家示範如何快速在本地部屬 DeepSeek-r1 模型。</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/78f24809604f/" class="btn btn-outline-primary" aria-label="Older" ><p>DeepSeek 本地部屬 | llama.cpp 與 ollama</p></a> <a href="/posts/8918612ba642/" class="btn btn-outline-primary" aria-label="Newer" ><p>OpenManus 教學：讓通用型 AI Agent 走進大眾</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/wsxqaza12">ChiChieh Huang</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.3.0" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.<br/> Automatically sync posts from Medium with <a href="https://zhgchg.li/posts/en-medium-to-jekyll/" target="_blank">ZhgChg.Li</a>.<br/>Last updated: 2025-06-15 11:02:16 +08:00</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/%E4%B8%AD%E6%96%87/">中文</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">ai</a> <a class="post-tag btn btn-outline-primary" href="/tags/tutorial/">tutorial</a> <a class="post-tag btn btn-outline-primary" href="/tags/installation/">installation</a> <a class="post-tag btn btn-outline-primary" href="/tags/rag/">rag</a> <a class="post-tag btn btn-outline-primary" href="/tags/research/">research</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-agent/">ai-agent</a> <a class="post-tag btn btn-outline-primary" href="/tags/artificial-intelligence/">artificial-intelligence</a> <a class="post-tag btn btn-outline-primary" href="/tags/langchain/">langchain</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
