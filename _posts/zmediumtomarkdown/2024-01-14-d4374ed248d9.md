---
title: "建立屬於你的 LLM | Llama2 教學"
author: "ChiChieh Huang"
date: 2024-01-14T17:13:27.844+0000
last_modified_at: 2025-02-20T10:51:57.701+0000
categories: ["LLM (大型語言模型)", "Tutorial"]
tags: ["llm","llama-2","meta","中文","installation"]
description: "一步一步下載使用 Llama2 教學"
image:
  path: /assets/d4374ed248d9/1*Yl_KcB1GZsR35tCS4r9j1w.png
render_with_liquid: false
---

### 建立屬於你的 LLM \| Llama2, 3 教學


![](/assets/d4374ed248d9/1*Yl_KcB1GZsR35tCS4r9j1w.png)


在這篇文章中，我將分享如何設置和使用 Llama2, 3，基本要求是電腦的顯示卡 vRAM 有 28G 或至少 14G，如果沒有的話可以參考我 [llama\.cpp 教學](../2451807f8ba5/) 的文章
### 步驟1\. 到 meta 網站申請 license

到 [meta 網站](https://ai.meta.com/resources/models-and-libraries/llama-downloads/){:target="_blank"} 申請下載 Llama model，你可以同時申請下載 Llama 2, Llama Guard3 和 code Llama。一般會需要等 1~2 天的時間，但我最近的經驗是，申請後10分鐘內就收到了許可。


![](/assets/d4374ed248d9/1*00a2mTMpL0UgFCWjJMIsWg.png)

### 步驟2\. 收到 meta 的商業許可

完成申請後，你將收到一封帶有商業許可的電子郵件。


![](/assets/d4374ed248d9/1*gbDX_1FpNLTS_tBeFR66NA.png)

### 步驟3\. 下載模型

我使用 github 的方式下載，另外 meta 還有提供 [huggingface](https://huggingface.co/meta-llama){:target="_blank"} 的方式下載，huggingface 提供的模型比 github 多了一種帶有 `-hf` 的 model，代表已轉換為 Hugging Face 檢查點。

如果你也是使用 github 的方式下載，可以跟著我下面的步驟執行：
### a\) \. clone 專案

首先去 [llama2 的 github](https://github.com/meta-llama/llama){:target="_blank"} 或是 [llama3 的 github](https://github.com/meta-llama/llama3){:target="_blank"} ，把專案 clone 下，以下示範為 llama2。


![](/assets/d4374ed248d9/1*Wrvswt3xljgNqIysXYC9qQ.png)

### b\) \. 執行 download\.sh

如果是使用的環境是 wins 子系統 WSL，那麼會遇到以下問題，研究後發現是沒有輸入 sudo 導致。


![](/assets/d4374ed248d9/1*rD16ErBQ5922l8wx8_oP5Q.png)

### c\) \. 輸入 email 中的 URL

接下來，將你在 Email 中收到的 URL 貼上（注意，示例中的 URL 已經過期）。


![meta 寄給你的信](/assets/d4374ed248d9/1*u9FI1Ro1jM1rTPvyHN4x2g.png)

meta 寄給你的信


![](/assets/d4374ed248d9/1*yJz512s0U66b80txqo65DQ.png)

### d\) \. 輸入你要下載的模型

我選取 7B 與 7B\-chat，輸入後便會自動下載。


![](/assets/d4374ed248d9/1*iyOIKnmBx4gNu1eCvbcnoQ.png)

### 步驟4\. 準備環境

接著要安裝 Llama2 所需要的 python 套件，你可以建立一個可以使用 cuda 的環境。我這邊使用 WSL2 的 conda 來建立：
```lua
conda create --name llama7B python=3.9b
conda activate llama7B
pip install -e .
```

注意：上述命令應在 clone 的資料夾中執行。
### 步驟5\. 快速測試

接著輸入以下指令，便可以快速測試你的 Llama2 是否可以正常使用。
- `— nproc_per_node 1` 代表模型平行 \(MP\) 值，不同模型有不同的 MP 值：7B\-1, 13B\-2, 70B\-8
- 你可以替換 example\_chat\_completion\.py 成你自己的 \.py
- `— ckpt_dir` 是你下載模型的資料夾

```css
torchrun --nproc_per_node 1 example_chat_completion.py \
    --ckpt_dir llama-2-7b-chat/ \
    --tokenizer_path tokenizer.model \
    --max_seq_len 512 --max_batch_size 6
```
### 支持鼓勵

如果文章對你有幫助，或願意鼓勵我持續創作，可以幫文章拍手，或點擊下方連結請我喝一杯咖啡，感謝支持\!


![](/assets/d4374ed248d9/1*QCQqlZr6doDP-cszzpaSpw.png)




_[Post](https://medium.com/@cch.chichieh/%E5%BB%BA%E7%AB%8B%E5%B1%AC%E6%96%BC%E4%BD%A0%E7%9A%84-llm-llama2-%E6%95%99%E5%AD%B8-d4374ed248d9){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
