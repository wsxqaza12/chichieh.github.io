---
title: "RAG (Retrieval Augmented Generation): 為自然語言處理揭開新篇章"
author: "ChiChieh Huang"
date: 2024-01-16T18:04:07.218+0000
last_modified_at: 2025-02-20T10:58:26.576+0000
categories: [""]
tags: ["rag","llm","research","中文","fine-tuning"]
description: "近期 RAG 的研究發展"
image:
  path: /assets/fced76fdb8b9/0*tSPdWAgLKlOIgTPB.png
render_with_liquid: false
---

### RAG \(Retrieval Augmented Generation\): 為自然語言處理揭開新篇章
### 一\. 為甚麼要用 RAG ?

如果使用 pretrain 好的 LLM 模型，應用在你個人的情境中，勢必會有些詞不達意的地方，例如問 LLM 你個人的訊息，那麼它會無法回答；這種情況在企業內部也是一樣，例如使用 LLM 來回答企業內部的規章條款等。

這種時候主要有三種方式來讓 LLM 變得更符合你的需求：
1. **Promt Enginerring** ：
輸入提示來指導 LLM 產生所需回應。例如常見的 In\-context Learning，透過在提示中提供上下文或範例，來形塑模型的回答方式。例如，提供特定回答風格的範例或包含相關的情境資訊，可以引導模型產生更合適的答案。
2. **Fine tuning：** 
這個過程包括在特定數據集上訓練 LLM，使其回應更符合特定需求。例如，一家公司可能會使用其內部文件 Fine tuning ChatGPT ，使其能夠更準確地回答關於企業內部規章條款等問題。然而，Fine tuning 需要代表性的數據集且量也有一定要求，且 Fine tuning 並不適合於在模型中增加全新的知識，或應對那些需要快速迭代新場景的情況。
3. **RAG \(Retrieval Augmented Generation\)** ：
結合了神經語言模型和擷取系統。擷取系統從資料庫或一組文件中提取相關信息，然後由語言模型使用這些信息來生成回應。可以把 RAG 想像成給模型提供一本教科書，讓它根據特定的問題去找資訊。此方法適用於模型需要整合即時、最新或非常特定的信息非常有用。但RAG 並不適合教會模型理解廣泛的領域或學習新的語言、格式或風格。



![RAG 與方法的比較
Gao, Yunfan et al\. “Retrieval\-Augmented Generation for Large Language Models: A Survey\.” \(2023\) \.](/assets/fced76fdb8b9/0*tSPdWAgLKlOIgTPB.png)

RAG 與方法的比較
Gao, Yunfan et al\. “Retrieval\-Augmented Generation for Large Language Models: A Survey\.” \(2023\) \.

目前的研究已經表明，RAG 在優化 LLM 方面，相較於其他方法具有顯著的優勢 \(Shuster et al\. , 2021 ; Yasunaga et al\. , 2022; Wang et al\. , 2023c; Borgeaud et al\. , 2022\)，主要的優勢可以體現在以下幾點：
1. RAG 透過外部知識來提高答案的 **準確性** ，有效地減少了虛假訊息，使得產生的回答更加準確可信。
2. 使用擷取技術能夠識別到最新的信息\(使用者提供\)，這使得 LLM 的回答能保持 **及時性** 。
3. RAG 引用資訊來源是使用者可以核實答案，因此其 **透明透** 非常高，這增強了人們對模型輸出結果的信任。
4. 透過擷取與特定領域資料，RAG 能夠為不同領域提供專業的知識支援， **客製化** 能力非常高。
5. 在 **安全性和隱私** 管理方面，RAG 透過資料庫來儲存知識，對資料使用有較好控制性。相較之下，經過 Fine tuning 的模型在管理資料存取權限方面不夠明確，容易外洩，這對於企業是一大問題。
6. 由於 RAG 不需更新模型參數，因此在處理大規模資料集時， **經濟效率** 方面更具優勢。


不過雖然 RAG 有許多優勢在，但這 3 種方法並不是互斥的，反而是相輔相成的。結合 RAG 和 Fine tuning ，甚至 Promt Enginerring 可以讓模型能力的層次性得增強。這種協同作用特別在特定情境下顯得重要，能夠將模型的效能推至最佳。整體過程可能需要經過多次迭代和調整，才能達到最佳的成效。這種迭代過程涵蓋了對模型的持續評估和改進，以滿足特定的應用需求。


![RAG 實際應用的過程
Gao, Yunfan et al\. “Retrieval\-Augmented Generation for Large Language Models: A Survey\.” \(2023\) \.](/assets/fced76fdb8b9/1*BRcERCALfqSoRC06ce5b2A.png)

RAG 實際應用的過程
Gao, Yunfan et al\. “Retrieval\-Augmented Generation for Large Language Models: A Survey\.” \(2023\) \.
### **二\. 什麼是 RAG ?**

這篇章旨在介紹 RAG 的過程與其使用的相關技術。總結來說 RAG 有兩個關鍵技術：擷取與生成
1. 擷取\(Retrieval\)：從大量知識庫中擷取出最相關的前幾個文件
2. 生成\(Generation\)：將擷取到的資訊轉成為自然流暢的文字。

### 擷取\(Retrieval\)

為了使 RAG 技術中的擷取更準確，關鍵在於如何獲得準確的語意空間、匹配查詢和文件的語意空間，以及如何使擷取器的輸出與大型語言模型的偏好相協調，我們在以下分別探討：
### a\. \) 如何獲得準確的語意空間

通常我們會把查詢和文件映射的多維空間，稱之為 **語意空間\(semantic space\)** ，而進行擷取時，我們是在語意空間中進行，因此若映射的不夠好，那麼對於整個 RAG 系統是個大災難。這邊介紹 2 步建立準確語意空間的方法。
1. **區塊優化 \(Chunk optimization\)**


在處理外部文檔時，首先需要將其拆分成更小的碎塊以提取細粒度特徵，然後將這些特徵嵌入\(embedded\)以表達其語義。然而，嵌入過大或過小的文本碎塊可能會導致較差的結果，因此需要考慮幾個重要的因素，例如擷取內容的性質、嵌入模型及其最佳塊大小、用戶查詢的預期長度和複雜性，以及具體應用對擷取結果的利用方式。因此目前有許多研究提出多種 **區塊優化** 方法：
1. **Sliding window technology** ：通過合併跨多個擷取過程的全球相關信息，實現分層擷取。
2. **small2big** ：它在初始搜索階段使用小文本塊，然後提供較大的相關文本塊供語言模型處理。
3. **Abstract embedding：** 優先考慮基於文檔摘要（或摘要）的前K次擷取，提供對整個文檔內容的全面理解。
4. **Metadata Filtering** ：技術透過文件的元資料進行過濾。
5. **Graph Indexing** ：將實體和關係轉化為節點和連接，顯著提高了相關性，特別是在多跳問題的情境下。


**2\. 微調嵌入模型** \( **Fine\-tuning Embedding Models\)**

確定了Chunk 的適當大小之後，我們需要透過一個嵌入模型（Embedding model）將 Chunk 和查詢嵌入到語意空間中。因此，嵌入模型是否能有效代表整個語料庫變得極為重要。傳統常用的嵌入模型有以下兩種：
- **Sentence\-Transformers：** 
是一種 python 的 BERT NLP 套件，提供各種訓練好的BERT 模型套件，適合單個句的處理。
- **text\-embedding\-ada\-002：** 
OpenAI 提出的新嵌入模型，適合處理包含256 或 512 Token 的文字區塊。


而如今，一些出色的嵌入模型已經問世，例如 AngIE\(Li and Li, 2023\)、Voyage\(VoyageAI, 2023\)、BGE\(BAAI, 2023\) 等，這些模型已經在大型模語料庫上 pre\-training。然而，當應用於特定領域時，它們準確捕捉領域特定信息的能力可能會受到限制。

此外我們還得對嵌入模型進行 Fine tuning，以確保模型能理解用戶查詢，這步對下游應用是必不可少的。嵌入模型 Fine tuning 的方法主要有 2 種：
1. **領域知識 Fine tuning：** 
嵌入模型的 Fine tuning 與 LLM 的 Fine tuning 不一樣，主要差別在嵌入模型的資料集包括查詢（Queries）、語料庫（Corpus）和相關文件（Relevant Docs）。資料集收集的過程非常繁雜，因此 LlamaIndex 專門為此引入了一系列關鍵的類別和函數，旨在簡化工作流程。
2. **針對下游任務 Fine tuning：** 
目前有研究使用 LLM 來 Fine tuning 嵌入模型。例如，PROMPTAGATOR \(Dai _et al\._ , 2022\)，如此可以解決一些由於數據不足而難以 Fine tuning 的問題。

### b\. \) 如何協調查詢和文件的語意空間?

在 RAG 應用中，有些擷取器用同一個嵌入模型來處理查詢和文檔，而有些則使用兩個不同的模型。此外，使用者的原始查詢可能表達不清晰或缺少必要的語義資訊。RAG 使用 2 種關鍵技術，來實現這個目標：
1. **Query Rewriting**


方法如 Query2Doc 和 ITER\-RETGEN 利用 LLMs，通過將原始查詢與額外的指導信息相結合，創建一個虛擬文檔\(Wang _et al\. ,_ 2023；Shao _et al\. ,_ 2023\)。其他還有如 HyDE \(Gao _et al\. ,_ 2022\)、RRR \(Ma _et al\. ,_ 2023\)、STEP\-BACKPROMPTING \(Zheng _et al\. ,_ 2022\)等。

2\. **Embedding Transformation**

相較於 Query Rewriting，這是一個更微觀的技術。LlamaIndex 在查詢編碼器後加入一個特殊的適配器，並對其進行微調，從而優化查詢的嵌入表示，使之更適合特定的任務。
### c\. \) 根據 LLM 的需求調整擷取結果

上述的方法旨在幫助我們提升擷取的效果，但是那些做法可能不能提升 LLM 模型的準確度，原因在於擷取的結果可能不是很符合 LLM 的偏好，因此如何讓擷取結果往 LLM 的偏好靠齊，是一個重要的領域。
1. **Fine\-tuning Retrievers**


核心概念是透過從 LLM 獲得的回饋訊號來調整嵌入模型，也就是使用 LLM 提供的分數來指導擷取器的訓練，這相當於用大語言模型來對資料集進行標註。近期的方法如 AAR \(Yu _et al\._ , 2023\)

2\. **Adapters**

上述的 Fine tuning 方法在實現上會有難度，如 API 的實現與算力的考量。因此，一些研究選擇外接 Adapters 來進行模型對齊。PRCA \(Yang _et al\._ , 2023\) 在上下文提取階段和獎勵驅動階段訓練適配器，並透過基於 Token 的autoregressiv 策略來優化擷取器的輸出。
### **生成\(Generation\)**

生成器是 RAG 的關鍵之一，它負責將擷取到的信息轉換為連貫和流暢的文本。在 RAG 中，生成器的輸入不僅包括典型的上下文信息，還包括通過檢索器獲得的相關文本片段。這種全面的輸入使生成器能夠深入理解問題的上下文，從而產生更具信息性和上下文相關性的回答。生成器會根據擷取到的文字來指導內容的生成，確保產生的內容與擷取資訊一致。

但擷取的內容五花八門，因此目前有一系列研究在探討生成器的適應性，讓其能應付來自查詢和文件的輸入資料。
### a\) \. 如何優化擷取到的訊息?

目前大宗做法是依賴無法調整但訓練完好的 LLM 模型\(如 ChatGPT\-4\)來生成，不過這些 LLM 仍然存在一些問題，例如上下文長度限制和對冗餘資訊的敏感度。為了解決這個問題，目前一些研究開始專注在 **擷取後處理\(post\-retrieval processing\)** ，這個作法是將經由擷取器得到的資訊去做進一步的處理、過濾或最佳化，提高擷取結果的品質，主要有 2 種主流作法：
1. **資訊壓縮 \(Information Compression\)**


即使擷取器能很棒的從龐大的知識庫中擷取到相關信息，但管理大量的擷取信息仍是一個挑戰。目前做法是擴大 LLM 的上下文長度來解決此問題，但這並不能很有效解決問題，上下文長度限制仍是一大問題。因此，壓縮資訊變得必要。資訊壓縮對減少雜訊、解決上下文長度限制和增強生成效果具有重要意義。

PRCA 透過訓練一個資訊擷取器來解決這個問題\(Yang _et al\._ , 2023\)。在上下文擷取階段，提供輸入文字 Sinput 時，它能夠產生一個輸出序列 Cextracted，代表從輸入文件中壓縮得到的上下文。 訓練過程旨在最小化 Cextracted 和實際上下文 Ctruth 之間的差異。

其他如 RECOMP \(Xu _et al\._ , 2023\) 或 Filter\-Ranker \(Ma _et al\._ , 2023\)。Filter\-Ranker 結合了大語言模型 \(LLM\) 和小語言模型 \(SLM\) 的優點。SLM 充當過濾器，LLM 則作為排序器。 研究表明，指導 LLM 重排 SLM 識別出的挑戰性樣本，能夠在各種資訊提取 \(IE\) 任務中帶來顯著的改進。

2\. **重排序 \(Reranking\)**

主要功能是優化擷取的信息集合。LLM 常常在引入額外上下文時面臨性能下降，而 Reranking 有效地解決了這一問題。其核心概念涉及重新排列文檔記錄，以使最相關的項目優先排在最前，從而限制文檔的總數量。這不僅解決了檢索過程中上下文窗口擴展的挑戰，而且還提升了檢索效率和響應速度。
### b\) \. 如何根據擷取到的訊息優化生成器?

在使用一般 LLM 時，輸入通常只會有一段文字查詢。但在 RAG 中，輸入不僅結合了查詢，還包括了擷取器擷取到的各種文檔，其中包含結構化與非結構化的資訊。這個額外的資訊會顯著影響 LLM 的理解，特別是對於較小的LLM。

在這種情況下，Fine tuning 模型以適應 「查詢\+擷取文檔」的輸入變得至關重要。RAG 中生成器的 Fine tuning 方法與 LLM 的一般微調方法基本一樣。接下來，我們將簡要描述一些涉及數據（格式化/非格式化）和優化功能的代表性工作。
1. **通用優化流程 \(General Optimization Process\)**


Self\-mem \(Cheng _et al\._ , 2023\) 採用了傳統訓練方法。其中給定輸入 x，擷取訊息 z，然後結合\(x, z\)，模型生成輸出 y。論文探討了兩種主流 Fine tuning方法，分別為 Joint\-Encoder \(Arora _et al\._ , 2023, Wang _et al\._ , 2022b, Lewis _et al\._ , 2020\) 和 Dual\-Encoder \(Xia _et al\._ , 2019, Cai _et al\._ , 2021, Cheng _et al\._ , 2022\)。

在 Joint\-Encoder 中，使用基於 encoder\-decoder 的標準模型。其中，encoder 首先對輸入進行編碼，decoder 透過注意力機制結合編碼結果，並以自回歸方式生成 Token。另一方面，在 Dual\-Encoder中，系統設置了兩個獨立的 encoder，每個 encoder 分別對輸入 \(查詢、上下文\) 和文檔進行編碼，之後將輸出依序經由 decoder，進行雙向交叉注意力處理。這兩種架構都利用 Transformer 作為基礎。

2\. **利用對比學習 \(Utilizing Contrastive Learning\)**

在為 LLM 準備訓練數據的階段中，通常會建立輸入和輸出的配對。這種傳統方法可能導致 「暴露偏差」，即模型僅在個別正確的輸出示例上進行訓練，接觸到單一的正確回饋，無法了解其他可能的生成 Token。這種限制可能會阻礙模型在真實世界中的表現，因為它可能過度適應訓練集中的特定示例，從而降低了其在不同上下文中泛化的能力。

為了緩解暴露偏差，SURGE \(Kang _et al\._ , 2023\) 提出使用圖文對比學習。這種方法包括一個對比學習目標，促使模型產生一系列可行且連貫的回應，擴展到訓練數據中遇到的實例之外。這種方法在減少過度適應和加強模型的泛化能力方面至關重要。
### 三\. RAG 生態系 ?


![](/assets/fced76fdb8b9/1*fbbUfWHObS2MWzbq6mIo7A.png)


RAG 的生態系蓬勃發展，在水平領域，從最初的文本問答領域以外，RAG 的應用逐漸拓展到更多模態資料，包括圖像、程式碼、結構化知識、影音等。在這些領域，已經湧現許多相關研究成果。

而相關的 Technical Stack 發展也滿迅速，如有名的 LangChain 和 LLamaIndex 這樣的關鍵工具，隨著 ChatGPT 的出現迅速獲得了人氣，它們提供了廣泛的與 RAG 相關的 API，並成為LLM 領域的佼佼者。

同時新興的 Technical Stack 也逐漸在發展，例如，Flowise AI10 優先考慮低代碼方法，使用者能夠透過簡單的拖曳界面部署包括 RAG 在內的 AI 應用。其他技術如 HayStack、Meltano11 和 Cohere Coral12 也因其在該領域的獨特貢獻而受到關注。

除了專注於 AI 的提供者之外，傳統軟件和雲服務提供者也在擴大其服務範圍，包括以 RAG 為中心的服務。如向量資料庫公司 Weaviate 推出的 Verba7 專注於個人助理應用，而亞馬遜 的 Kendra 提供智能企業搜索服務，允許用戶使用內置連接器在各種內容存儲庫中導航。
### 參考資料

Gao, Yunfan et al\. “Retrieval\-Augmented Generation for Large Language Models: A Survey\.” \(2023\) \.
### 支持鼓勵

如果文章對你有幫助，或願意鼓勵我持續創作，可以幫文章拍手，或點擊下方連結請我喝一杯咖啡，感謝支持\!


![](/assets/fced76fdb8b9/1*QCQqlZr6doDP-cszzpaSpw.png)




_[Post](https://medium.com/@cch.chichieh/rag-retrieval-augmented-generation-%E7%82%BA%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E6%8F%AD%E9%96%8B%E6%96%B0%E7%AF%87%E7%AB%A0-fced76fdb8b9){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
