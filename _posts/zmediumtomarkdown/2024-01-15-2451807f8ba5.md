---
title: "用手機就能跑 LLaMA 2! llama.cpp 教學"
author: "ChiChieh Huang"
date: 2024-01-15T17:07:54.589+0000
last_modified_at: 2025-02-20T10:58:49.152+0000
categories: [""]
tags: ["llama-2","llm","llama-cpp","chatbots","中文"]
description: "使用 llama.cpp 建立屬於你的 LLM"
image:
  path: /assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png
render_with_liquid: false
---

### 用筆電就能跑 LLaMA \! llama\.cpp 教學


![](/assets/2451807f8ba5/0*k7pC8E9erRanEQtu.png)



> llama\.cpp 更新非常多了，有興趣的朋友請參考我於 2025/2/17 寫的新 [文章](../78f24809604f/) 。 





隨著人工智能的快速發展，大型語言模型（LLM）如 Llama 2, 3 已成為技術前沿的熱點。然而，Llama 2 最小的模型有7B\(Llama 3 最小為 8B\)，需要 14G 左右的記憶體，這不是一般消費級顯卡跑得動的，因此目前有很多方法在研究如何減少其資源使用，例如 [llama\.cpp](https://github.com/ggerganov/llama.cpp){:target="_blank"} ，號稱可以樹莓派上進行推理； 還有 [pyllama](https://github.com/juncongmoo/pyllama){:target="_blank"} 等。經過處理，最低只用 4G 的 RAM 就可以推理。

這篇文章將向你展示如何使用 llama\.cpp 量化模型並架設 server，使用你專屬的 LLM。
### 步驟1\. clone llama\.cpp 專案

把 [llama\.cpp](https://github.com/ggerganov/llama.cpp){:target="_blank"} 的 repo clone 下來
```bash
git clone https://github.com/ggerganov/llama.cpp.git
```


![](/assets/2451807f8ba5/1*iPz_zGuFtUEFxtCs4P-eUQ.png)

### 步驟2\. 使用 Makefile 進行編譯

使用 `make` 對 llama\.cpp 專案進行編譯，過程中會生成很多檔案，如：
- \./llama\-cli：用於指令操作
- \./llama\-quantize：用於量化二進位檔案
- \./llama\-server：啟動伺服器



![](/assets/2451807f8ba5/1*0N8YWwej3vU7qe7lZIGS5Q.png)


上述的方式只能使用 CPU 運行，如果你要使用 GPU：
- Windows/Linux用戶：建議與BLAS（或cuBLAS如果有GPU）一起編譯，可以提高prompt處理速度

```bash
make GGML_CUDA=1 # LLAMA_CUBLAS 已棄用
```
- macOS用戶：無需額外操作，llama\.cpp已對ARM NEON做優化， 並且已自動啟用BLAS。 M系列晶片建議使用Metal啟用GPU推理，顯著提升速度。

```bash
LLAMA_METAL=1 make
```

— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —

如果 `make` 時遇到以下錯誤：
```bash
: not foundld-info.sh: 2:
: not foundld-info.sh: 4:
: not foundld-info.sh: 9:
scripts/build-info.sh: 31: Syntax error: end of file unexpected (expecting "then")
make: *** [Makefile:676: common/build-info.cpp] Error 2
```

這是由於檔案結尾字元不一樣導致，wins Git clone下載得到資料夾\(CRLF\)後，上傳至 linux 系統中\(LF\)，便會造成這種錯誤，可以使用以下方式修復：
```bash
sudo apt install dos2unix
find -type f -print0 | xargs -0 dos2unix
```
### 步驟4\. 準備環境

建議使用 conda 來管理
```lua
conda create --name llamaCpp python=3.9
conda activate llamaCpp
python3 -m pip install -r requirements.txt
```
### 步驟5\. 量化模型

使用 `convert.py` 來對模型進行量化。這邊示範如何把 Llama2\-chat 7B 量化：
### a\) \. 下載 meta Llama 27B chat

這邊可以參考我之前的 [文章](../d4374ed248d9/) ，你也可以依照你的需求下載 Llama 3。假設你照著我之前的做法做，那麼你會有一個 clone 下來的 llama 專案，並且裡面載了 llama\-2–7b\-chat/，看一下這個資料夾中有哪些檔案：


![](/assets/2451807f8ba5/1*QGecv1vIc47jxb08lERjMA.png)

### b\) \. 使用 convert\-hf\-to\-gguf\.py 轉換為 GGUF 格式

llama\.cpp 能夠執行的模型文件格式被稱為 GGUF 格式。可以使用 llama\.cpp 提供的 `convert-hf-to-gguf.py` 工具，來將 llama\-2–7b\-chat 模型轉換為 GGUF 格式。

`convert-hf-to-gguf.py` 已經在上面準備環境的步驟中安裝，因此只需要執行以下指令。

註：舊版 llama\.cpp 使用 `convert.py`
- 注意要使用 `— vocab-dir` 來指定 tokenizer\.model 的位置，如果沒有指定會遇到 Error `FileNotFoundError: spm tokenizer.model not found.`
- 使用 `--outtype` 參數來決定資料的型態，一般來說會自動偵測可以不指定。\!注意這不是量化
- 使用 `— outfile` 來指定轉換後模型的位置

```bash
cd llama.cpp
python3 convert-hf-to-gguf.py models/llama-2-7b-chat --vocab-dir ../llama --outtype fP16
 --outfile models/llama-2-7b-chat/llama-2-model.gguf
```

執行後我遇到一個 Error：
```vbnet
ValueError: The model's vocab size is set to -1 in params.json. Please update it manually. Maybe 32000?
```

這個問題需要改 llama\-2–7b\-chat/ 中的 `params.json` ，將 vocab\_size 從 \-1 改成 32000。
```yaml
{"dim": 4096, "multiple_of": 256, "n_heads": 32, "n_layers": 32, "norm_eps": 1e-06, "vocab_size": -1} # 原始
{"dim": 4096, "multiple_of": 256, "n_heads": 32, "n_layers": 32, "norm_eps": 1e-06, "vocab_size": 32000} # 更改
```

再執行一次：
```bash
python3 convert.py models/llama-2-7b-chat --vocab-dir ../llama --outtype fP16
 --outfile models/llama-2-7b-chat/llama-2-model.gguf
```

便會在 `— outfile` 中生成 llama\-2\-model\.gguf。
### c\) \. 使用 llama\-quantize 進行量化

使用 \./llama\-quantize 可以對模型進行量化，可以選擇的量化尺度有： 8\-Bit、6\-Bit 到 2\-Bit 都可以進行。使用方法如下：

註：舊版 llama\.cpp 使用 `quantize`
```bash
./llama-quantize models/llama-2-7b-chat/llama-2-model.gguf models/llama-2-7b-chat/llama-2_q4.gguf Q4_K
```

檔案大小由原始的12\.5 GB 變成 Q8: 6\.66 GB；Q4: 3\.79 GB。若不想要量化，網路上滿多已經幫你做好的量化模型，可以下載直接使用。

### 步驟6\. 載入模型

之後便可以使用 llama\.cpp 的 llama\-cli，來做一些最基礎的用法：

註：舊版 llama\.cpp 使用 `main`
```bash
./llama-cli -m models/llama-2-7b-chat/llama-2_q4.gguf --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.3
./llama-cli -m models/llama-2-7b-chat/llama-2_q4.gguf -n 256 --repeat_penalty 1.0 --color -i -r "User:" -f prompts/chat-with-bob.txt
```
- `-c` 控制上下文的長度，數值越大越能參考較長的對話歷史，default=512
- `-f` 指定prompt模板
- `-n` 控制回復產生的最大長度，default=128
- `-b` 控制batch size，default=512
- `-t` 控制線程數量，default=8
- `— repeat_penalty` 控制中對重複文字的懲罰力度
- `— temp` 溫度係數，數值越低迴復的隨機性越小，反之越大
- `— top_p` , `top_k` 控制解碼取樣的相關參數
- `-m` 指定模型路徑
- `-ngl` 指定 GPU 讀取層數



![](/assets/2451807f8ba5/1*glhfW0REtdNlEsgiR415uQ.png)

### 步驟7\. 伺服器 server

llama\.cpp 也提供架設 server 的功能，通過 HTTP API 來存取模型，只要使用 `./llama-server` 就可以快速架設，預設會在 `http://127.0.0.1:8080/` 開啟一個 LLM Service，參數的部分與 `.main/` 差不多，也可以進到 server 再調整。

註：舊版 llama\.cpp 使用 `server`
```bash
./llama-server -m models/llama-2-7b-chat/llama-2_q4.gguf -ngl 100 #使用 GPU 版
```


![](/assets/2451807f8ba5/1*aqeRgnAj_mdn0PAdXhO67Q.png)



![](/assets/2451807f8ba5/1*sCJeqhRELoCRb1pvtqJWIA.png)

### 支持鼓勵

如果文章對你有幫助，或願意鼓勵我持續創作，可以幫文章拍手，或點擊下方連結請我喝一杯咖啡，感謝支持\!


![](/assets/2451807f8ba5/1*QCQqlZr6doDP-cszzpaSpw.png)




_[Post](https://medium.com/@cch.chichieh/%E7%94%A8%E6%89%8B%E6%A9%9F%E5%B0%B1%E8%83%BD%E8%B7%91-llama-2-llama-cpp-%E6%95%99%E5%AD%B8-2451807f8ba5){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
