---
title: "LLM 各種技巧 | Prompt Engineering 指南"
author: "ChiChieh Huang"
date: 2024-03-10T17:45:04.337+0000
last_modified_at: 2025-02-20T10:57:20.138+0000
categories: ["LLM (大型語言模型)", "Research"]
tags: ["prompt","llm","paper","中文"]
description: "截止至今，關於 LLM 的優化與技巧層出不窮，幾乎每個月都有新的技術和方法論被提出，因此本篇主要是要介紹在各種不同情境下，LLM 的各種 Prompt Engineering (提示工程) 技巧，每篇都有附上論文連結與架構圖，方便你快速檢閱，希望能助幫你深入了解。"
image:
  path: /assets/6ac4201a4cbe/0*NiV5pUfgtxkHnPih.png
render_with_liquid: false
---

### LLM 各種技巧 \| Prompt Engineering 大總結 \| 指南

截止至今，關於 LLM 的優化與技巧層出不窮，幾乎每個月都有新的技術和方法論被提出，因此本篇主要是要介紹在各種不同情境下，LLM 的各種Prompt Engineering 技巧，每篇都有附上論文連結與架構圖，方便你快速檢閱，希望能助幫你深入了解 Prompt Engineering 領域的最新進展及其發展趨勢。

這篇介紹的大綱如下圖，參考 [Sahoo et al\. \(2024\)](https://arxiv.org/abs/2402.07927){:target="_blank"} 最新的研究成果，主要分成 12 個部分，並會對各部分的技術分別做介紹：


![Sahoo et al\. \(2024\) A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications\.](/assets/6ac4201a4cbe/0*NiV5pUfgtxkHnPih.png)

Sahoo et al\. \(2024\) A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications\.
### 1\. New Tasks Without Extensive Training
### 1\.1 Zero\-Shot Prompting

零樣本提示技術 \(Zero\-Shot Prompting\) 是 LLM 領域裡的一項重要創新。由 [Radford et al\. \(2019\)](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask){:target="_blank"} 提出，這技術使我們能夠在缺乏大規模專門訓練資料的情況下，通過巧妙設計的提示來引導模型執行新的任務。這意味著，模型接收到的是任務的描述，而不是針對該任務的具體訓練標籤或資料。這項技術依賴於模型本身的知識庫，它可以利用這些提示來對新的任務作出反應和預測。以下為範例：

Input:
```
Classify the text into neutral, negative or positive.
Text: I think the vacation is okay.
Sentiment:
```

Outpu:
```
Neutral
```
### 1\.2 Few\-Shot Prompting

Few\-Shot Prompting 是由 [Brown et al\. \(2020\)](https://arxiv.org/abs/2005.14165){:target="_blank"} 提出，與零樣本提示相比，它透過提供少數輸入輸出範，來幫助模型學習特定任務。論文中有描寫到，通過精選的高質量範例，能夠顯著提升模型在執行複雜任務時的表現，尤其是在完全沒有示例的情況下更為明顯。儘管如此，這種方法由於需要更多的輸入 token，可能會在處理長文本時遇到困難。此外，範例的挑選對於模型的最終表現至關重要，不恰當的範例選擇可能會導致模型學習到不精確或有偏見的信息。

Input:
```
A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses
the word whatpu is:
We were traveling in Africa and we saw these very cute whatpus.
To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses
the word farduddle is:
```

Outpu:
```
When we won the game, we all started to farduddle in celebration.
```
### 2\. Reasoning and Logic

在推理與邏輯領域，我們見證了多種創新技術的誕生，這些技術使 LLM 能夠進行更加深入和複雜的思考過程。技術如 Chain\-of\-Thought \(CoT\)、Automatic Chain\-of\-Thought \(Auto\-CoT\)、Self\-Consistency、Logical CoT等，都旨在促進模型以更結構化和邏輯性的方式處理信息，從而提高問題解決的準確性和深度。
### **2\.1 Chain\-of\-Thought \(CoT\) Prompting LLMs**

為了克服 LLM 在處理複雜推理任務方面的限制， [Wei et al\. \(2022\)](https://arxiv.org/abs/2201.11903){:target="_blank"} 提出了一種稱為 CoT 的創新方法。該技術通過引入一種特殊的提示策略，旨在促進模型進行更為連續和逐步的思考過程。相較於傳統的提示方法，連貫思考技術的主要貢獻在於能夠更有效地激發 LLM 產出結構化且深入考慮的回答。

通過一系列實驗，這一技術證明了其在促進模型執行邏輯推理中的獨特效用，特別是在使模型對問題進行更深層次理解的方面。例如，它能詳細描繪出解決複雜數學問題所需的邏輯步驟，這一過程非常類似於人類的解題思維。利用 CoT，研究者們在使用 PaLM 540B 模型進行的數學及常識推理測試中，達到了空前的準確率，高達90\.2%。


![Wei et al\. \(2022\) Chain\-of\-Thought Prompting Elicits Reasoning in Large Language Models\.](/assets/6ac4201a4cbe/1*t3DX50EEwM2Vxe8sHIWSxA.png)

Wei et al\. \(2022\) Chain\-of\-Thought Prompting Elicits Reasoning in Large Language Models\.
### 2\.2 Automatic Chain\-of\-Thought \(Auto\-CoT\) Prompting

建立手動的 CoT 範例雖然可以提高模型的推理能力，但這個過程既耗時又效率低下。為了解決這一問題， [Zhang et al\. \(2022\)](https://arxiv.org/abs/2210.03493){:target="_blank"} 提出了 Auto\-CoT 技術。這項技術能夠自動生成 「讓我們一步步來思考」式的提示，從而協助大型語言模型形成推理鏈。此技術尤其關注於避免單一推理鏈中可能發生的錯誤，通過多樣化的樣本生成來提升整體的穩定性。它能夠針對各種問題產生多個獨特的推理鏈，並將它們組合成一個終極範例集合。這種自動化和多樣化的樣本生成方法有效地降低了出錯率，提升了少樣本學習的效率，並避免了手工構建 CoT 的繁瑣工作。應用這種技術後，在使用 GPT\-3 進行的算術和符號推理任務測試中，相比於傳統的 CoT，準確率分別提高了1\.33%和1\.5%。


![Zhang et al\. \(2022\) Automatic Chain of Thought Prompting in Large Language Models\.](/assets/6ac4201a4cbe/1*RxFRDDE1N_8XZ6Eugn6_RA.png)

Zhang et al\. \(2022\) Automatic Chain of Thought Prompting in Large Language Models\.
### 2\.3 Self\-Consistency

[Wang et al\. \(2022\)](https://arxiv.org/abs/2203.11171){:target="_blank"} 提出了一種新型解碼策略\- \- Self\-Consistency，其目標在於「取代鏈式思考提示中使用的天真貪婪解碼」。Self\-Consistency 方法從語言模型的 decoder 中提取多條不同的推理路徑，從而生成多種可能的推理鏈。然後，通過綜合這些推理鏈來尋找最為一致的答案。此策略建立在一個觀點之上：那些需要深度分析的問題通常具有更多的推理路徑，從而增加找到正確答案的可能性。

將 Self\-Consistency 與 CoT 結合使用，在多個標準測試中都達到了明顯的準確率提升，如在 GSM8K 測試中提高了17\.9%，在 SVAMP 測試中提高了11\.0%，在 AQuA 測試中提高了12\.2%，在 StrategyQA 測試中提高了6\.4%，以及在 ARC 挑戰中提高了3\.9%。


![Wang et al\. \(2022\) Self\-Consistency Improves Chain of Thought Reasoning in Language Models\.](/assets/6ac4201a4cbe/1*DPh_WlWge_EIFp0S8qhlGQ.png)

Wang et al\. \(2022\) Self\-Consistency Improves Chain of Thought Reasoning in Language Models\.
### 2\.4 Logical Chain\-of\-Thought \(LogiCoT\) Prompting

對於 LLM 來說，具備進行邏輯推理的能力，是對於解答跨領域的複雜多步問題的重要關鍵。 [Zhao et al\. \(2023\)](https://arxiv.org/abs/2309.13339){:target="_blank"} 提出的 LogiCoT，與之前的逐步推理方法 \(例如CoT\) 相比，引入了一個全新的框架。該框架吸取了 symbolic logic 的精髓，以一種更加結構化和條理清晰的方式來增強推理過程。特別是，LogiCoT 採用了反證法這一策略，也就是通過證明某一推理步驟若導致矛盾則該步驟錯誤，從而來核查和糾正模型產生的推理步驟。這一「思考\-核驗\-修正」的循環流程，有效地降低了邏輯錯誤和不正確的假設。在Vicuna\-33b和 GPT\-4 的測試中，LogiCoT 對推理能力的提升顯著，相比傳統 CoT，在GSM8K 資料集上的準確率分別提升了 0\.16% 和 1\.42%，在 AQuA 資料集上則提升了 3\.15% 和 2\.75%。


![Zhao et al\. \(2023\) Enhancing Zero\-Shot Chain\-of\-Thought Reasoning in Large Language Models through Logic\.](/assets/6ac4201a4cbe/1*7-BGSDgg7Tqn6iMdJC4wGQ.png)

Zhao et al\. \(2023\) Enhancing Zero\-Shot Chain\-of\-Thought Reasoning in Large Language Models through Logic\.
### 2\.5 Chain\-of\-Symbol \(CoS\) Prompting

當面臨涉及複雜空間關係的任務時，LLM 經常遇到挑戰，部分原因是它們依賴於容易模糊且可能帶有偏見的自然語言。為了克服這一限制， [Hu et al\. \(2023\)](https://arxiv.org/abs/2305.10276){:target="_blank"} 提出了 CoS 的新方法。這種方法選擇不使用自然語言，而是采用簡化的符號作為提示，其優勢在於使提示變得更加清晰、簡潔，同時顯著提高了模型處理空間關係問題的能力，也使得模型的運作原理更易於被人理解。

然而，CoS 技術在可擴展性、適用範圍、與其他技術的整合，以及基於符號的推理解釋性方面，仍存在一定的挑戰。值得注意的是，使用 CoS 技術後，ChatGPT 在 Brick World 空間任務的準確率顯著提升，從 31\.8% 躍升至92\.6%。此外，在簡化提示的過程中，所需的符號數量也減少了高達65\.8%，這不僅提升了效率，而且保持了高準確性。


![Hu et al\. \(2023\) Chain\-of\-Symbol Prompting Elicits Planning in Large Langauge Models\.](/assets/6ac4201a4cbe/1*iGJvAbmOExRvgqRX1fUDdQ.png)

Hu et al\. \(2023\) Chain\-of\-Symbol Prompting Elicits Planning in Large Langauge Models\.
### 2\.6 Tree\-of\-Thoughts \(ToT\) Prompting

[Yao et al\. \(2023\)](https://arxiv.org/abs/2305.10601){:target="_blank"} 與 [Long \(2023\)](https://arxiv.org/abs/2305.08291){:target="_blank"} 提出了稱為 ToT 的新型提示框架，旨在增強模型在處理需要深度探索和前瞻性思考的複雜任務上的能力。ToT 在現有提示方法的基礎上作了進一步的擴展，通過創建一個包含中間推理步驟的樹狀結構來實現，這些步驟被稱作「思維」。每一「思維」代表著朝向最終答案前進的一系列連貫語言序列。這種結構讓語言模型能夠針對解決問題的進展，有目的地評估這些「思維」。ToT透過整合產生及評估「思維」的功能與搜索算法\(如寬度優先搜索或深度優先搜索\)，實現了對推理過程的系統性探索。這使得模型能在找到有潛力的解決方案時進行拓展，或在遇到錯誤時進行回溯。在「24點遊戲」這一任務上，ToT的效能尤為顯著，成功率高達74%，大幅超過傳統方法的4%。此外，在處理單詞級任務時，ToT也表現出色，其成功率達到60%，明顯高於傳統方法的16%。


![Yao et al\. \(2023\) Tree of Thoughts: Deliberate Problem Solving with Large Language Models\.](/assets/6ac4201a4cbe/1*-uqZQkXlDuYx84DD7Mzlmw.png)

Yao et al\. \(2023\) Tree of Thoughts: Deliberate Problem Solving with Large Language Models\.
### 2\.7 Graph\-of\-Thoughts \(GoT\) Prompting

我們的思考過程往往是非線性的，並非一步接一步地推進，這給基於傳統的 ToT 方法帶來了挑戰。針對這一點， [Yao et al\. \(2023\)](https://arxiv.org/abs/2305.16582){:target="_blank"} 提出了一種創新的「圖思維」\(GoT\) 提示方法。該方法通過構建思維圖譜來模擬人類大腦的非線性思考模式，使得在不同的思維路徑之間可以自由跳躍、回溯和整合資訊。這使得從多個角度進行思考成為可能，從而突破了傳統線性思維的局限。GoT的核心創新在於將推理過程視為一個有方向的圖結構，並通過靈活的模塊化設計來支持思維的多樣化轉換。這種方法不僅更加貼近人類的思考模式，還顯著增強了模型在處理複雜問題上的能力。實際應用中，GoT相比於傳統的連貫思考\(CoT\)提示，在多個任務上展現出顯著的效能提升。例如，在GSM8K資料集上，T5\-base和T5\-large模型的準確率分別提升了3\.41%和5\.08%。同時，在ScienceQA上，相較於最先進的多模態CoT方法，準確率分別增加了6\.63%和1\.09%。


![Yao et al\. \(2023\) Beyond Chain\-of\-Thought, Effective Graph\-of\-Thought Reasoning in Large Language Models\.](/assets/6ac4201a4cbe/1*w4EchJ6ConTlElM_JC2DfA.png)

Yao et al\. \(2023\) Beyond Chain\-of\-Thought, Effective Graph\-of\-Thought Reasoning in Large Language Models\.
### 2\.8 System 2 Attention \(S2A\) Prompting

在 LLM 的應用中，soft attention 有時會吸引不相關的信息，這可能會降低模型生成答案的準確度。為了克服這一挑戰， [Weston and Sukhbaatar \(2023\)](https://arxiv.org/abs/2311.11829){:target="_blank"} 提出了一種稱為 S2A 的創新方法。這種方法通過重構輸入的上下文，讓模型能夠集中於最關鍵的信息部分，從而顯著提高了信息處理的質量和回應的相關性。S2A 特別通過一個兩階段過程來改進注意力機制和提高回答質量 — — 首先是對上下文的重新生成，接著是在這個精煉的上下文上進行答案的生成。這個方法在包括事實性問答、長文本生成和解決數學問題等多個任務上進行了測試。在事實性問答任務中，S2A達到了80\.3%的高準確率，明顯提升了信息的準確性；而在長文本生成方面，它同樣提升了文本的客觀性，其得分達到3\.82分（滿分為5分）。


![Weston and Sukhbaatar \(2023\) System 2 Attention \(is something you might need too\) \.](/assets/6ac4201a4cbe/1*QABwiZ38wTsP8ylv8EuS8g.png)

Weston and Sukhbaatar \(2023\) System 2 Attention \(is something you might need too\) \.
### 2\.9 Thread of Thought \(ThoT\) Prompting

[Zhou et al\. \(2023\)](https://arxiv.org/abs/2311.08734){:target="_blank"} 提出的 ThoT，這是專為提高 LLM 在處理複雜情境下的推理能力而設計的技術。這一方法模仿人類的思考過程，通過將複雜的情境分解成更小、更易於管理的部分來逐步進行分析。它採用了一種雙階段策略，即首先對每一個小部分進行概括和審視，隨後進一步細化資訊以得出最終的答案。ThoT 的靈活性是其一大亮點，使其能夠作為一個多功能的「即插即用」組件，有效地提升了多種模型和提示技術的推理效率。在對問答和對話類資料集進行測試時，特別是在複雜的情境中，ThoT展現了顯著的效能提升，分別達到了47\.20%和17\.8%。


![Zhou et al\. \(2023\) Thread of Thought Unraveling Chaotic Contexts\.](/assets/6ac4201a4cbe/1*87Uf0z4Vv5MhEeFNifvk9Q.png)

Zhou et al\. \(2023\) Thread of Thought Unraveling Chaotic Contexts\.
### 2\.10 Chain\-of\-Table Prompting

傳統的方法如 CoT、PoT 和 ToT 在展示推理步驟時，多依賴於自由文本或程式碼形式，這在處理複雜表格資料時往往會遇到挑戰。針對這一問題， [Wang et al\. \(2024\)](https://arxiv.org/abs/2401.04398){:target="_blank"} 開發了一種創新的表格鏈式 \(Chain\-of\-Table\) 提示方法。該方法通過對表格進行逐步的 SQL/DataFrame 操作，實現了動態的表格推理過程，其中每一次的迭代都旨在改善中間結果，從而提升了 LLM 利用邏輯推理鏈進行預測的能力。值得注意的是，表格鏈式提示方法在 TabFact 和WikiTQ 這兩個標準的表格資料集上實現了顯著的效能提升，分別達到了8\.69% 和6\.72%。


![Wang et al\. \(2024\) Chain\-of\-Table: Evolving Tables in the Reasoning Chain for Table Understanding\.](/assets/6ac4201a4cbe/1*2pnsiQ6OU2wUIf1AnwTWNA.png)

Wang et al\. \(2024\) Chain\-of\-Table: Evolving Tables in the Reasoning Chain for Table Understanding\.
### 3\. Reduce Hallucination

減少幻覺現象是 LLM 的一個關鍵挑戰。技術如 Retrieval Augmented Generation \(RAG\)、ReAct Prompting、Chain\-of\-Verification \(CoVe\)等，都是為了減少 LLM 產生無依據或不準確輸出的情況。這些方法通過結合外部信息檢索、增強模型的自我檢查能力或引入額外的驗證步驟來實現。
### 3\.1 Retrieval Augmented Generation \(RAG\)

雖然 LLM 在文本生成領域已經取得了突破性的進展，但它們對有限且固定訓練資料的依賴，限制了它們在需要廣泛外部知識的任務上提供準確答案的能力。傳統的提示技術無法克服這一限制，而且需要進行成本高昂的模型重新訓練。面對這一挑戰， [Lewis et al\. \(2020\)](https://arxiv.org/abs/2005.11401){:target="_blank"} 提出了一種稱為 Retrieval Augmented Generation \(RAG\) 的創新方法，它通過將資訊檢索技術無縫融入提示過程中，提供了一個全新的解決方案。RAG 方法分析用戶的輸入，生成針對性的查詢，在一個預建的知識庫中檢索相關資訊，然後將檢索到的資訊片段整合進原始提示，為之增添背景上下文。這種方法不僅提升了答案的創新性和準確性，而且通過其靈活的特性，突破了傳統模型的局限，為那些依賴於最新知識的任務帶來了顯著的改進。在 ODQA 的標準測試中，RAG模型超越了 seq2seq 模型和特定任務的架構，其準確匹配得分在TriviaQA 資料集上達到了56\.8%，在 Natural Questions 資料集上達到了44\.5%。

RAG的詳細 [介紹](../fced76fdb8b9/) 與 [實作](../d6838febf8c4/) 可以參考我的其他文章。
### 3\.2 ReAct Prompting

與傳統研究將推理和行動視為獨立元素的方法不同， [Yao et al\. \(2022\)](https://arxiv.org/abs/2210.03629){:target="_blank"} 提出ReAct 技術，在賦予 LLM 生成推理的同時，也給予其採取行動的能力。這種一體化的方法促進了推理與行動之間更強的協同作用，使模型在面對突發事件時，能夠更加有效地擬定、跟蹤及更新其行動計劃。ReAct 技術已被運用於多種語言處理和決策任務中，並在效能上超越了當前的先進方法。特別是在問題解答 \(HotpotQA\) 和事實核查 \(Fever\) 任務中，ReAct 通過與Wikipedia API 交互，有效地應對了資訊的虛構與錯誤傳播問題，提供了更加清晰的解決方案路徑。在如 ALFWorld 和 WebShop 這樣的互動式決策任務中，ReAct 同樣展現了優異的表現，成功率分別達到 34% 和 10%，這些成績是在最小上下文範例輸入的條件下實現的。


![Yao et al\. \(2022\) ReAct: Synergizing Reasoning and Acting in Language Models\.](/assets/6ac4201a4cbe/1*5IFSuGyJMn-HugviFTmSDg.png)

Yao et al\. \(2022\) ReAct: Synergizing Reasoning and Acting in Language Models\.
### 3\.3 Chain\-of\-Verification \(CoVe\) Prompting

為了幻覺現象， [Dhuliawala et al\. \(2023\)](https://arxiv.org/abs/2309.11495){:target="_blank"} 提出了一種稱為 CoVe 的方法。這個方法主要有四個步驟：
1. 生成初步答案
2. 規劃驗證問題以檢驗工作
3. 獨立解答這些問題
4. 根據驗證的結果來修正初步答案


CoVe 模仿人類進行驗證的思維過程，提升了大語言模型輸出的一致性與準確性。在處理列表問題、問答和長文本生成等任務時，CoVe 有效降低了虛構資訊的發生，同時確保了提供信息的真實性。通過精心設計的驗證問題，模型能夠辨識自身的錯誤並進行修正，從而顯著提高了準確率。


![Dhuliawala et al\. \(2023\) Chain\-of\-Verification Reduces Hallucination in Large Language Models\.](/assets/6ac4201a4cbe/1*scHHZJHnjpQZ_IkE48vzIw.png)

Dhuliawala et al\. \(2023\) Chain\-of\-Verification Reduces Hallucination in Large Language Models\.
### 3\.3 Chain\-of\-Note \(CoN\) Prompting

Retrieval\-augmented language models \(RALMs\) 通過整合外部知識以減少資訊虛構現象，但這些外部資訊的準確性不總是正確，有時候甚至可能會誤導答案。面對判斷現有知識是否充分的挑戰，標準 RALMs 往往在缺乏確切資訊時難以回答 「不知道」。為了解決這些問題， [Yu et al\. \(2023\)](https://arxiv.org/abs/2311.09210){:target="_blank"} 提出了一個新方法，旨在通過有效管理噪音較大和不相關的文檔，以及準確處理未知情境來增強 RALMs 的穩健性。CoN 方法通過系統性地評估文檔的相關性，專注於篩選出關鍵且可靠的資訊，同時排除那些無關的內容。這使得模型在給出答案時，能夠更加精確且與上下文緊密相關。在多個開放域問答資料集上的實驗證明，CoN 方法顯著提高了對於含有較大噪音文檔的準確匹配得分，平均提升了7\.9分，並將對於超出預訓練知識範圍的問題的拒答率提高了10\.5分，從而在性能和可靠性上獲得了明顯的提升。


![Yu et al\. \(2023\) Chain\-of\-Note: Enhancing Robustness in Retrieval\-Augmented Language Models\.](/assets/6ac4201a4cbe/1*uVxx3RSwob30TeblzGEPtg.png)

Yu et al\. \(2023\) Chain\-of\-Note: Enhancing Robustness in Retrieval\-Augmented Language Models\.
### 4\. User Interface

在這章節中，我們將探討如何通過 Active\-Prompt 技術增強與使用者的交互。這牽涉到設計能夠激勵使用者，使其提供更有幫助的反饋或信息的提示，從而實現更高效和滿意的交互體驗。
### 4\.1 Active Prompting

[Diao et al\. \(2023\)](https://arxiv.org/abs/2302.12246){:target="_blank"} 開發的 Active Prompting，旨在使 LLM 更有效地適應各種複雜的推理任務。這個方法引入針對任務的範例提示和 CoT，來提升模型在複雜問答中的表現。與傳統依賴固定樣本的 CoT 不同，Active Prompting 採用了一種新策略，專注於識別並選擇對模型進步最有幫助、最具不確定性的問題進行標註。這一方法得到了基於不確定性的主動學習策略的啟發，透過評估不同的不確定性指標來優化問題的選擇過程。在八項複雜推理任務的表現上，Active Prompting 顯著優於自我一致性策略，在 text\-davinci\-002 和code\-davinci\-002 模型上分別達到了平均 7\.0% 和 1\.8% 的提升，展示了其領先的技術效果。


![Diao et al\. \(2023\) Active Prompting with Chain\-of\-Thought for Large Language Models\.](/assets/6ac4201a4cbe/1*Zmg55jo2aBTN54Fh3phyHQ.png)

Diao et al\. \(2023\) Active Prompting with Chain\-of\-Thought for Large Language Models\.
### 5\. Fine\-Tuning and Optimization

這部分將介紹如何優化模型的表現。這包括使用機器學習技術來發現和應用最有效的提示策略，從而進一步提升LLM的效率和準確性。
### 5\.1 Automatic Prompt Engineer \(APE\)

一般而言，為 LLM 設計有效的 Prompts 需專家細心打造，這是一項複雜的任務。然而， [Zhou et al\. \(2022\)](https://arxiv.org/abs/2211.01910){:target="_blank"} 提出的APE 技術，開啟了自動創建並選擇指令的新途徑。APE 技術突破了手動和固定提示的限制，能夠針對特定任務動態生成並選出最有效的提示。這一方法先分析用戶輸入，設計一系列候選指令，再透過強化學習選擇最優提示，並能即時適應不同情境。經過在多樣的BIG\-Bench 測試套件和 CoT 任務上的廣泛測試，APE 展現了顯著成效，在大部分情況下\(19/24個任務\)勝過了人工編寫的 Prompts，顯著增強了 LLM 的推理性能。APE 技術的創新性進展，為 LLM 處理更廣泛任務提供了更高效、更靈活的方式，最大化發揮了它們在各種應用場景中的潛力。


![Zhou et al\. \(2022\) Large Language Models Are Human\-Level Prompt Engineers\.](/assets/6ac4201a4cbe/1*HY28LI5uz4CVA-Eic1g8-g.png)

Zhou et al\. \(2022\) Large Language Models Are Human\-Level Prompt Engineers\.
### 6\. Knowledge\-Based Reasoning and Generation
### 6\.1 Automatic Reasoning and Tool\-use \(ART\)

LLM 在處理複雜任務時，因推理能力有限和無法利用外部工具而受限。針對這一問題， [Paranjape et al\. \(2023\)](https://arxiv.org/abs/2303.09014){:target="_blank"} 提出的 ART 技術，賦予了 LLM 透過多步驟過程進行推理並無縫整合外部知識的能力。ART 技術有效地補充了推理的不足，使 LLM 能夠處理更複雜的問題，遠超簡單的文本生成。通過整合外部專業知識和計算工具，ART 為 LLM 帶來了前所未有的多功能性和實用性，使它們能在科學研究、數據分析和決策支持等領域發揮作用。ART 通過結構化程序自動化推理步驟，免除了繁瑣的手動設計需求，其動態工具整合能力確保了與外部工具的順暢協作。在 BigBench 和 MMLU 這兩個挑戰性基準的實證測試中，ART 展示了卓越的效果，不僅超越了傳統引導技巧，在某些情況下甚至達到了與精心設計的示範相媲美的水平。


![Paranjape et al\. \(2023\) ART: Automatic multi\-step reasoning and tool\-use for large language models\.](/assets/6ac4201a4cbe/1*86xbBXmOt6NKb71hYk00hg.png)

Paranjape et al\. \(2023\) ART: Automatic multi\-step reasoning and tool\-use for large language models\.
### 7\. Improving Consistency and Coherence
### 7\.1 Contrastive Chain\-of\-Thought \(CCoT\) Prompting

傳統的 CoT 技術，經常漏掉了從錯誤中學習的重要環節。為解決此， [Chia et al\. \(2023\)](https://arxiv.org/abs/2311.09277){:target="_blank"} 提出 CCoT 技術。這種技術通過同時提供正確與錯誤的推理示例來引導模型，就像是在探索一張既標示正確路徑又指出錯誤彎道的地圖，展現了 CCoT 的獨到之處。這種雙重視角的方法在 SQuAD 和 COPA 等推理基準測試中得到了驗證，促使LLM進行逐步推理，在戰略性和數學推理的評估中相比傳統 CoT 取得了4%到16%的提升。當與 self\-consistency 結合使用時，性能進一步提升了約5%。然而，這項技術仍面臨一些挑戰，如如何為不同問題自動生成對比示例，以及其在推理之外的其他自然語言處理任務中的適用性問題。


![Chia et al\. \(2023\) Contrastive Chain\-of\-Thought Prompting\.](/assets/6ac4201a4cbe/1*kuZsftz9ZfdbfbOJ3zSlZg.png)

Chia et al\. \(2023\) Contrastive Chain\-of\-Thought Prompting\.
### 8\. Managing Emotions and Tone
### 8\.1 Emotion Prompting

雖然 LLM 在許多任務上展示了出色的性能，但它們在理解心理學和情緒信號方面的能力仍有待提高。為了解決這一問題， [Li et al\. \(2023\)](https://arxiv.org/abs/2307.11760){:target="_blank"} 提出了EmotionPrompt 技術，這一方法受到了研究語言對人類情緒表現影響的心理學研究啟發，通過在提示中加入 11 個情緒激勵句子，旨在增強 LLM 的情緒智能。實驗結果顯示，引入這些情緒激勵句子顯著提高了 LLM 在各類任務上的表現。具體而言，EmotionPrompt 在指令學習任務中實現了 8% 的性能提升，在 BIG\-Bench 任務上更是實現了高達 115% 的顯著飛躍，這充分證明了它在提高 LLM 處理情緒信號方面的有效性。此外，涉及 106 名參與者的評估顯示，與標準提示相比，使用 EmotionPrompt 可以在創造性任務的表現、真實性和責任感等方面平均提高 10\.9%。


![Li et al\. \(2023\) Large Language Models Understand and Can be Enhanced by Emotional Stimuli\.](/assets/6ac4201a4cbe/1*peBtbEzqoxE1zKdPGeQ_Kg.png)

Li et al\. \(2023\) Large Language Models Understand and Can be Enhanced by Emotional Stimuli\.
### 9\. Code Generation and Execution
### 9\.1 Scratchpad Prompting

雖然基於 Transformer 的 LLM 在撰寫簡單編程任務的代碼方面表現出色，但在需要精確推理的複雜、多步驟算法計算任務上則面臨挑戰。針對這一問題， [Nye et al\. \(2021\)](https://arxiv.org/abs/2112.00114){:target="_blank"} 提出了一種新的方法，該方法著重於任務設計而非對模型本身進行修改，引入了「筆記本」概念。這種策略使得模型能夠在給出最終答案前，產生一系列中間步驟。採用筆記本提示法後，模型在 MBPP\-aug 的成功率達到了46\.8%。結合 CodeNet 和單行數據集之後，模型展現出最佳性能，正確的最終輸出比例達到了26\.6%，完美執行路徑的比例為24\.6%。然而，筆記本提示法也有其限制，包括固定的上下文窗口限制在512個步驟內，以及高度依賴監督學習來有效利用筆記本。


![Nye et al\. \(2021\) Show Your Work: Scratchpads for Intermediate Computation with Language Models\.](/assets/6ac4201a4cbe/1*jh44t6yGb80zzb22XH8HdQ.png)

Nye et al\. \(2021\) Show Your Work: Scratchpads for Intermediate Computation with Language Models\.
### 9\.2 Program of Thoughts \(PoT\) Prompting

由於傾向於算術錯誤、處理複雜方程能力不足，以及在表達複雜迭代過程中的效率低下。為了增強 LLM 在數值推理方面的能力， [Chen et al\. \(2022\)](https://arxiv.org/abs/2211.12588){:target="_blank"} 提出了 PoT，鼓勵利用外部語言解釋器處理計算步驟。通過這種方法，如 Codex這類模型能夠通過執行 Python 程序來顯示其推理過程，在處理包括數學文字題和金融問題的數據集時，相比於傳統的 CoT 提示法，性能平均提高了約12%。


![Chen et al\. \(2022\) Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\.](/assets/6ac4201a4cbe/1*016SQVgVvegUyNSUr7iqYg.png)

Chen et al\. \(2022\) Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\.
### 9\.3 Structured Chain\-of\-Thought \(SCoT\) Prompting

LLM 在代碼生成領域通常採用的 CoT 方法，在生成代碼之前首先產生自然語言的中間推理步驟。雖然這在自然語言生成上非常有效，但在代碼生成任務中，這種方法的準確性較低。針對這一問題， [Li et al\. \(2023\)](https://arxiv.org/abs/2305.06599){:target="_blank"} 提出了一種專門針對代碼生成的創新 Prompt — — SCoT。SCoT 通過將程序結構 \(如序列、分支和循環\) 融入到推理步驟中，顯著提升了 LLM 生成結構化源代碼的能力。這種方法特別強調從源代碼的角度出發來考慮需求，與傳統的 CoT 相比，在代碼生成的效率上實現了顯著的改進。該方法在 ChatGPT 和 Codex 上進行的三個基準測試 \(HumanEval、MBPP和MBCPP\) 中驗證了其有效性，並證明了其性能相較於 CoT 提示法高出至多13\.79%。


![Li et al\. \(2023\) Structured Chain\-of\-Thought Prompting for Code Generation\.](/assets/6ac4201a4cbe/1*2PykGf21gFHCsa5b-I3-1Q.png)

Li et al\. \(2023\) Structured Chain\-of\-Thought Prompting for Code Generation\.
### 9\.4 Chain\-of\-Code \(CoC\) Prompting

儘管 CoT 在提升LLM 的語義推理能力上表現優異，但在處理需要數值或符號推理的問題時則顯得有些力不從心。針對這一問題， [Li et al\. \(2023\)](https://arxiv.org/abs/2312.04474){:target="_blank"} 提出了CoC 技術，目的是通過編程強化模型在邏輯與語義任務上的推理能力。CoC鼓勵 LLM 將語義子任務轉化為靈活的偽代碼，這不僅能讓解釋器識別並處理未定義的行為，還能通過「LMulator」進行模擬操作。實驗結果顯示，CoC 在 BIG\-Bench Hard 測試中以 84% 的準確率超越了 CoT 和其他baseline，準確率提升了12%。


![Li et al\. \(2023\) Chain of Code: Reasoning with a Language Model\-Augmented Code Emulator\.](/assets/6ac4201a4cbe/1*IwRmQNV8336DWxpAwonPHQ.png)

Li et al\. \(2023\) Chain of Code: Reasoning with a Language Model\-Augmented Code Emulator\.
### 10\. Optimization and Efficiency
### 10\.1 Optimization by Prompting \(OPRO\)

在各個領域中，尋找最佳解決方案通常需要通過不斷的嘗試錯誤。 [Yang et al\. \(2023\)](https://arxiv.org/abs/2309.03409){:target="_blank"} 提出了一個創新思路：利用 LLM 來輔助尋找解決方案，這一方法被稱為 OPRO。這種方法的特點在於，它透過 LLM 提示，根據問題描述逐步尋找解決方案，使其能夠快速適應不同問題，並根據需要調整尋找解決方案的過程。通過對如線性回歸和旅行商問題等典型問題的案例分析，這項研究展示了 LLM 在尋找解決方案方面的巨大潛力。同時，它探討了如何優化提示，以在處理自然語言任務時達到最高的準確率，進一步證明了 LLM 的高靈敏度。通過OPRO優化的提示，相較於人工設計的提示，在 GSM8K 數據集上的表現提升了高達 8%，在 Big\-Bench 的一些更具挑戰性的任務上提升了高達 50%。


![Yang et al\. \(2023\) Large Language Models as Optimizers\.](/assets/6ac4201a4cbe/0*u591nf0LTsVDJyqg.png)

Yang et al\. \(2023\) Large Language Models as Optimizers\.
### 11\. Understanding User Intent
### 11\.1 Rephrase and Respond \(RaR\) Prompting

[Deng et al\. \(2023\)](https://arxiv.org/abs/2311.04205){:target="_blank"} 的研究中指出，我們在使用 LLM時，經常忽略了人類思維方式與 LLM 思維方式之間的差異。為了彌補這一差距，他們提出了一種名為 RaR 的新方法。這種方法讓 LLM 能夠在提示中重新表述和擴展問題，從而提高了對問題的理解和回答的準確度。通過將改寫和回答結合，RaR 的雙步驟方法在各類任務上均實現了顯著的性能提升。研究發現，相比隨機提出的人類問題，經過改寫的問題能更清晰地傳達語義，減少問題的模糊性。這些發現為我們理解和提高 LLM 在不同應用中的有效性，提供了寶貴的見解。


![Deng et al\. \(2023\) Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\.](/assets/6ac4201a4cbe/1*f1QSSBsAT3OeGDq8kN3sDA.png)

Deng et al\. \(2023\) Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\.
### 12\. Metacognition and Self\-Reflection
### 12\.1 Take a Step Back Prompting

面對複雜多步推理的挑戰， [Zheng et al\. \(2023\)](https://arxiv.org/abs/2310.06117){:target="_blank"} 針對高級語言模型如 PaLM\-2L，提出了 Take a Step Back Prompting。這項創新讓模型能夠進行高層次的抽象思考，從具體案例總結出基本原則和高級概念。Take a Step Back Prompting 採用一個涵蓋抽象化和推理的雙步驟過程，經過廣泛的實驗驗證，在 STEM、知識問答和多步推理等推理密集型任務上應用該技術，顯著提升了 PaLM\-2L 的推理能力。尤其是在 MMLU 的物理和化學、TimeQA以及MuSiQue等任務上，性能分別提高了7%、27%和7%。


![Zheng et al\. \(2023\) Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\.](/assets/6ac4201a4cbe/1*s5KSd3fAOwOiArdeO7yz-A.png)

Zheng et al\. \(2023\) Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\.
### Conclusion

在 LLM 領域中，Prompt Engineering 已經變成改變規則的關鍵力量，它為LLM 的潛力提供了新的解鎖方式。以上我們彙總了 29 種基於它們獨特功能目標的不同 Prompt 技術，希望能幫助你理解並選擇適合你的 Prompt。
### 支持鼓勵

如果文章對你有幫助，或願意鼓勵我持續創作，可以幫文章拍手，或點擊下方連結請我喝一杯咖啡，感謝支持\!


![](/assets/6ac4201a4cbe/1*QCQqlZr6doDP-cszzpaSpw.png)




_[Post](https://medium.com/@cch.chichieh/llm-%E5%90%84%E7%A8%AE%E6%8A%80%E5%B7%A7-prompt-engineering-%E6%8C%87%E5%8D%97-6ac4201a4cbe){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
